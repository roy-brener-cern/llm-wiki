var relearn_searchindex = [
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "Use of muons reconstructed with Software Release \u003e= 22 Muon Momentum Corrections Muon Selection Tool Muon Efficiency Corrections Systematic Uncertainties Example of how setup the tools Instructions on how to setup the various MCP calibration tools for physics analyses are given in the links below. For instructions relative to Athena releases older than release 22, please consult our old twiki page.",
    "description": "Use of muons reconstructed with Software Release \u003e= 22 Muon Momentum Corrections Muon Selection Tool Muon Efficiency Corrections Systematic Uncertainties Example of how setup the tools Instructions on how to setup the various MCP calibration tools for physics analyses are given in the links below. For instructions relative to Athena releases older than release 22, please consult our old twiki page.",
    "tags": [],
    "title": "Guidelines for Physics Analyses",
    "uri": "/guidelines/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Identification",
    "content": "AOD to ntuples (MuonxAODAnalysis) Call MuonSelectionTool in analysis framework.\nMuonxAODAnalysis is a simple EventLoop-based dumper of AOD/DAOD muon information into ntuples. Based on Yohei Yamaguchi’s original AodMuonAna.\nSetup (required once): Requires Cmake and AnalysisBase\n(latest version from: /cvmfs/atlas.cern.ch/repo/sw/software/24.2/AnalysisBase):\nmkdir -p build run git clone https://:@gitlab.cern.ch:8443/atlas-mcp/muonxaodanalysis.git source cd build asetup AnalysisBase,24.2.12,here cmake ../source \u0026\u0026 make \u0026\u0026 source */setup.sh cd .. In subsequent sessions: For simply running, you don’t need to repeat the above procedure. In subsequent sessions you can just do:\ncd build asetup --restore source */setup.sh cd .. Running on local datasets cd run SubmitLocal.py --inputDS \u003cdirectory path containing root files\u003e Optional arguments:\n–forceRun2WP to force the use of Run2 WP for Run3 datasets Twp example datasets for 2023 data and ttbar MC on eos\n/eos/atlas/user/y/yoyamagu/MuonID/data23_13p6TeV.00451896.physics_Main.deriv.DAOD_PHYS.f1352_m2171_p5632/ /eos/atlas/user/y/yoyamagu/MuonID/mc23_13p6TeV.601230.PhPy8EG_A14_ttbar_hdamp258p75_dil.deriv.DAOD_PHYS.e8514_e8528_s4111_s4114_r14622_r14663_p5737/ In the directory, just one root file for each dataset.\nYou can find output tree under submitDir/data-ANALYSIS/\nRunning on the grid (optional) Requires a valid grid certificate and setup of rucio, panda and pyAMI:\nvoms-proxy-init -voms atlas lsetup rucio panda pyAMI Submit a job on the grid as follows:\ncd run SubmitGrid.py --inputDS \u003cLogical DS Name\u003e Optional arguments:\n–forceRun2WP to force the use of Run2 WP for Run3 datasets –version (-v) to specify a suffix for output DS name ntuples to histograms Very simple example to generate histograms using TTree::MakeClass.\nTTree::MakeClass method generates a skeleton class designed to loop over the entries of the tree.\n$ root -l submitDir/data-ANALYSIS/data23_13p6TeV.00451896.physics_Main.deriv.DAOD_PHYS.f1352_m2171_p5632.root root [] analysis-\u003eMakeClass(\"MyClass\") Modifying MyClass::Loop of MyClass.C to generate histograms.\n... Long64_t nentries = fChain-\u003eGetEntriesFast(); TH1F *dimuon_mass_reco = new TH1F(\"dimuon_mass_reco\",\";m_{#mu#mu} [GeV];\",150,0,150); TH1F *dimuon_mass_medium = new TH1F(\"dimuon_mass_medium\",\";m_{#mu#mu} [GeV];\",150,0,150); TH1F *nprecisionLayers = new TH1F(\"nprecisionLayers\",\";Number of precision layers;\",8,0,8); TH1F *qOverPsignif = new TH1F(\"qOverPsignif\",\";q/p significance;\",50,0,10); TLorentzVector tlv1; TLorentzVector tlv2; ... In the for-loop, we add event selection. Let us require at least two muons, pT\u003e25 GeV to leading muon as an example. After selecting events, we fill the histograms.\n... nb = fChain-\u003eGetEntry(jentry); nbytes += nb; // if (Cut(ientry) \u003c 0) continue; if(muon_pt-\u003esize() \u003c 2) continue; if(muon_pt-\u003eat(0) \u003c 25.) continue; tlv1.SetPtEtaPhiE(muon_pt-\u003eat(0),muon_eta-\u003eat(0),muon_phi-\u003eat(0),muon_e-\u003eat(0)); tlv2.SetPtEtaPhiE(muon_pt-\u003eat(1),muon_eta-\u003eat(1),muon_phi-\u003eat(1),muon_e-\u003eat(1)); dimuon_mass_reco-\u003eFill((tlv1+tlv2).M()); if(!muon_Medium-\u003eat(0) || !muon_Medium-\u003eat(1)) continue; dimuon_mass_medium-\u003eFill((tlv1+tlv2).M()); // assume subleading muon as probe nprecisionLayers-\u003eFill(muon_nprecisionLayers-\u003eat(1)); qOverPsignif-\u003eFill(muon_qOverPsignif-\u003eat(1)); ... Outside of the for-loop, we draw the histograms.\n... TCanvas* c_dimuon_mass_reco = new TCanvas(\"c_dimuon_mass_reco\"); dimuon_mass_reco-\u003eDraw(); TCanvas* c_dimuon_mass_medium = new TCanvas(\"c_dimuon_mass_medium\"); dimuon_mass_medium-\u003eDraw(); TCanvas* c_nprecisionLayers = new TCanvas(\"c_nprecisionLayers\"); nprecisionLayers-\u003eDraw(); TCanvas* c_qOverPsignif = new TCanvas(\"c_qOverPsignif\"); qOverPsignif-\u003eDraw(); ... We will now load MyClass. The first step is to load the class file. Then we can call the Loop method, which will build and display the histograms.\nroot [] .L MyClass.C root [] MyClass m root [] m.Loop() We see Zmumu peaks, as well as nprecisionLayers/qOverPsignificance distributions.",
    "description": "AOD to ntuples (MuonxAODAnalysis) Call MuonSelectionTool in analysis framework.\nMuonxAODAnalysis is a simple EventLoop-based dumper of AOD/DAOD muon information into ntuples. Based on Yohei Yamaguchi’s original AodMuonAna.\nSetup (required once): Requires Cmake and AnalysisBase\n(latest version from: /cvmfs/atlas.cern.ch/repo/sw/software/24.2/AnalysisBase):\nmkdir -p build run git clone https://:@gitlab.cern.ch:8443/atlas-mcp/muonxaodanalysis.git source cd build asetup AnalysisBase,24.2.12,here cmake ../source \u0026\u0026 make \u0026\u0026 source */setup.sh cd .. In subsequent sessions: For simply running, you don’t need to repeat the above procedure. In subsequent sessions you can just do:",
    "tags": [],
    "title": "Hands-on",
    "uri": "/identification/hands-on/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MuonCalibrationFit",
    "content": "MuonCalibrationFit package replies on C++ code (a simple implementation of AthAnalysis) to read ntuples, store histograms and analyse said histograms. The python part of MuonCalibrationFit, on the other hand, interfaces with AthAnalysis, dictates how exactly the calibration fit should be performed, in addition to job submission and output handling.\nThe underlying python classes are found under source/MuonCalibrationFit/python/Classes, but the most relevant class to job submission is the Job class as seen here. The Job class is what we create for every single calibration iteration (such as Iter1 from Quick Start), and by setting its attributes, or by calling its methods, we can change how a specific calibration is run.\n## A simple start\nTo highlight the most important job methods, here we have some pseudo code for creating and running a Job:\nTestJob1 = JobUtils.Job('A_barebone_job1', Detector='ID') TestJob1.Files = JobUtils.Files(os.path.expandvars('{}/share/NTupleList/V8Data/'.format(MCF_Dir)), 'Data_mc16d.txt', 'Signal_mc16d.txt', 'Background_mc16d.txt') # reads in the base files TestJob1.BaseFilePath = \"/eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/muonptcalib_v08/MinimalVars/\" # point to where ntuples are stored TestJob1.RunningSystem = \"Condor\" TestJob1.Regions = os.path.expandvars('{}/share/RegionDefinition/FirstRegion.txt'.format(MCF_Dir)) # 5 coarse eta regions TestJob1.Run() # here's how we actually set the job running! Once TestJob1 has finished running, we might want to continue from its outputs, as well as inheriting some of its settings (so we don’t have to manually set it every single time we create a new Job object! This can be achieved by:\nTestJob2 = JobUtils.Job('A_barebone_job2', Detector='ID') TestJob2.StartFrom('A_barebone_job1') # here it inherits all the attributes from TestJob1! TestJob2.initMinWithPrevResults = True # here it initialises the calibration parameters as the fit results from the previous iteration TestJob2.Run() # here we go again! One thing to note here is that the StartFrom() method overwrites all current attributes once it’s called, so if you want to tweak a few attributes on top of what you started with, tweak it after calling StartFrom() or you wouldn’t actually change any attributes!! Speaking of attributes…\n## Job attributes\nAn exhaustive list of job options is shown here. Most of these options can be left as default, with more relevant options are listed below:\nGeneral Attributes Description RunningSystem ‘Condor’, ‘Slurm’, ‘Interactive’ RunningMode ‘User’ or ‘Auto’. Auto: the main script needs to be allowed to run continously to keep submitting jobs; User: the user takes control of submitting each stage SlurmSettingDict Slurm setting as a dictionary. E.g. “{“RunJob”: {“Time”:“12:00:00”, “Memory”: “16000M”}, “MergerJob”: {“Time”:“00:20:00”, “Memory”: “1000M”}}” Level ‘INFO’, ‘DEBUG’ or ‘VERBOSE’, printout level DoCheck If set to True, run a separate iteration using only the calibration output from this iteration to check the template distribution. initMinWithPrevResults If set to True and StartFrom a given iteration, initialise calibration parameters as the output from the previous iteration. ApplySaggitaBiasCorr If set to True, apply sagitta bias before deriving calibration. OverwriteCurrentCalibGuess Set path to injection file. E.g. link OverwriteParams Set the parameters that are overwritten according to injection file, separated by comma. E.g. “s0,s1,r0,r1,r2” BkgWeightMultipler If set to any value, apply it as a flat multiplier on background weight. DynamicSmoothSampling If set to True, apply dynamic sampling to Jpsi. See getDynamicSampling() in Tools.h link Input Attributes Description Files The txt files storing the base names for data, signal and background files. E.g. link BaseFilePath The file path for where the ntuples are stored. E.g. /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/muonptcalib_v08/MinimalVars Regions Region definition file for this iteration to run on. Examples are seen here. CheckRegions If DoCheck is set to True, CheckRegions is the regions to run checks InputScanSizeFile If doing GridScan, this is the txt file which defines scan size for each parameter in each region. Fit Attributes Description Detector ‘ID’, ‘CB’ or ‘MS’ BkgParametrization ‘Exponential’ or else. Fits background as Exponential function or Chebychev polynomial. BkgFitTechnique ‘Analytical–\u003eCB+G’, ‘Analytical–\u003e2CB’, ‘Template’. Defines how to parametrise signal. RunMode This sets the minimisation algorithm, function and number of retrials (except for GridScan, where it stands for the number of grids). The syntax is as follows: “Minimiser[params_to_fit]:Function:NRetrials”. If you want to run multiple algorithms sequentially, you can use “+” sign as a delimeter. E.g. “Simplex[d_s0,d_s1]:HighLevelChi2:1+Simplex:Chi2:1+GridScan:Chi2:5” translates to: Use Simplex as minimiser, and minimise HL Chi2 with d_s0 and d_s1 floated (“Simplex[d_s0,d_s1]:HighLevelChi2:1”). Then use simplex again to minimise Chi2 with all parameters floated (“Simplex:Chi2:1”). Lastly, run GridScan with 5 grids using Chi2, and with all parameters floated. MaxEvents Number of events to run on. Set to -1 to use all available events. ReweightPairTypeMethod Kinematic reweighing for pair pt/eta. For Rel21 run 2, use ‘V52DDataIncl’. A full list of reweigh types can be found here Template Attributes Description (Min-, Max-) MuonsPtForJpsi Leading and subleading muon pt cut for Jpsi (MinLeading-, MinSubLeading-) MuonPtForZ Leading and subleading muon pt cut for Z JpsiMass- (Bins, Min, Max) Options for the jpsi mass templates. The created templates will have binning of (Bins, Min, Max). ZMass- (Bins, Min, Max) Options for the Z mass templates. The created templates will have binning of (Bins, Min, Max). JpsiMassPts, ZMassPts Creates templates in bins of Jpsi(Z) subleading(leading) pts. E.g. setting Job.JpsiMassPts=“6.3 9 20” will create two Jpsi mass templates. One for events with 6.3 \u003c subleading pt \u003c 9, and the other with 9 \u003c subleading pt \u003c 20. Parameter Attributes Description Note: Here, all attributes are prefixed by one of the following parameter strings: ‘dS0’, ‘dS1’, ‘dR0’, ‘dR1’, ‘dR2’ *_Fit Whether a parameter is fit or not. *_Init Initial value for this parameter. *_Overwrite Whether a parameter is overwritten by injection file or not. If injecting a specific parameter, set _Fit to False. *_ScanSize If doing GridScan, the total range of the grid. *_Step If doing GridScan, the step between each grid. HLChi2 Attributes Description Use_MCError If doing HLChi2, set to true to account for the MC error in HLChi2 calculation in its denominator. Mean_Weight and Std_Weight The relative weight for the mean only term and standard deviation only term of HLChi2. *_MeanRange, *_StdevRange The range to calculate HLChi2 for mean and standard deviation. If set, the range to calculate HLChi2 is (mean - nstdev, mean+nstdev), where mean and stdev are the corresponding quantity from each histograms, and n is the set value. Systematics variation For Run 2 calibration fit, the systematic impact for calibration is evaluated by changing certain fit parameters/settings and calculate its difference wrt nominal fit. Without going into any details, the changed attributes are listed in the following json file.",
    "description": "MuonCalibrationFit package replies on C++ code (a simple implementation of AthAnalysis) to read ntuples, store histograms and analyse said histograms. The python part of MuonCalibrationFit, on the other hand, interfaces with AthAnalysis, dictates how exactly the calibration fit should be performed, in addition to job submission and output handling.\nThe underlying python classes are found under source/MuonCalibrationFit/python/Classes, but the most relevant class to job submission is the Job class as seen here. The Job class is what we create for every single calibration iteration (such as Iter1 from Quick Start), and by setting its attributes, or by calling its methods, we can change how a specific calibration is run.",
    "tags": [],
    "title": "JobOptions for python script",
    "uri": "/scale_resolution/muoncalibrationfit/joboptions/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "The MuonCalNTupleMaker code is used by the MMC group produce the mini-ntuples used to run the sagitta bias correction code and the calibration fit code. The code runs on the MuonPerformanceAnalysis output files (MMC.root)\nWorkflow concept The mini-ntuples used by the group differ from the ntuples produced by the MuonPerformanceAnalysis in that they contain fewer variables (and are therefore easier to manage). In addition to this, the trigger selection and matching, together with a loose cut on the $p_T$ is applied (see source/MuonCalNTupleMaker/MuonCalNTupleMaker/Selector.h). The SampleWeight variable is also defined as the multiplication of the Luminosity (hardcoded in source/MuonCalNTupleMaker/MuonCalNTupleMaker/Defs.h), the sample cross-section (hardcoded in source/MuonCalNTupleMaker/MuonCalNTupleMaker/Defs.h) and divided by the sum of weights calculated in the pre-processing. The TotalWeight variable is defined too with the multiplication of the SampleWeight, EventWeight and the BeamSpotWeight.\nSetup the code To setup the MuonCalNTupleMaker code on lxplus or any other server with access to cvmfs follow these instructions\nmkdir MyWorkingArea # Create your working area cd MyWorkingArea # Move to your working area setupATLAS mkdir build run git clone --recursive https://gitlab.cern.ch/atlas-mcp/muoncalntuplemaker.git source cd build lsetup \"asetup AthAnalysis,22.2.113\" emi #lsetup \"asetup AthAnalysis,24.2.27\" emi cmake ../source \u0026\u0026 make -j 4 cd ../run/ source ../build/x86_64-el9-gcc13-opt/setup.sh # Setup the environment The next time you wish to setup the code, it will be sufficient to setup AthAnalysis environment.\ncd MyWorkingArea/build/ asetup --restore cd ../run/ source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment Intro to the code The main in to the code is utils/processNTuple.cxx. This file takes a single input file, processes it, and outputs it. To control what variables are read (for speeding up), and what variables are saved to the output, it is controlled through a config. See source/MuonCalNTupleMaker/data/config/nominal.txt for an example. More informations in the are in the dedicated section.\nRun the code To run easly the code, a top level python script source/python/processSample.py that will allow for job submission.\nprocessSample.py -i \u003c/path/input/directory/\u003e -o \u003c/path/output/directory/\u003e -c ../source/MuonCalNTupleMaker/data/config/nominal.txt --runLocal Input and Output directories, together with the config file, must be explicity specified.\nOptions Name Default Notes --queue longlunch Queue to submit to --dryRun False Does not submit to anything --runLocal False Run in parallel --submitCondor False Submit jobs to the batch --submitCedarContainer False submission for cedarContainer --submitLxbatchCluster False submission for lxbatch cluster (INFN RM1) --maxProcs 4 Number of parallel processed --remergeResplit False expert option to merge and split jobs --remergeResplitSize 1 expert option to find how many to merge jobs with --cacheFile weightCache.pkl File to store cache things in --overwriteCache False overwrite cache More informations on the sumission are in the dedicated section.",
    "description": "The MuonCalNTupleMaker code is used by the MMC group produce the mini-ntuples used to run the sagitta bias correction code and the calibration fit code. The code runs on the MuonPerformanceAnalysis output files (MMC.root)\nWorkflow concept The mini-ntuples used by the group differ from the ntuples produced by the MuonPerformanceAnalysis in that they contain fewer variables (and are therefore easier to manage). In addition to this, the trigger selection and matching, together with a loose cut on the $p_T$ is applied (see source/MuonCalNTupleMaker/MuonCalNTupleMaker/Selector.h). The SampleWeight variable is also defined as the multiplication of the Luminosity (hardcoded in source/MuonCalNTupleMaker/MuonCalNTupleMaker/Defs.h), the sample cross-section (hardcoded in source/MuonCalNTupleMaker/MuonCalNTupleMaker/Defs.h) and divided by the sum of weights calculated in the pre-processing. The TotalWeight variable is defined too with the multiplication of the SampleWeight, EventWeight and the BeamSpotWeight.",
    "tags": [],
    "title": "MuonCalNTupleMaker",
    "uri": "/scale_resolution/muoncalntuplemaker/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MMC Tutorial",
    "content": "This is a summary of the commands. For more details please follow the tutorial here.\nPreparation setupATLAS lsetup git mkdir SagittaBiasTutorial cd SagittaBiasTutorial mkdir build run Clone the code First we setup the SagittaBiasCorrections code.\ngit atlas init-workdir https://:@gitlab.cern.ch:8443/atlas/athena.git cd athena git clone --recursive https://:@gitlab.cern.ch:8443/atlas-mcp/SagittaBiasCorrections.git cd SagittaBiasCorrections/InDetAlignExample git checkout tutorial # it is recommended to work on the development branch git branch -v -a Compiling the code cd ../../../build cd build asetup Athena,24.0.60 cmake ../athena/Projects/WorkDir make -j4 source */setup.sh How to run interactively cd ../athena/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/ cd run Validation --ntupleType 2 --TrkCollection CB Making Plots with more iterations Validation --iterations 5 --ntupleType 2 --TrkCollection CB cd ../scripts root .x thePlots.C(“../run/Output_file.root”,5)",
    "description": "This is a summary of the commands. For more details please follow the tutorial here.\nPreparation setupATLAS lsetup git mkdir SagittaBiasTutorial cd SagittaBiasTutorial mkdir build run Clone the code First we setup the SagittaBiasCorrections code.\ngit atlas init-workdir https://:@gitlab.cern.ch:8443/atlas/athena.git cd athena git clone --recursive https://:@gitlab.cern.ch:8443/atlas-mcp/SagittaBiasCorrections.git cd SagittaBiasCorrections/InDetAlignExample git checkout tutorial # it is recommended to work on the development branch git branch -v -a Compiling the code cd ../../../build cd build asetup Athena,24.0.60 cmake ../athena/Projects/WorkDir make -j4 source */setup.sh How to run interactively cd ../athena/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/ cd run Validation --ntupleType 2 --TrkCollection CB Making Plots with more iterations Validation --iterations 5 --ntupleType 2 --TrkCollection CB cd ../scripts root .x thePlots.C(“../run/Output_file.root”,5)",
    "tags": [],
    "title": "Sagitta Bias corrections",
    "uri": "/scale_resolution/tutorial/sagitta/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MuonCalNTupleMaker",
    "content": "Run on multiple samples To run on multiple samples, a simple bash script can be created with the following lines.\nfor folder in \u003cpath/of/all/the/samples\u003e/group.perf-muons.mc20_13TeV* do echo $folder processSample.py --input ${folder} --output \u003cpath/of/the/output/\u003e --config ../source/MuonCalNTupleMaker/data/config/nominal.txt --runLocal done it will create a folder localRunLogs in which different folders with the same name as your sample will be created. Inside each folder (after the end of the running time) a .txt file for each root file will be created with the log of the job. Another folder jobList is created in which different folders with the same name as your sample will be created. Inside each folder, a bash file is created with the commands to run to create each output-root file.\nSubmit to condor To submit the job to condor, the --submitCondor should be used instead of --runLocal. It will create a folder condor in which different folders with the same name as your sample will be created. Inside each folder (after the end of the running time) the .log, .out, .err files for each root file will be created with the log, output and error of the job. Another folder jobList is created in which different folders with the same name as your sample will be created. Inside each folder, a bash file is created with the commands to run to create each output-root file. The --queue \u003cname_queue\u003e can be used to determine which queue can be used. The following options for condor are available:\nespresso [20 minutes] microcentury [1 hour] longlunch [2 hours] workday [8 hours] tomorrow [24 hours] testmatch [3 days] nextweek [7 days] Submit to other clusters To submit the job to another cluster (Cedar and Lxbatch are supported)), the --submitCedarContainer or --submitLxbatchCluster should be used instead of --runLocal. As for the condor submission, a folder slurm/ or lxbatchCluster are created to collect the logs. The jobList will contain the bash files. More clusters can easily be added by inheriting the submission class in python/jobSubmission.py.\nTest submission The --dryRun option can be used to test the submission together with the --dryRun, --submitCondor, --submitCedarContainer and --submitLxbatchCluster. It will create all the folders needed and it prints the submission commands.",
    "description": "Run on multiple samples To run on multiple samples, a simple bash script can be created with the following lines.\nfor folder in \u003cpath/of/all/the/samples\u003e/group.perf-muons.mc20_13TeV* do echo $folder processSample.py --input ${folder} --output \u003cpath/of/the/output/\u003e --config ../source/MuonCalNTupleMaker/data/config/nominal.txt --runLocal done it will create a folder localRunLogs in which different folders with the same name as your sample will be created. Inside each folder (after the end of the running time) a .txt file for each root file will be created with the log of the job. Another folder jobList is created in which different folders with the same name as your sample will be created. Inside each folder, a bash file is created with the commands to run to create each output-root file.",
    "tags": [],
    "title": "Submission and Run",
    "uri": "/scale_resolution/muoncalntuplemaker/submission/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "Introduction This page currently holds information and tips on how to use muons reconstructed with software releases \u003e=22. For example reprocessed with rel-22 Run-2 datasets with mc20 or Run-3 datasets with mc21/mc23a/mc23c/mc23d.\nWarning Please note that the current Run3 recommendations are valid only reprocessed 2022, reprocessed 2023, and 2024 data, i.e. for the following condition-DB tags:\n2022: \u003e= CONDBR2-BLKPA-2022-14 2023: \u003e= CONDBR2-BLKPA-2023-04 2024: \u003e= CONDBR2-BLKPA-2024-03 For older tags, the previous set of recommendations should be used instead. Please check the condition-DB tag corresponding to your datasets’ reconstruction tags (r15869, r15774, r15810, etc.) using AMI (example). The condition database tag can be checked knowing the reconstruction tag used for your dataset using AMI: ). More informations about the tags can be found here.\nOverview of latest prescriptions The table below summarizes the set of relevant tools and configurations corresponding to the latest MCP recommendations. Further down in this page, more details and instructions are provided.\nAnalysisBase\u003e=25.2.53 TWiki Tool (Calibration) Release Comments Selection working points Muon Selection Tool CP::MuonSelectionTool - WP: Loose, Medium, Tight, LowPt, HighPt Reconstruction and selection efficiency Muon Efficiency Corrections CP::MuonEfficiencyScaleFactors Run2: 230213_Preliminary_r22run2 Run3: 250418_Preliminary_r24run3 (old Run3: 240711_Preliminary_r24run3) Use separate MESF tool instance, WorkingPoint = WP (see above) See Muon Efficiency Corrections section for instructions on Run3 recommendations. Scale factors and data-driven efficiencies from the T\u0026P analysis from Run-3 data for Tight, Medium, Loose, HighPt with $p_T$ \u003e 5 GeV and $|\\eta|$ \u003c 2.5 and for LowPt with $p_T$ \u003e 3.5 GeV and $|\\eta|$ \u003c 2.5 are derived using the whole 2022+2023 dataset. For Run2 Scale Factors, Loose, Medium and Tight WP are supported up to 3 GeV. HighPt WP is supported too (UseBEEBISInHighPtRun3 flag should be true for the new set of recommendations. Isolation efficiency Muon Efficiency Corrections CP::MuonEfficiencyScaleFactors Run2: 230213_Preliminary_r22run2 Run3: 250418_Preliminary_r24run3 (old Run3: 240711_Preliminary_r24run3) Use separate MESF tool instance, WorkingPoint = WP+\"Iso\" suffx, e.g. PflowLoose_VarRadIso (see here for available wps). TTVA efficiency Muon Efficiency Corrections CP::MuonEfficiencyScaleFactors Run2: 230213_Preliminary_r22run2 Run3: 250418_Preliminary_r24run3 (old Run3: 240711_Preliminary_r24run3) Use separate MESF tool instance, WorkingPoint = TTVA Momentum MuonMomentumCorrections CP::MuonCalibTool Run2 and Run3: Recs2025_03_26_Run2Run3 (old Run3) Recs2024_05_06_Run2Run3 Latest Run 3 [data22, data23, data24] (using Rel23) [to be used with Analysis Release \u003e= 25.2.53] and Rel22 Run2 preliminary corrections Useful references We don’t have release 22 documentation yet, even if there is a paper in preparation. Info from release 21 are still valid in most cases though.\nRelease Setup We suggest to use releases \u003e=25.2.53, e.g. asetup AnalysisBase 25.2.53 or asetup AthAnalysis 25.2.53. Earlier releases can in principle also be used but due to a change in the behaviour of the MuonSelectionTool, release 25.2.52 should NOT be used with the latest recommendations.\nImplementation of the tools Some minimal example code for C++-based implementation of the MCP tools is available in this repository, with the actual implementation provided in the header and cxx files.\nThe recommendations In this section the recommendations are discussed. The technical implementation of the tool is discussed below.\nIdentification WPs Supported identification WPs are described here.\nIsolation WPs Supported isolation WPs are described here.\nTTVA WP The recommended WP for Muon track-to-vertex association (TTVA) is: |$d_0^{BL}$ significance | \u003c 3 AND $|\\Delta z_0^{BL} sin(\\theta)|$ \u003c 0.5 mm. Those variables are defined at this link.",
    "description": "Introduction This page currently holds information and tips on how to use muons reconstructed with software releases \u003e=22. For example reprocessed with rel-22 Run-2 datasets with mc20 or Run-3 datasets with mc21/mc23a/mc23c/mc23d.\nWarning Please note that the current Run3 recommendations are valid only reprocessed 2022, reprocessed 2023, and 2024 data, i.e. for the following condition-DB tags:\n2022: \u003e= CONDBR2-BLKPA-2022-14 2023: \u003e= CONDBR2-BLKPA-2023-04 2024: \u003e= CONDBR2-BLKPA-2024-03 For older tags, the previous set of recommendations should be used instead. Please check the condition-DB tag corresponding to your datasets’ reconstruction tags (r15869, r15774, r15810, etc.) using AMI (example). The condition database tag can be checked knowing the reconstruction tag used for your dataset using AMI: ). More informations about the tags can be found here.",
    "tags": [],
    "title": "Use of muons reconstructed with Software Release \u003e= 22",
    "uri": "/guidelines/release22/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools",
    "content": "MCP’s documentation is prepared using this repository.",
    "description": "MCP’s documentation is prepared using this repository.",
    "tags": [],
    "title": "User Documentation",
    "uri": "/commonsoftware/userdocs/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Open tasks",
    "content": "A Qualification Project (AQP) procedure All the tasks listed below are not only meant for AQPs, but if you’re considering those for that purpose, here you find the things you should know. AQPs should have a local supervisor (from the institute of the qualifier) and a technical supervisor (usually one MCP convener/subconvener/expert). Central information can be found here. In MCP we tipically apply the following procedure:\nthe qualifier and the local supervisor get in touch with the conveners and the relevant subconvener(s) to mainfest their interest on possible topics for an AQP; a chat is set-up to identify the best-fitting task for the qualifier among the projects of interest for MCP; after converging on a topic, the conveners prepare a draft for the AQP description and iterate on that with the qualifier and the local supervisor; then the proposal is sent to Physics Coordination (PC) that provides feedback; after getting approval by Physics Coordination, a JIRA ticket is opened on the MCP board to keep track of the AQP (the ticket is only used to keep track e.g. of presentations on the APQs, not for technical discussions that will happen on dedicated tickets if needed); the jira ticket needs to get the positive reply from PC first; then the team leader of the qualifier’s institute submits the AQP proposal via the Glance interface; the project should go under Physics and not under Muon System; the authorship committee reviews the proposal, provides eventual feedback, and when appropriate approves the project. After starting the project, the qualifier will receive appropriate mentoring on the relevant muon performance topics by the MCP group. The local supervisor is required to guarantee the qualifier receives from the institute’s group appropriate mentoring on technical topics (e.g. usage of ROOT, programming…). The qualifier will provide regular reports in the relevant MCP sub-group meetings, and when requested more formal reports in MCP plenaries (on average two or three presentations at the MCP plenary meeting). The qualifier is requested to provide any software tool developed during the project to the group, submitting any relevant code into MCP’s repositories. At the end of the project the qualifier is required to document the work performed in an INT note and with a final summary presentation at the MCP plenary. After iterating on the INT note report, the technical supervisor will finalize the project by submitting the final report to Glance.\nA template for the presentation of an AQP at the MCP plenary can be found here\nNote Before the qualification checkpoint, the local supervisor should make a presentation of the qualifier’s work outlining the qualifier’s progress and what the qualifier has produced. In this presentation possible problems and if necessary a new timeline of work should be discussed.",
    "description": "A Qualification Project (AQP) procedure All the tasks listed below are not only meant for AQPs, but if you’re considering those for that purpose, here you find the things you should know. AQPs should have a local supervisor (from the institute of the qualifier) and a technical supervisor (usually one MCP convener/subconvener/expert). Central information can be found here. In MCP we tipically apply the following procedure:\nthe qualifier and the local supervisor get in touch with the conveners and the relevant subconvener(s) to mainfest their interest on possible topics for an AQP; a chat is set-up to identify the best-fitting task for the qualifier among the projects of interest for MCP; after converging on a topic, the conveners prepare a draft for the AQP description and iterate on that with the qualifier and the local supervisor; then the proposal is sent to Physics Coordination (PC) that provides feedback; after getting approval by Physics Coordination, a JIRA ticket is opened on the MCP board to keep track of the AQP (the ticket is only used to keep track e.g. of presentations on the APQs, not for technical discussions that will happen on dedicated tickets if needed); the jira ticket needs to get the positive reply from PC first; then the team leader of the qualifier’s institute submits the AQP proposal via the Glance interface; the project should go under Physics and not under Muon System; the authorship committee reviews the proposal, provides eventual feedback, and when appropriate approves the project. After starting the project, the qualifier will receive appropriate mentoring on the relevant muon performance topics by the MCP group. The local supervisor is required to guarantee the qualifier receives from the institute’s group appropriate mentoring on technical topics (e.g. usage of ROOT, programming…). The qualifier will provide regular reports in the relevant MCP sub-group meetings, and when requested more formal reports in MCP plenaries (on average two or three presentations at the MCP plenary meeting). The qualifier is requested to provide any software tool developed during the project to the group, submitting any relevant code into MCP’s repositories. At the end of the project the qualifier is required to document the work performed in an INT note and with a final summary presentation at the MCP plenary. After iterating on the INT note report, the technical supervisor will finalize the project by submitting the final report to Glance.",
    "tags": [],
    "title": "AQPs procedure",
    "uri": "/open-tasks/aqps_procedure/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MuonCalibrationFit",
    "content": "For fits on MS (Muon Spectrometer) muons the alignment of the muon system, which is paramaterized by $r_2$ in the standard momentum correction equation, has a non-negligeblie impact on the fit, and cannot be set to 0 as for ID and CB muons. However this does not mean that this parameter has to be left free in the fit, adding another degree of freedom. Instead we calculate the values of $r_2$ directly from the alignment. To do this a python script CreateR2Injection.py is provided in the Scripts directory.\nBackground The $\\Delta r_2$ fit parameter is calculated via alignment information from the MDTs. Calculating this paramter explicitily avoids leaving this parameter free in the MS fits. For each of the muon regions (BA, EC, EE, BEE, and previously CS) we are given alignment values and uncertainties curtoesy of Pierre-Francoise, typically in the from of a chart. Here are the alignment values and their uncertainties for years 2022 and 2023: These values are then combined with the instrinsic MDT resolution and error. The values used are the same as those given in 2015-2016 and are $\\sigma_\\text{MDT} = 0.00001897103 \\pm 0.000015378$ GeV$^{-1}$. Then the intrisic resolution and the alignment values, as well as their uncertainties, are combined as $\\Delta p_2 = \\sqrt{\\sigma_{\\mu_{tot}}^2 + \\sigma_{\\text{MDT}}^2}$\nCalculating r2 To calculate r2 you need to have the alignment values either as a .tex or a .txt format. For Run 3, the files can be found here for the .tex file and here for the .txt format. If you only have a latex file available, a text file will be created during the running of the program and placed in the output. In general the program can be run like this\nCreateR2Injection.py -i ../../share/InjectedValues/Run3/run3_reprocessed_alignment.txt -c ../../share/InjectedValues/Run3/run3_old_alignment.txt -o ../../share/InjectedValues/Run3 To explain the arguments:\n-i is the input, this takes either a .txt or .tex as mentioned before -o is the output directory, this is where plots or injection values will be placed at the conclusion of the script -c is an optional comparison argument which changes the run mode of thejscript and should point at another .txt or .tex file which has alignment information you would like to compare the new ones against. When selecting the comparison mode, the script will produce a plot of the new $r_2$ values and their uncertainties, as well as a comparison plot with the comparison alignment values. If not selecting any argument for comparison, the script will produce the injection values instead.\nCreating injection values To create the injection values needed for running the MS fits, you should run the script without a comparison argument, like so:\nCreateR2Injection.py -i ../../share/InjectedValues/Run3/run3_reprocessed_alignment.txt -o ../../share/InjectedValues/ This will use the MS region information located in the RegionDefinition folder of the MuonCalibrationFit code found here to calculate r2 injection values for the same regions, which will be placed in the output folder. To see how these injection files should be used look at the JobOptions page found here.",
    "description": "For fits on MS (Muon Spectrometer) muons the alignment of the muon system, which is paramaterized by $r_2$ in the standard momentum correction equation, has a non-negligeblie impact on the fit, and cannot be set to 0 as for ID and CB muons. However this does not mean that this parameter has to be left free in the fit, adding another degree of freedom. Instead we calculate the values of $r_2$ directly from the alignment. To do this a python script CreateR2Injection.py is provided in the Scripts directory.",
    "tags": [],
    "title": "Calculating R2 for Muon Spectormeter fits",
    "uri": "/scale_resolution/muoncalibrationfit/r2injection/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MMC Tutorial",
    "content": "Preparation setupATLAS lsetup git mkdir MMCCalibrationTutorial cd MMCCalibrationTutorial mkdir build run Clone the code First we setup the MuonCalibrationFit code. Cloning the branch with the el9 setup (master is still Centos7 but sooner we will swap the master to the el9 configuration).\ngit clone --recursive -b ER_MCPTutorial https://gitlab.cern.ch/atlas-mcp/MuonCalibrationFit.git source Compiling the code cd build asetup AthAnalysis,25.2.16,here cmake ../source \u0026\u0026 make -j 4 source x86_64-el9-gcc13-opt/setup.sh # Setup the environment Setup the environment variables cd ../run/ mkdir Plots/ export PYTHONPATH=$PYTHONPATH:$TestArea/../source/MuonCalibrationFit/python/Classes export PYTHONPATH=$PYTHONPATH:$TestArea/../source/MCPCalTemplateFit/python/Classes export HTMLRAWDIR=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCCalibrationTutorial/run/Plots/ # \u003c--- change the path here export HTMLDISPLAYDIR=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCCalibrationTutorial/run/Plots/ # \u003c--- change the path here export EMAIL_ADDRESS=\u003cUSERNAME\u003e@cern.ch # \u003c--- change the username here How to run interactively We can use the already defined jobOption for this tutorial ../source/MuonCalibrationFit/python/Run3_CB_Test.py\npython ../source/MuonCalibrationFit/python/Run3_CB_Test.py Produce the standard MMC txt file We will now convert the output of the MuonCalibrationFit into a txt file to check the results\npython ../source/MuonCalibrationFit/python/Finalise_Results.py Output if the fit failed (we will try to understand why) you can copy the output from\ncp -r /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Calibration/Run3_CB_MCPTutorial_1109_10k_test_CB_Iter1 . cp /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Calibration/Run3_CB_MCPTutorial_1309_10k_CB.txt . Check the differences of the parameters with respect to a reference To do so, there is an easy script to produce plots of the parameters of the Custom calibration and the Reference one (in this case, we will use the release set of recommendations for 2023).\ncp /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Calibration/doParameterComparison.cxx . root -l 'doParameterComparison.cxx(\"Run3_CB_MCPTutorial_1309_10k_CB.txt\")' the output is the “Plots/” folder in which you will see the plots for each parameters and the Comparison.pdf which is a Summary set of slides with all the plots.",
    "description": "Preparation setupATLAS lsetup git mkdir MMCCalibrationTutorial cd MMCCalibrationTutorial mkdir build run Clone the code First we setup the MuonCalibrationFit code. Cloning the branch with the el9 setup (master is still Centos7 but sooner we will swap the master to the el9 configuration).\ngit clone --recursive -b ER_MCPTutorial https://gitlab.cern.ch/atlas-mcp/MuonCalibrationFit.git source Compiling the code cd build asetup AthAnalysis,25.2.16,here cmake ../source \u0026\u0026 make -j 4 source x86_64-el9-gcc13-opt/setup.sh # Setup the environment Setup the environment variables cd ../run/ mkdir Plots/ export PYTHONPATH=$PYTHONPATH:$TestArea/../source/MuonCalibrationFit/python/Classes export PYTHONPATH=$PYTHONPATH:$TestArea/../source/MCPCalTemplateFit/python/Classes export HTMLRAWDIR=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCCalibrationTutorial/run/Plots/ # \u003c--- change the path here export HTMLDISPLAYDIR=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCCalibrationTutorial/run/Plots/ # \u003c--- change the path here export EMAIL_ADDRESS=\u003cUSERNAME\u003e@cern.ch # \u003c--- change the username here How to run interactively We can use the already defined jobOption for this tutorial ../source/MuonCalibrationFit/python/Run3_CB_Test.py",
    "tags": [],
    "title": "Calibration",
    "uri": "/scale_resolution/tutorial/calibration/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MuonCalNTupleMaker",
    "content": "The config file is needed to setup the code. Possibilities are already present in the source/MuonCalNTupleMaker/data/config/ folder.\nGlobal configuration The first part is related to the overall configuration parameters:\ntreeName: MuonMomentumCalibrationTree metaTreeName: MetaDataTree doHighpT: true addSmearVars: true The name of the tree is defined in the first two lines while the user options can be setted in the last two lines setting to true or false the doHighpT and addSmearVars.\nVariables configuration A regex matching is used to select which variable will be readed in the MuonPerformanceAnalysis ntuples and stored in the mini-ntuples. This is to make the code faster by reading less variables to memory and to speed-up also the Calibration and Sagitta Bias code. If even one matched condition is set to false, that variable will not be read. The checking happens in order the options are listed.\nAn example is shown below.\npattern: (.*), keep: true pattern: Trig(.*), keep: false pattern: Match(.*), keep: false pattern: (.*)CB_Chi2(.*), keep: false pattern: (.*)CB_NDoF(.*), keep: false pattern: (.*)CB_QoverP(.*), keep: false pattern: (.*)EnergyLoss(.*), keep: false pattern: (.*)EnergyLossSigma(.*), keep: false pattern: (.*)EnergyLossType(.*), keep: false pattern: (.*)ID_Chi2(.*), keep: false pattern: (.*)ID_NDoF(.*), keep: false pattern: (.*)ID_QoverP(.*), keep: false pattern: (.*)MCaST_RawCategory(.*), keep: false pattern: (.*)ME_Chi2(.*), keep: false pattern: (.*)ME_NDoF(.*), keep: false pattern: (.*)ME_QoverP(.*), keep: false pattern: (.*)MS_Chi2(.*), keep: false pattern: (.*)MS_NDoF(.*), keep: false pattern: (.*)MS_QoverP(.*), keep: false pattern: (.*)MSOE_Chi2(.*), keep: false pattern: (.*)MSOE_NDoF(.*), keep: false pattern: (.*)MSOE_QoverP(.*), keep: false pattern: (.*)NumberOfPrecisionLayers(.*), keep: true pattern: (.*)NumberOfGoodPrecisionLayers(.*), keep:true pattern: (.*)NumberOfPrecisionHoleLayers(.*), keep: false pattern: (.*)NumberOfPhiLayers(.*), keep: false pattern: (.*)NumberOfPhiHoleLayers(.*), keep: false pattern: (.*)NumberOfTriggerEtaLayers(.*), keep: false pattern: (.*)NumberOfTriggerEtaHoleLayers(.*), keep: false pattern: (.*)NumberOfContribPixelLayers(.*), keep: false pattern: (.*)Holes, keep: false pattern: (.*)Outliers, keep: false pattern: (.*)Fakes, keep: false pattern: (.*)ExpectBLayerHit, keep: false pattern: (.*)ExpectInnermostPixelLayerHit, keep: false pattern: (.*)ExpectNextToInnermostPixelLayerHit, keep: false pattern: (.*)NumberOfGangedPixels, keep: false pattern: (.*)NumberOfPixelDeadSensors, keep: false pattern: (.*)NumberOfSCTDeadSensors, keep: false pattern: (.*)NumberOfTRTHighThresholdHitsTotal, keep: false pattern: (.*)NumberOfTRTDeadStraws, keep: false pattern: (.*)NumberOfOutliersOnTrack, keep: false pattern: (.*)StandardDeviationOfChi2OS, keep: false pattern: (.*)AEOT_DeltaTrans, keep: false pattern: (.*)AEOT_DeltaAngle, keep: false",
    "description": "The config file is needed to setup the code. Possibilities are already present in the source/MuonCalNTupleMaker/data/config/ folder.\nGlobal configuration The first part is related to the overall configuration parameters:\ntreeName: MuonMomentumCalibrationTree metaTreeName: MetaDataTree doHighpT: true addSmearVars: true The name of the tree is defined in the first two lines while the user options can be setted in the last two lines setting to true or false the doHighpT and addSmearVars.",
    "tags": [],
    "title": "Configs",
    "uri": "/scale_resolution/muoncalntuplemaker/configs/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "The outputs of the MuonCalNTupleMaker are stored in the eos common space: /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/ Usually two subfolders are present, “minimal” and “standard”. The first one “minimal” contains the inputs used for all the codes while the second contains additional inputs and are used for tests. Use the first one to reduce the momery usage and the running time of the codes. Sometimes “calibrated” folders are present with the calibration injected (in the tree you can see variables with “_calib” in the name).\nRun Folder name Size Release/mc Notes 2 CalibrationJune2016_v05 264 MB 21/mc16 old tests 2 LeadingMuonSagittaTest 16 GB 21/mc16 Trees for the studies of Sagitta_LeadMuonOnly 2 Sagitta_LeadMuonOnly 178 MB 21/mc16 old studies about global bias studies 2 v56_MCPCALIB 2.5 GB 21/mc16 old samples of mc16 2 inputsForFinalRun2Calib 653 GB 21/mc16 histograms and trees used for final Run2 recommendation 2 muonptcalib_v08 2.1 TB 21/mc16 Run2 ntuples, same configuration of rel22 Run2 ntuples 2 merged_paper_inputs 2.4 GB 21/mc16 Used for the final Run2 paper 2 muonptcalib_rel22_231122_v02 1.7 TB 22/mc20 Run2 recommendations 2 Run2_Rel22_JPsi_trigger_mu18mu6 6.2 GB 22/mc20 new J/psi sample with different filter threshold (more stat wrt nominal) 2 SagittaMapForCalibTool 1.7 MB 22/mc20 Sagitta bias maps for Run2 rel22 + others 2 validation_rel22_run2 11 GB 22/mc20 Plots and histograms for the sagitta bias validation part in rel22 — — — — — 3 muonptcalib_Run3_T0_v2 155 GB 22/mc21 Run3 pre-recommendations 3 validation_data22 11 GB 22/mc21 files used for checks with the 2022 data 3 Run3_2022repro_67v31 435 GB 23/mc23 2022 data and mc samples 3 Run3_2023_67v27 518 GB 23/mc23 2023 data and mc samples 3 Run3_2023_67v53_MC23d 48 GB only mc23 mc samples for 2023 with new pileup weights 3 Run3_2022repro_67v75 243 GB 23/mc23 2022 reprocessed data with new alignment + mc23a, will be used for the new Run3 2022 consolidated recommendations 3 Run3_2023repro_67v75 432 GB 23/mc23 2023 reprocessed data with new alignment + mc23d, will be used for the new Run3 2023 consolidated recommendations 3 Run3_2024_Tier0 220 GB 23/only data 2024 data for the monitoring (only standard, standard_repro_data2024 contains the reprocessed 2024 data with new alignment) 3 Run3_2024_67v88 399 GB 23/only data 2024 data (first period reprocessed with new alignment) + mc23e, will be used for the new Run3 2024 preliminary recommendations — — — — — X MMCTutorial 33 MB X macros/samples used for the tutorial — — — — — In bold are highlighted the folder which contains the input for a particular recommendation.",
    "description": "The outputs of the MuonCalNTupleMaker are stored in the eos common space: /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/ Usually two subfolders are present, “minimal” and “standard”. The first one “minimal” contains the inputs used for all the codes while the second contains additional inputs and are used for tests. Use the first one to reduce the momery usage and the running time of the codes. Sometimes “calibrated” folders are present with the calibration injected (in the tree you can see variables with “_calib” in the name).",
    "tags": [],
    "title": "Muon Calibration common space",
    "uri": "/scale_resolution/mmccommonspace/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "Warning Please note that usage of MuonCalibrationAndSmearingTool and MuonCalibrationPeriodTool is not supported anymore, and these tools will be removed in the upcoming releases. Please use the MuonCalibTool tool, which as the same interface class as IMuonCalibrationAndSmearingTool. As such, technical usage will not change, just how the tool is setup.\nTo apply momentum corrections to reconstructed muons, the MuonMomentumCorrections package is used. The corresponding tool is CP::MuonCalibTool. The tool must be applied to both data and MC separately for each data-taking year, and calibrates MC momentum scale and resolution to match data. Furthermore, it allows to improve on data the residual momentum bias for μ+ vs μ- tracks (sagitta bias correction).\nFor analyses using the HighPt selection, please make sure you set the do2StationsHighPt property to true (available from Recs2020_03_03), unless you have explicitly disabled 2-station tracks in the HighPt selection by setting Use2stationMuonsHighPt to false in MuonSelectionTool.\nCP::MuonCalibTool tool The MuonCalibTool provides a way to automatically handle application of relevant correction for different mc and data/taking periods. It is available in the MuonMomentumCorrections package, see the header and source code.\nIt provides several user modes, which automatically configure the recommendations from MCP on momentum calibration. The user mode is configurable via the calibrationMode property, that can be set to MuonCalibTool::correctData_CB (0), MuonCalibTool::correctData_IDMS (1) or MuonCalibTool::notCorrectData_IDMS (2) MuonCalibTool::notcorrectData_CB (3). The current default option is noOption, forcing all analyses are required to make an explicit choice.\nNote PHYSLITE is using correctData_CB since the April 30th 2024 (Athena 25.0.5). Before the setup was notCorrectData_IDMS which was more conservative (this is not yet released for Run 3, so please use correctData_CB instead.\nNote If you are setting up the tool through a python config, the enums are not available and the user needs to specify the integer corresponding to the enum. These values are listed in the bracket in front of the enum option in the text above. See here an example of how doing the enum (to be updated).\nFew options of the tool can be configured for all user modes. For those related to the high pT smearing check, details can be found here. Please read this twiki for detailed checks that need to be performed for high pT analyses. The same tool is present also in the Release 22 (and above).\nTool Options Release (Default: Recs2025_03_26_Run2Run3). It is not recommended to explicitly set this. calibMode (Default: -1). see above. IsRun3Geo (Default: False). It should match the usage for MuonSelectionTool do2StationsHighPt (Default: False). For details, click here. doExtraSmearing (Default: False). For details, click here. This option is not available for Run3. For the current recommendation, there are targeted setups for analyses with different requirements. Please follow the priority ordered list below to get the recommended tool setting. If your analysis does not fall into one of these categories, please contact MCP.\nBlinded analyses that satisfy the high-pt working point requirements as defined here or predominantly select muons with pT \u003e 300 GeV: use the notCorrectData_IDMS setup for Run 2 only and notCorrectData_CB setup for Run 3. For Run3, please contact the MCP conveners and the Muon Momentum Calibration conveners. Unblinded/precision analyses that satisfy the high-pt working point requirements as defined here or predominantly select muons with pT \u003e 300 GeV: use the correctData_IDMS setup (Run 2 only). For Run3, please contact the MCP conveners and the Muon Momentum Calibration conveners. Analyses sensitive to the Z AFB modelling: This tool setup is currently not test in the tool, Please contact MCP if you would like this setup implemented Analyses limited by Muon resolution uncertainties: use the correctData_IDMS setup (Run 2 only). Precision analyses that are limited by Muon scale uncertainties: use the correctData_IDMS setup (Run2 only). All other analyses: use the correctData_CB setup (both Run2 and Run3). Warning For Release \u003e=22, the options notCorrectData_IDMS and correctData_IDMS were affected by a bug in the MuonCalibIntScaleSmearTool. It has been fixed since (see !74927, which was included in the Athena 25.0.20 release).\nTool Setup // Use the latest way from ASG to setup the tool. The following initializing is just an example. auto corrTool = new CP::MuonCalibTool(); corrTool-\u003esetProperty(\"calibMode\", calibMode ); corrTool-\u003esetProperty(\"IsRun3Geo\", isRun3Geo ); corrTool-\u003esetProperty(\"doExtraSmearing\", doExtraSmearing ); // see above corrTool-\u003esetProperty(\"do2StationsHighPt\", do2StationsHighPt ); // see above For expert usage, please contact MCP. All options that were orignally in the MCAST tool are still support, but the usage of these is not recommended, unless approved by MCP.\nProblem with hits/holes information on the combined muon track As discussed on ATLIDTRKCP-396, the information about inner detector hits/holes is not accurate when accessed via the combined muon track (e.g. using the muon.primaryTrackParticle() accessor for a combined muon). The correct information is obtained from the ID track (muon.trackParticle(xAOD::Muon::InnerDetectorTrackParticle)). Users should always rely on MuonSelectionTool for muon selection, which retrieves the correct information. However, be aware in case your analysis uses track hit information in any way beyond the application of MuonSelectionTool.",
    "description": "Warning Please note that usage of MuonCalibrationAndSmearingTool and MuonCalibrationPeriodTool is not supported anymore, and these tools will be removed in the upcoming releases. Please use the MuonCalibTool tool, which as the same interface class as IMuonCalibrationAndSmearingTool. As such, technical usage will not change, just how the tool is setup.\nTo apply momentum corrections to reconstructed muons, the MuonMomentumCorrections package is used. The corresponding tool is CP::MuonCalibTool. The tool must be applied to both data and MC separately for each data-taking year, and calibrates MC momentum scale and resolution to match data. Furthermore, it allows to improve on data the residual momentum bias for μ+ vs μ- tracks (sagitta bias correction).",
    "tags": [],
    "title": "Muon Momentum Corrections",
    "uri": "/guidelines/muonmomentumcorrections/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group",
    "content": "On this page, information for developers wishing to work on the MuonEfficiencyCorrections package is collected. Documentation for end users of the tool can be found here.\nProviding new input files The tool uses histograms stored in ROOT files as input. Any histograms of type TH1F, TH2F, TH3F, TH2Poly are supported.\nThe binning of the SF is determined by the axis binning of the histogram provided. The axis title is used to tell the tool which variable an axis is binned in. Examples are ‘pt’, ’eta’, ‘phi’, ’etaphi bin’, or ‘charge’. For a SF binned in eta and phi, the axis binned in eta would therefore be titled ‘muon eta’ and axis binned in phi would be titled ‘muon phi’.\nAt least two histograms need to be provided:\nOne histogram named SF, with the scale factor as bin contents and the statistical error on the scale factor as bin errors. One histogram named SF_sys, with the systematic error on the scale factor as bin contents. Optionally, the tool also supports Data and MC efficiencies. To provide (one of) them, the naming scheme below needs to be followed:\nOne histogram named Eff, with the efficiency measured in the data as bin contents and the statistical error on the efficiency as bin errors. One histogram named Eff_sys, with the systematic error on the efficiency measured in the data as bin contents. One histogram named MC_Eff, with the efficiency measured in MC as bin contents and the statistical error on the efficiency as bin errors. One histogram named MC_Eff_sys, with the systematic error on the efficiency measured in MC as bin contents. Furthermore, histograms can be ‘binned’ in time. This can be done by creating multiple versions of the histograms, with additional suffixes indicating the respective time period. For example, per-period SFs can be provided with the histograms ‘SF_A’, ‘SF_B’, …, ‘SF_sys_A’, ‘SF_sys_B’, … Similarly, per-run efficiencies would be provided with the histograms ‘Eff_200804’, ‘Eff_200812’, …, ‘Eff_sys_200804’, ‘Eff_sys_200812’, … In principle, any string can be used as suffix, as long as the same string is available in the LumiData tree, which is a TTree that is required to be provided in the same input file. It has one entry for each time period, and three branches:\nPeriod (string): name of time period used as suffix for the histograms (e.g. ‘A’, ‘B’, ‘200804’, ‘200812’, …) FirstRun (int): run number of the first run in the given time period (e.g. ‘200804’) LastRun (int): run number of the last run in the given time period (e.g. ‘300344’). It is recommended to set the LastRun of the last run/period in time to 999999 (open end) in order to use this one as pre-recommendation for future data taking. Based on this information, the tool will then automatically return the corresponding SF/efficiency by checking the randomRunNumber decorator of the xAOD::EventInfo class and propagate the systematic and statistical errors.",
    "description": "On this page, information for developers wishing to work on the MuonEfficiencyCorrections package is collected. Documentation for end users of the tool can be found here.\nProviding new input files The tool uses histograms stored in ROOT files as input. Any histograms of type TH1F, TH2F, TH3F, TH2Poly are supported.\nThe binning of the SF is determined by the axis binning of the histogram provided. The axis title is used to tell the tool which variable an axis is binned in. Examples are ‘pt’, ’eta’, ‘phi’, ’etaphi bin’, or ‘charge’. For a SF binned in eta and phi, the axis binned in eta would therefore be titled ‘muon eta’ and axis binned in phi would be titled ‘muon phi’.",
    "tags": [],
    "title": "MuonEfficiencyCorrections for Developers",
    "uri": "/efficiency/muonefficiencycorrections/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools",
    "content": "The MuonPerformanceAnalysis code is used by the MCP group produce the ntuples used by the Efficiency sub-group and the Momentum Scale and Resolution sub-group. The code can run directly on AOD files or on derivations DAOD files.\nSetup the code To setup the MuonPerformanceAnalysis code on lxplus or any other server with access to cvmfs follow these instructions\nmkdir MyWorkingArea # Create your working area cd MyWorkingArea # Move to your working area git clone --recursive ssh://git@gitlab.cern.ch:7999/atlas-mcp/MuonPerformanceAnalysis.git #Cloning the repository will also clone it's submodules mkdir run build\t# Create build and run folders cd build/ setupATLAS asetup Athena,23.0.35,here cmake ../MuonPerformanceAnalysis make -j cd ../run source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment The next time you wish to setup the code, it will be sufficient to setup AthAnalysis environment.\ncd MyWorkingArea/build/ asetup --restore cd ../run source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment Run the code python -m MuonPerformanceAlgs.runMPA --runMMC -i /ptmp/mpp/dcieri/MCP_samples/mc21_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.recon.AOD.e8453_s3873_r13829/AOD.29791402._000087.pool.root.1 Submit on the grid Setup the code as described above. Then, do:\nlsetup rucio panda pyami python MuonPerformanceAnalysis/MuonPerformanceAlgs/scripts/SubmitToGrid.py --ProdNumber \u003cName of the Production\u003e (--FilesPerJob 5) (--Test) (--official) --submit_streams MCS -i \u003cdataset or path to list containing all datasets\u003e Debugging the code To write printouts, use the ATH_MSG_DEBUG function. Then execute the code with the following flag\n--outputLevel 2\" The output level values are taken from https://gitlab.cern.ch/atlas/athena/-/blob/master/Control/AthenaCommon/python/Constants.py\nWorkflow concept The software is used the calibration group simply to make a dump in the ntuple of the relevant quantities used in the analysis. It select events with at least two combined muons (xAOD::Muon::MuonType::Combined) with\nquality \u003c Loose (xAOD::Muon::Loose, 2) $p_T \u003e 3\\ GeV$ $|\\eta| \u003c 2.75$ Then it select events in the following mass window $2.0 \u003c m_{\\mu\\mu}^{(J/\\psi)} \u003c 4.4\\ GeV$ $7.0 \u003c m_{\\mu\\mu}^{(\\Upsilon)} \u003c 13.0\\ GeV$ $60.0\\ GeV \u003c m_{\\mu\\mu}^{(Z)}$ Stored variabiles Trig\u003ctrig_name\u003e Match\u003ctrig_name\u003e EvtNumber EventWeight AverageMu ActualMu PileupWeight BeamSpotWeight d0Significance z0SinTheta neflowisol20 topoetcone20 ptcone20_TightTTVA_pt500 ptvarcone30_TightTTVA_pt500 ptvarcone30_TightTTVA_pt1000 Chi2 NDoF CalibPt TrackPars TrackCovMatrix EnergyLoss EnergyLossSigma EnergyLossType MCaST_RawCategory NumberOfPrecisionLayers NumberOfPrecisionHoleLayers NumberOfPhiLayers NumberOfPhiHoleLayers NumberOfTriggerEtaLayers NumberOfTriggerEtaHoleLayers NumberOfContribPixelLayers Hits Holes Outliers Fakes NumberOfGoodPrecisionLayers ExpectBLayerHit ExpectInnermostPixelLayerHit ExpectNextToInnermostPixelLayerHit NumberOfGangedPixels NumberOfPixelDeadSensors NumberOfSCTDeadSensors NumberOfTRTHighThresholdHitsTotal NumberOfTRTDeadStraws NumberOfOutliersOnTrack StandardDeviationOfChi2OS AEOT_DeltaTrans AEOT_DeltaAngle",
    "description": "The MuonPerformanceAnalysis code is used by the MCP group produce the ntuples used by the Efficiency sub-group and the Momentum Scale and Resolution sub-group. The code can run directly on AOD files or on derivations DAOD files.\nSetup the code To setup the MuonPerformanceAnalysis code on lxplus or any other server with access to cvmfs follow these instructions\nmkdir MyWorkingArea # Create your working area cd MyWorkingArea # Move to your working area git clone --recursive ssh://git@gitlab.cern.ch:7999/atlas-mcp/MuonPerformanceAnalysis.git #Cloning the repository will also clone it's submodules mkdir run build\t# Create build and run folders cd build/ setupATLAS asetup Athena,23.0.35,here cmake ../MuonPerformanceAnalysis make -j cd ../run source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment The next time you wish to setup the code, it will be sufficient to setup AthAnalysis environment.",
    "tags": [],
    "title": "MuonPerformanceAnalysis",
    "uri": "/commonsoftware/mpa/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group",
    "content": "The MuonTPPostProcessing package is used by the MCP group to calculate the reconstruction and identification efficiencies and scale factors, from the tag-and-probe ntuples produced with the MuonPerformanceAnalysis tool.\nWorkflow concept The efficiency is calculated by defining some basic probe selection, and then selecting some criteria, which is called match. Then, foreach variable for which we want to calculate the efficiency, we need to create a histogram for both probes and matches. The efficiency is then measured by the further processing of these histograms.\nThe MuonTPPostProcessing workflow is then divided in two parts:\nWriteTagAndProbeHistos: Runs over the T\u0026P ntuples and produces root files with the probe-match histograms. GeneratePlots: Analyses the histogram files, to calculate the efficiencies and scale-factors. Setup the code To setup the MuonTPPostProcessing package on lxplus or any other server with access to cvmfs follow these instructions\nmkdir MyWorkingArea # Create your working area cd MyWorkingArea # Move to your working area git clone --recursive ssh://git@gitlab.cern.ch:7999/atlas-mcp/MuonTPPostProcessing.git source # clone the repo mkdir run build # Create build and run folders asetup AthAnalysis,22.2.113,here # Setup AthAnalysis with the latest validated release cd build cmake ../source \u0026\u0026 make -j2 # Compile the code source x86*/setup.sh # Setup the environment The next time you wish to setup the code, it will be sufficient to setup AthAnalysis and the MuonTPPostProcessing environment.\ncd MyWorkingArea asetup AthAnalysis,22.2.113,here # Setup AthAnalysis with the latest validated release source build/x86*/setup.sh # Setup the environment Creating Tag\u0026Probe Histograms To produce the histograms required to calculate the efficiency, we need to define some settings, like the input files, the probe-and-match selection, and the actual variables. In the MuonTPPostProcessing, these information is steered using dedicated configuration files:\nInput Config files: They contain the location of the T\u0026P files and some auxillary files (e.g. GRL). For more details, see the dedicated page. Run Config files: They define the analysis stream to run, probe- \u0026 match-selections. For more info, check the dedicated page. Histo Config files: They define the variables piped into TH1/TH2 objects with proper labeling. More details, in the dedicated page. Default configuration files are already stored in the repository, in the data/ folder, for the default MCP analyses. Specific instructions on how to run them, are shown here.\nThese files have a dedicated syntax, which is described in the dedicated sections. You can also write comment blocks in the files, by using the # character. You can also include other config files recursively using the Import keyword, relatively to the main path of the repository. E.g.\nImport MuonTPPostProcessing/HistoConf/BasicHistosForZmumuReco.conf The ifdef statements MuonTPPostProcessing knows at the stage of the run-config \u0026 histo-config parsing whether it runs on data or MC. The IfDefHelpers allow the code to skip/load lines depending on the input. E.g.\n### The following lines are skipped on data input files ### The situation is reveresed using ifndef instead of ifdef ifdef isMC GlobalCut int int probe_truth_origin = 13 Import MuonTPPostProcessing/Cake.conf endif You can also define your own flags in the input config files, e.g.\ndefine my_flag And make later use of them in the run- \u0026 histo config files.\nifdef my_flag NewVar Type 1D Name flag_is_on Reader floatGeV tag_pt Template mytemplate EndVar endif Run the code To execute the code use WriteTagProbeHistos command.\nWriteTagProbeHistos -i \u003cinput_config\u003e -r \u003crun_config\u003e -h \u003chisto_config\u003e (--nevents \u003cn_events\u003e -o \u003coutput_file.root\u003e) To process full directories of input and run config with a single histo config file, use the SubmitToBatch script.\nSubmitToBatch.py −−jobName MCP_Tutorial −I \u003cinput folder/files\u003e −R \u003crun folder/files\u003e −H \u003chisto cfg\u003e (−−nFilesPerJob) −−engine SLURM/LOCAL/SGE/HTCONDOR The resulting ntuple will be then stored inside your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nCalculating Efficiencies and Scale Factors Once you produced your T\u0026P histogram files, you can finally use the python framework to produce efficiency plots and scale factors. For that, you need to execute the GeneratePlots.py macro on a chosen DSConfig file.\nTo produce efficiency plots use the following command,\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a \u003cAnalysis Stream\u003e -o \u003coutput_folder\u003e where possible Analysis stream are defined inside Defs.py.\nSimilarly, to produce Scale Factor files,\nCreateSF.py -a \u003cAnalysis Stream\u003e -c \u003cDSConfig_file\u003e --outputDir \u003coutput_folder\u003e --InCfgDir \u003cpath_to_input_config_file\u003e (--run3) More details on the plotting and scale-factors generation macro are in the dedicated section.",
    "description": "The MuonTPPostProcessing package is used by the MCP group to calculate the reconstruction and identification efficiencies and scale factors, from the tag-and-probe ntuples produced with the MuonPerformanceAnalysis tool.\nWorkflow concept The efficiency is calculated by defining some basic probe selection, and then selecting some criteria, which is called match. Then, foreach variable for which we want to calculate the efficiency, we need to create a histogram for both probes and matches. The efficiency is then measured by the further processing of these histograms.",
    "tags": [],
    "title": "MuonTPPostProcessing",
    "uri": "/efficiency/postprocessing/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "Muon Performance open tasks This section presents open tasks for people willing to contribute to MCP.\nMost of them can be used to define ATLAS Qualification Projects (AQPs). After finding an interesting topic, the procedure for defining an AQP is described in AQPs procedure. Completed tasks with links to the relevant documentation are presented in the dedicated section.\nThe previous version of this page can be found here. Muon Software tasks can be found here.\nThe aim of these studies is to achieve a deep understanding of all aspects related to muon reconstruction and performance with Run-2 and Run-3 data and MC. Many of the studies suggested here are expected to follow a timescale decoupled from recommendations timescales with the goal of improving data reconstruction and simulation, in collaboration with software experts.\nThese tasks can be turned into an ATLAS authorship qualification project (AQP). For more details please contact MCP conveners. The list is however not only intended for AQP tasks, and OTPs can be provided for already qualified people that will work on those. All tasks marked with the symbol are suitable for AQPs, all tasks marked with the symbol are suitable for contributions from physics analysis teams.\nShow tasks suitable for AQPs Show tasks suitable for physics analyses In the next sections, the following color codings are used:\nUncovered task, needs person-power (contact respective ID/Efficiency/Isolation/MMC convenors) Ongoing task, in need of person-power (contact respective ID/Efficiency/Isolation/MMC convenors) Covered task, doesn't need person-power at the moment General Tasks Performance studies for the phase-II muon system Develop WPs and study efficiency and resolution for the phase-II upgrade.\nContributors: Person power needed\nDevelopment of MCP ART tests Develop ART monitoring tests for MCP (which are currently outdated) and possibly harmonize as much as possible with PhysVal and DQA/DQM.\nContributors: Person power needed\nMuon Working Points Tasks Standard Working Points\nStudy and monitoring of selection efficiencies and qualities with evolving reconstruction changes and data-taking conditions, feedback to DQ, SW, and Detector groups Contributors: Peiyan Shaozho Hu ATLASMCP-247\nEstimated FTE: 0.3\nDevelopment and optimisation of CaloMuonScore WPs for high eta region A fraction of muons is not reconstructed by standard algorithms due to cabling/missing instrumentation in a specific parts of ATLAS detector (e.g. at eta ~0). In those instances, ID + calo information is used to tag on the muons to increase muon efficiency reconstruction. The CaloMuonScore is a new ML reconstruction algorithm based on a convolutional neural network trained on calorimeter energy deposits and that has been developed to replace old cut based MuTagIMO algorithm. It has been shown in Ricardo Woelker's thesis, that for eta below 1, this algorithm has a similar signal muon efficiency as the MuTagIMO but much better fake muon rejection in the region. Testing and validating the identical model for eta \u003e 1 has been done in the context of AQP ATLASMCP-224 by Lu Zheng. Nevertheless, as the segmentation and geometry of the calorimeter is very different for the eta \u003e 1, the network needs to be retrained and optimised.\" Contributors: Person power needed, Lu Zheng\nHigh-pT WP\nValidation \u0026 Tuning of High-pT WP Due to several changes of Muon Spectrometer system, the performance of the high pT WP needs to be optimised and validated for Run3. This is an extension of Elizaveta Cherepanova's studies who investigated inclusion of new BEE \u0026 BIS7/8 chambers. Contributors: Person power needed, Elizaveta Cherepanova\nEstimated FTE: 0.15\nDevelopment of hybrid medium/high-pT WP Most analyses use the medium working point (WP) for muon selection. However, for muons with transverse momentum above 100 GeV, the MCP recommends high-pT WPs due to limitations in the medium WP and its uncertainties. This task aims to develop a hybrid WP that applies the high-pT criteria to muons above 100 GeV and retains the medium WP for lower-pT muons, improving performance in analyses involving high-pT muons while preserving the standard selection elsewhere.\nContributors: Person power needed\nBad muon veto High pT muons are especially sensitive to the alignment of the muon system. Misalignment of the chambers may lead to increased resolution of the muons creating long tails. These pathological misreconstructed cases have been vetoed in Run2 as they can significantly affect various analyses focusing on highly energetic events with muons. Due to changes to ATLAS Muon system for Run3, this veto needs to be reoptimised.\nContributors: Stylianos Angelidakis\nLow-pT WP\nNew optimisation of Low-pT MVA WP, with new variables and new methods Optimisation of the low-pT MVA WP and applying DNN ML technique to investigate performance of the low pT muon reconstruction and validating it against the current MVA and cut based method. Contributors: Luca Pagani ATLASMCP-249\nMuon mis-id rate measurement Use HF hadron decays (B-\u003eJ/PsiK and D*-\u003eKpipi) to measure muon fake rate. Finished for Run2 / Rel. 21 by Fabiola Raffaeli. Development of same method in Run3 is needed.\nContributors: Person power needed\nMuon Efficiency Measurement Tasks Software Development and Maintenance\nSoftware maintainer \u0026 development of MCP analysis framework Contributors: Person power needed\nEstimated FTE: 0.25\nDevelopment of validation techniques for efficiency SFs Contributors: Person power needed\nEstimated FTE: 0.25\nProduction of T\u0026P and calibration n-tuples Contributors: Dimitris Iliadis\nEstimated FTE: 0.15\nMonitoring\nAutomating validation of recommendations Implement a set of automatic validation checks on efficiency scale factors and systematics, for example as unit tests\nContributors: Aryan Borkar\nEfficiency Measurements\nEfficiency SF production for standard ID and Isolation WPs Contributors: Person power needed\nEstimated FTE: 0.3\nEfficiency calibration for LRT muons Measure efficiency for LRT muons in \u003e=Rel.22 (joint task with the WP subgroup)\nContributors: Sagar, Laura, Max\nStudy of performance and efficiency of Forwards Muons Studying muon reconstruction efficiency and resolution of forward muons (in the eta region of 2.5 to 2.7 outside the Inner Detector acceptance) with the Run-3 dataset. Contributors: Celine Stauch ATLASMCP-215\nEstimated FTE: 0.3\nImprovements/Optimisation\nInvestigate CaloScore and new SegmentTagged probe definitions Contributors: Person power needed\nEstimated FTE: 0.25\nDiminish non-closure uncertainties in Z T\u0026P Contributors: Carolina Rossi\nEstimated FTE: 0.25\nDiminish non-closure uncertainties in J/Psi T\u0026P Contributors: Person power needed\nEstimated FTE: 0.2\nImprove robustness of T\u0026P measurement in low-statistic bins Contributors: Person power needed\nEstimated FTE: 0.3\n2D SF for Isolation Contributors: Arnav\nEstimated FTE: 0.3\nMultidimensional T\u0026P for Z and J/Psi Contributors: Person power needed\nEstimated FTE: 0.3\nImpact of pileup uncertainties in the efficiencies in T\u0026P and SF uncertainties Contributors: Person power needed\nEstimated FTE: 0.3\nMC-to-MC Isolation SFs Contributors: Lars, Simon\nEstimated FTE: 0.3\nHigh-pt bad muon veto systematics Contributors: Person power needed\nEstimated FTE: 0.2\nResearch and Development\nT\u0026P with Upsilon events Contributors: Person power needed\nEstimated FTE: 0.3\nIsolation Efficiencies with ttbar events Contributors: Person power needed\nEstimated FTE: 0.3\nMuon Isolation Tasks Standard isolation working points\nOptimisation/calibration of (standard) isolation WP Contributors: Person power needed\nEstimated FTE: 0.25\nDeveloping of PLIT in Run 3 Re-training of PLIT algorithm in Run 3 and re-define PLIT-based isolation WPs for muons.\nContributors: Person power needed\nRebuild of Prompt Lepton Improved Veto (PLIV) Contributors: Person power needed\nEstimated FTE: 0.3\nMuon Isolation and Trigger studies Study muon isolation as function of different offline triggers and investigate biases on muon fake rate. Contributors: Person power needed\nEstimated FTE: 0.3\nSoftware Development and Maintenance\nMaintenance of Overlap Removal tool Contributors: Person power needed\nEstimated FTE: 0.2\nMaintenance/validation of `IFFTruthClassifier` Contributors: Kenny\nEstimated FTE: 0.2\nMaintenance of PLIT framework (`LeptonDumper` and `TopCPToolkit`) Contributors: Ema, Zongpu, Stephen\nEstimated FTE: 0.2\nMaintenance of `FakeBkgTool` Contributors: Person power needed\nEstimated FTE: 0.2\nResearch and Development\nPLIT SF for leptonic taus This task NEEDS PLIT to be delivered first Contributors: Person power needed\nEstimated FTE: 0.2\nInvestigate impact of jet activity on muon reconstruction and isolation efficiencies Run-III Release 22 Contributors: Person power needed\nEstimated FTE: 0.3\nMitigate parton showering uncertainty on muon isolation WPs in Run-III Release 22 Contributors: Person power needed\nEstimated FTE: 0.3\nMuon Momentum/Scale Calibration Tasks Derivation of calibration constants pT calibration on data (sagitta bias corrections) estimating biases from data and producing corrections with existing machinery and status quo of improvements Contributors: Salvador Marti, Tamar Zakareishvili\nEstimated FTE: 0.1\nConsolidated scale and resolution calibration for 2022-2024 Contributors: Ehsan Musajan, Tamas Baer, Xingyu Wu\nEstimated FTE: 0.3\nData monitoring and preliminary scale/res calibration for 2025 data-taking The qualifier will be expected to utilise the MCP pipeline to develop the monitoring for 2025 data taking using simple observables such as the mean and RMS of the Z peak (this is temporarily covered by the MCP pipeline developed by Kevin Nelson). Once the 2025 data-taking has concluded, the qualifier will acquire relevant training for running the MuonCalibrationFit package, and derive a preliminary set of calibration constants. Contributors: Person power needed\nEstimated FTE: 0.3\nResearch and Development Physics Analysis Studies of high pT extrapolation of sagitta bias corrections Contributors: Person power needed\nEstimated FTE: 0.2\nResolution \u0026 bias studies with samples with specific detector effects: Distorted geometries, material changes, Inner tracker conditions, spectrometer conditions. Contributors: Person power needed\nEstimated FTE: 0.25\nStudy of ID-MS misalignment and its impact on data Study of ID-MS misalignment in simulation and data. The current mis-alignement between the two sub-detectors is of about 1mm and not well reproduced in simulation. This degrades performance of the CB fit in several ways. It would be important to estimate: a better mapping of the effect in data, a clear assessment of the impact on performance, the reduction of the effect using dedicated corrections. Contributors: Person power needed\nDedicated calibration for Large Radius Tracks (LRT) muons Contributors: Person power needed\nEstimated FTE: 0.2\nEstimation of sagitta bias using J/ψ → μμ events This task aims to apply the sagitta bias estimation method, which currently uses Z → μμ events, to J/ψ → μμ decays. Due to its narrower natural decay width, the J/ψ resonance provides a sharply peaked invariant mass distribution that is potentially more sensitive to charge-dependent curvature biases, particularly in the low-pT regime. The goal is to evaluate δs(η, φ) using J/ψ events and compare them to those obtained from Z decays, cross-checking and extending the calibration into complementary momentum ranges. Challenges include lower statistics at high pT, potential background contamination, and trigger efficiency effects. Contributors: Person power needed\nEstimated FTE: 0.2\nImprovements for scale/resolution calibration This is an umbrella task for many things that can be persued for the scale/resolution calibration. This includes: Inclusion of Jpsi reweighing Investigation of calibration onto truth variables Study the impact of dr2 parameter on MS fit Contributors: Person power needed\nEstimated FTE: 0.6\nOptimization of muon momentum resolution for ID and MS tracks Currently using and effective per-event linear combination of relative weights before corrections, assuming the relative weights remain unchanged. Previous investigations have shown that this entails a small uncertainty inducing the need of extra smearing and / or systematic. The study of more precise statistical combinations should allow to reduce these effects. Contributors: Zhenyu Zhao ATLASMCP-250\nMuon momentum resolution studies for high-pT muons The doExtraSmearing tool should be used by analyses that use Medium ID WP and have a non-negligible fraction of high pT muons. The recommendations are to use the HighPt ID WP in these cases. The tool serves to understand the impact of the muons passing the Medium but not the HighPt ID WP by adding additional smearing for these muons. However, the tool was developed in Rel21 Run2 and is currently not available for Run3. Contributors: Mustapha Biyabi ATLASMCP-225\nSoftware and computing Automated CI pipeline for calibration constants One of the main bottlenecks for releasing the recommendations is the systematics tuning. It is therefore useful to develop an automated pipeline that produces plots via the validation framework everytime the calibration constants are updated. Contributors: Person power needed\nEstimated FTE: 0.3\nSoftware development of validation framework A new validation framework (CATValidation) is being developed to replace the MomentumValidation framework for faster processing \u0026 more robust fitting procedure. Help will be appreciated for the following: Sagitta bias integration Inclusion of Upsilon Do get in touch if you want to help! Contributors: Siyuan Yan\nEstimated FTE: 0.3\nSoftware development of calibration framework The current framework for deriving scale/resolution constants, the MuonCalibrationFit framework has numerous problems: it's iterative, computationally expensive and possibly unstable due to the local minima spilling over to subsequent iterations. We're looking into an alternative method for a simultaneous fit for all calibration parameters via polynomial regression, and other ideas/suggestions are very welcomed! Contributors: Giacomo Artoni\nEstimated FTE: 0.3\nMini-ntuple production and data monitoring Run the mini-ntuple software to get the inputs directly from the group-perf disk and run on them using condor Contributors: Person power needed\nEstimated FTE: 0.1",
    "description": "Muon Performance open tasks This section presents open tasks for people willing to contribute to MCP.\nMost of them can be used to define ATLAS Qualification Projects (AQPs). After finding an interesting topic, the procedure for defining an AQP is described in AQPs procedure. Completed tasks with links to the relevant documentation are presented in the dedicated section.\nThe previous version of this page can be found here. Muon Software tasks can be found here.",
    "tags": [],
    "title": "Open tasks",
    "uri": "/open-tasks/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools",
    "content": "This page lists the official production of ntuples used:\nfor efficiency measurements and isolation studies as well as to provide the scale factors for these measurements and for momentum calibration purposes The MuonPerformanceAnalysis (MPA) framework is used to produce these ntuples.\nThe ntuples used for efficiency and isolation measurements have the suffixes NTUP_MCP and EXT0 (when the production is done on the Grid) The ntuples used for momentum calibration have the suffixes NTUP_MCPSCALE and EXT1 (when the production is done on the Grid) The efficiency and isolation measurements are using data from “physics_Main”, “physics_BphysDelayed” and “calibration_BPhysPEB” streams. The momentum calibration measurements are using data from “physics_Main” and “physics_BphysDelayed” streams. During datataking, the above mentioned ntuples are produced automatically at Tier-0 (so only for real data) and copied on the dedicated eos area of mcp group. For the mc production, Grid facilities are used but the ntuples are copied at the same place. The rucio storage element (RSE) where the mcp ntuples are replicated is the following: /eos/atlas/atlasgroupdisk/perf-muons/ *Run3 data and MC production to be used for providing recommendations * Date \u0026 Place of Production Input Datasets Output NTUPLES (precise rucio patterns) MPA version Athena release Notes (inc. relevant jira tickets) Purpose of production June 2025 (Grid) data23 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.100_25.34_v2 | grep EXT0” 67.100 25.0.34 Build with TriggerMatchingTool match variable set to true Run3 Year 2023 Include continuous SF variables June 2025 (Grid) mc23d Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 67.100_25.34_v2 | grep 15224 | grep EXT0” 67.100 25.0.34 Build with TriggerMatchingTool match variable set to true Run3 Year 2023 Include continuous SF variables June 2025 (Grid) mc23d Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 67.100_25.34_v2 | grep 15530 | grep EXT0” 67.100 25.0.34 Build with TriggerMatchingTool match variable set to true Run3 Year 2023 Include continuous SF variables February 2025 (Grid) mc23e (high pT fix - only efficiency) Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 16083 | grep 67.91 | grep 25.23 | grep EXT0” 67.91.0 25.0.23 BIS78, BEE included in high-pT selection Run3 Year 2024 Preliminary recommendations December 2024 (Grid) mc23e Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 16083 | grep 67.88 | grep 25.23 | grep EXT1” 67.88.0 25.0.23 T\u0026P efficiency ntuples are obsolete - use the high-pT fix instead (Feb. 2025) Run3 Year 2024 Preliminary recommendations November 2024 (Grid) mc23d Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15224 | grep 67.75 | grep 25.0.10 | grep EXT0 | grep -v calmuons”, “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15530 | grep 67.75 | grep 25.0.10 | grep EXT0 | grep -v calmuons” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15224 | grep 67.75 | grep 25.0.10 | grep EXT1 | grep -v calmuons”, “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15530 | grep 67.75 | grep 25.0.10 | grep EXT1 | grep -v calmuons” 67.75.0 25.0.10 - Run3 Year 2023 Consolidated recommendations November 2024 (Grid) Data 2023, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.75_25.0.10_v2 | grep physics_Main | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.75_25.0.10_v2 | grep physics_Main | grep EXT1” 67.75.0 25.0.10 - Run3 Year 2023 Preliminary recommendations November 2024 (Grid) Data 2023, stream: BphysDelayed Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.75_25.0.10_v2 | grep BphysDelayed | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.75_25.0.10_v2 | grep BphysDelayed | grep EXT1” 67.75.0 25.0.10 - Run3 Year 2023 Preliminary recommendations November 2024 (Grid) Data 2022, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.75 | grep 25.0.10 | grep physics_Main | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.75 | grep 25.0.10 | grep physics_Main | grep EXT1” 67.75.0 25.0.10 - Run3 Year 2022 Consolidated recommendations November 2024 (Grid) Data 2022, stream: BphysDelayed Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.75.0_athena25.0.10 | grep BphysDelayed | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.75.0_athena25.0.10 | grep BphysDelayed | grep EXT1” 67.75.0 25.0.10 - Run3 Year 2022 Consolidated recommendations October 2024 (Grid) Data 2015, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data15 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data15 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2016, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data16 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data16 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2017, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2018, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep physics_Main | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2016, stream: BphysDelayed Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data16 | grep BphysDelayed | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data16 | grep BphysDelayed | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2017, stream: BphysPEB Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep BphysPEB | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep BphysPEB | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2018, stream: BphysPEB Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep BphysPEB | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep BphysPEB | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2017, stream: BhysLS Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep BphysLS | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data17 | grep BphysLS | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) Data 2018, stream: BphysLS Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep BphysLS | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data18 | grep BphysLS | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) mc20a Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13167 | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13167 | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) mc20d Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13144 | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13144 | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) mc20e Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13145 | grep 67.81.0_25.0.18 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc20 | grep 13145 | grep 67.81.0_25.0.18 | grep EXT1” 67.81.0 25.0.18 DAOD_MUON1 Run2 Final recommendations October 2024 (Grid) mc23a Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 14622 | grep 67.75 | grep 25.0.10 | grep EXT0”, “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15540 | grep 67.75 | grep 25.0.10 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 14622 | grep 67.75 | grep 25.0.10 | grep EXT1”, “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep mc23 | grep 15540 | grep 67.75 | grep 25.0.10 | grep EXT1” 67.75.0 25.0.10 - Run3 Year 2022 Consolidated recommendations September 2024 (Grid) Data 2024, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep physics_Main | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep physics_Main | grep EXT1” 67.75.0 25.0.10 includes everything up to run 480277, for later runs use the Tier-0 ntuples Reprocessed datasets with updated alignment conditions (CONDBR2-BLKPA-2024-03) September 2024 (Grid) Data 2024, stream: BphysDelayed Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep Delayed | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep Delayed | grep EXT1” 67.75.0 25.0.10 includes everything up to run 480277, for later runs use the Tier-0 ntuples Reprocessed datasets with updated alignment conditions (CONDBR2-BLKPA-2024-03) September 2024 (Grid) Data 2024, stream: Bphys_PEB Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep PEB | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.75.0_athena25.0.10_fixed | grep PEB | grep EXT1” 67.75.0 25.0.10 - Reprocessed datasets with updated alignment conditions (CONDBR2-BLKPA-2024-03) July 2024 (Grid) Data 2023, stream: physicsMain Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.70.0_athena24.0.23 | grep data23 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.70.0_athena23.0.23 | grep data23 | grep EXT1” 67.70.0 23.0.23 - Trigger NSW bugfix checks July 2024 (Grid) Data 2022, stream: physicsMain Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.70.0_athena24.0.23 | grep data22 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.70.0_athena23.0.23 | grep data22 | grep EXT1” 67.70.0 23.0.23 - Trigger NSW bugfix checks May 2024 (Grid) mc23_13p6TeV.601190.PhPy8EG_AZNLO_ Zmumu.merge.AOD.e8514_e8528_ s4162_s4114_r15515_r15516 mc23_13p6TeV.601190.PhPy8EG_AZNLO_ Zmumu.merge.AOD.e8514_e8528_ s4159_s4114_r15513_r15514 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.64.0_athena25.0.5 | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.64.0_athena25.0.5 | grep EXT1” 67.64.0 25.0.5 - Trigger NSW bugfix checks April 2024 (Grid) MC23d Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.53.0_athena24.0.27 | grep mc23_13p6TeV | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.53.0_athena24.0.27 | grep mc23_13p6TeV | grep EXT1” 67.53.0 24.0.27 mc23d filelist To be used for checks December 2023 (Grid) Data 2022, stream: physics_Main Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data22 | grep Main | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data22 | grep Main | grep EXT1” 67.37.0 24.0.16 full reprocessing of data22 with release 23.0.20 (see details in JIRA) Run3 Year 2022 Consolidated recommendations November 2023 (Grid) Data 2022, stream: physics_BphysDelayed Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data22 | grep Delayed | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data22 | grep Delayed | grep EXT1” 67.37.0 24.0.16 full reprocessing of data22 with release 23.0.20 (see details in JIRA) Run3 Year 2022 Consolidated recommendations April 2024 (Grid) Data 2022, stream: Bphys_PEB Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.37.0_athena24.0.16 | grep PEB | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data22 | grep 67.37.0_athena24.0.16 | grep PEB | grep EXT1” 67.37.0 24.0.16 full reprocessing of data22 with release 23.0.20 (see details in JIRA) Run3 Year 2022 Consolidated recommendations December 2023 (Grid) mc23a to be used with data2022 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.31.0_athena24.0.16 | grep mc23_13p6TeV | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.31.0_athena24.0.16 | grep mc23_13p6TeV | grep EXT1” 67.31.0 24.0.16 mc23a filelist Run3 Year 2022 Consolidated recommendations November 2023 (Grid) Data 2023, stream: physics_Main period F run range: 451094 - 455924 period G run range: 455975 - 456749 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data23 | grep Main | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data23 | grep Main | grep EXT1” 67.37.0 24.0.16 dedicated reprocessing of data23 period F only with release 23.0.39 (see details in JIRA) to be combined with period G runs Tier-0 original processing Run3 Year 2023 Preliminary recommendations November 2023 (Grid) Data 2023, stream: Bphys_Delayed period F run range: 451094 - 455924 period G run range: 455975 - 456749 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data23 | grep Delayed | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0_athena24.0.16 | grep data23 | grep Delayed | grep EXT1” 67.37.0 24.0.16 dedicated reprocessing of data23 period F only with release 23.0.39 (see details in JIRA) to be combined with period G runs Tier-0 original processing Run3 Year 2023 Preliminary recommendations April 2024 (Grid) Data 2023, stream: Bphys_PEB period F run range: 451094 - 455924 period G run range: 455975 - 456749 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.37.0_athena24.0.16 | grep PEB | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep data23 | grep 67.37.0_athena24.0.16 | grep PEB | grep EXT1” 67.37.0 24.0.16 dedicated reprocessing of data23 period F only with release 23.0.39 (see details in JIRA) to be combined with period G runs Tier-0 original processing Run3 Year 2023 Preliminary recommendations November 2023 (Grid) mc23c to be used with data2023 Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.37.0 | grep 24.0.16_bugfix | grep EXT0” Mom Calibration: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep 67.27.0 | grep EXT1” 67.37.0, 67.27.0 24.0.16, 24.0.15 mc23c filelist Run3 Year 2023 Preliminary recommendations *Run2 release 22 data and MC production used for providing recommendations * Date \u0026 Place of Production Input Datasets Output NTUPLES (precise rucio patterns) MPA version Athena release Notes (inc. relevant jira tickets) Purpose of production 2021 (Grid) BphysPEB (17+18) BphysDelayed (16) BphysLS (17+18) physicsMain (15+16+17+18) mc20a mc20d mc20e Tag\u0026Probe: “rucio list-datasets-rse CERN-PROD_PERF-MUONS | grep v66.16.1” 66.16.1 23.0.4 BPhysPEB filelist BphysDelayed filelist BphysLS filelist physics_Main filelist mc20a filelist mc20d filelist mc20e filelist Final rel22 Run2 recommendations 2021 (Grid) BphysDelayed (16) physicsMain (15+16+17+18) mc20 Mom Calibration: - 66.21.0 23.0.4 BphysDelayed filelist physics_Main filelist mc20 filelist Final rel22 Run2 recommendations",
    "description": "This page lists the official production of ntuples used:\nfor efficiency measurements and isolation studies as well as to provide the scale factors for these measurements and for momentum calibration purposes The MuonPerformanceAnalysis (MPA) framework is used to produce these ntuples.\nThe ntuples used for efficiency and isolation measurements have the suffixes NTUP_MCP and EXT0 (when the production is done on the Grid) The ntuples used for momentum calibration have the suffixes NTUP_MCPSCALE and EXT1 (when the production is done on the Grid) The efficiency and isolation measurements are using data from “physics_Main”, “physics_BphysDelayed” and “calibration_BPhysPEB” streams. The momentum calibration measurements are using data from “physics_Main” and “physics_BphysDelayed” streams.",
    "tags": [],
    "title": " MuonPerformanceAnalysis ntuple's production",
    "uri": "/commonsoftware/productions/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MuonCalibrationFit",
    "content": "Fit output Web display",
    "description": "Fit output Web display",
    "tags": [],
    "title": "Calibration results and what to do with them",
    "uri": "/scale_resolution/muoncalibrationfit/results/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "MuonSelectionTool is a tool provided by the MCP group to select good quality muons for physics analysis. The tool selects muons based on (configurable):\nη cut identification quality (as defined in the Loose, Medium, Tight, HighPt, and LowPt working points) ID track requirements Tool usage in Release 21 The guideline for release 21 can be seen in the MCP twiki page\nTool usage in Release 22 Create and configure an instance of the tool The tool can be configured via a set of properties. In Athena, this is simply done in the JobOptions. In AnalysisBase, you can use the SetProperty method. Available Properties:\nMaxEta: An upper cut on |η| (default is 2.7). MuQuality: The chosen working point, see below. UseBEEBISInHighPtRun3: Allow the usage of the BIS78 and BEE chambers in the HighPt WP. This should be true for the reprocessed datasets (see guidelines) and false for the old processing. Supported working points The tool can be configured to six different selection levels: Tight, Medium, Loose, VeryLoose, HighPt, LowPt, corresponding to the MuQuality property set to 0, 1, 2, 3, 4 and 5 (default is 1 - Medium quality). The Tight selection is a subset of Medium, which in turn is a subset of Loose. The HighPt selection is nominally a subset of Medium, but when used in conjunction with the bad muon veto (see dedicated instructions below) this relationship is broken, as Medium and HighPt come with different implementations of the bad muon veto. Note that VeryLoose is not recommended for physics analysis. It should not be thought of as a working point, but rather as a way to classify muons that pass some basic pre-selection but fail the Loose working point requirements, which is useful for example in derivation skimming.\nCommon requirement on the ID tracks A series of quality requirements are applied to the ID tracks used for Combined, Segment Tagged, and Calotagged muons. They are designed to ensure a minimum number of hits in each of the ID sub-systems, hence selecting only reasonably well reconstructed tracks. The requirements are applied in the region |η|\u003c2.5 and consist of:\nat least 1 Pixel hit; at least 5 SCT hits; less than 3 Pixel or SCT holes; Run 2 Working Points The main selection criteria for Run2 for the different working points are summarized in the table below. For more details, have a look at the support note or the code, or contact MCP. The LowPt MVA WP is not supported for the time being, as it will require a retraining.\nTight Quality Combined nprecisionLayers \u003e 1 AND combined fit chi2/Ndof \u003c 8 AND η/pT dependent cuts on qOverP significance and ID/ME/CB momentum imbalance Medium Quality Combined qOverP significance \u003c 7 AND ( nprecisionLayers \u003e 1 OR ( nprecisionLayers == 1 AND nprecisionHoleLayers \u003c 2 AND |η|\u003c0.1 ) ) SiliconForward |η| \u003e 2.5 AND nprecisionLayers \u003e 2 StandAlone |η| \u003e 2.5 AND nprecisionLayers \u003e 2 Loose Quality Combined As for Medium OR ( pT \u003c 7 GeV AND |η| \u003c 1.3 AND precisionLayers \u003e 0 AND isAuthor(MuGirl) AND isAuthor(MuTagIMO) ) StandAlone \u0026 SiFwd As for Medium CaloTagged |η| \u003c 0.1 AND ( passes CaloScore WP4) SegmentTagged |η| \u003c 0.1 High-pT Quality Combined nprecisionLayers \u003e 1 (except EM+EO. require middle for == 2, or require inner+middle for \u003e 2) with non-overlapping small/large MS sectors AND veto on specific MS regions AND qOverP significance \u003c 7 Low-pT Quality Combined { isAuthor(MuidCo) OR [ isAuthor(MuGirl) AND isAuthor(MuTagIMO) ] } AND { nprecisionLayers \u003e 1 for |η| in 1.3 - 1.55, otherwise nprecisionLayers \u003e 0 } AND { MBS \u003c 3 AND SCS \u003c 3 AND SNS \u003c 3 } Note CaloScore WP4 corresponds to a cut on the CaloScore discriminant of:\nfor muons with pT\u003c20 GeV: CaloMuonScore \u003e= (-1.80e-4 * pT^3 +5.02e-3 * pT^2 -4.62e-2 * pT + 1.12) for muons with pT\u003e=20 GeV: CaloMuonScore \u003e= 0.77 Run 3 Working Points For usage on run3 data/MC samples the MuonSelectionTool must be configured setting the IsRun3Geo property to True, to avoid getting a crash.\nThe Supported WPs for Run 3 are summarised in the table below.\nTight Quality Combined nprecisionLayers \u003e 1 AND combined fit chi2/Ndof \u003c 8 AND η/pT dependent cuts on qOverP significance and ID/ME/CB momentum imbalance Medium Quality Combined qOverP significance \u003c 7 AND ( nprecisionLayers \u003e 1 OR ( nprecisionLayers == 1 AND nprecisionHoleLayers \u003c 2 AND |η|\u003c0.1 ) ) SiliconForward Not yet supported StandAlone Not yet supported Loose Quality Combined As for Medium OR ( pT \u003c 7 GeV AND |η| \u003c 1.3 AND nprecisionLayers \u003e 0 AND isAuthor(MuGirl) AND isAuthor(MuTagIMO) ) StandAlone \u0026 SiFwd As for Medium CaloTagged |η| \u003c 0.1 AND ( passes CaloScore WP4) SegmentTagged |η| \u003c 0.1 High-pT Quality Combined nprecisionLayers \u003e 1 (require middle for == 2, or require inner+middle for \u003e 2) with non-overlapping small/large MS sectors AND veto on specific MS regions AND qOverP significance \u003c 7 Low-pT Quality Combined { isAuthor(MuidCo) OR [ isAuthor(MuGirl) AND isAuthor(MuTagIMO) ] } AND { nprecisionLayers \u003e 1 for |η| in 1.3 - 1.55, otherwise nprecisionLayers \u003e 0 } AND { MBS \u003c 3 AND SCS \u003c 3 AND SNS \u003c 3 } Usage of NSW in identification NSW can be used as a precision layer in identification if\nyour dataset contains the NSW hit information MuonSelectionTool recalculates nprecisionLayers with NSW. release and p-tag of DAOD \u003e=24.2.24 and p-tag \u003e= p5834 You can use NSW as a precision layer. Endcap HighPt WP is enabled. \u003e=24.2.24 and p-tag \u003c p5834 NSW is not counted as a precision layer. Endcap HigPt WP is disabled. You may see an error when running MuonSelectionTool. To avoid the error, ExcludeNSWFromPrecisionLayers=True and RecalcPrecisionLayerswNSW=False \u003c 24.2.24 NSW is not counted as a precision layer. Endcap HigPt WP is disabled. Usage of the HighPt selection Any analysis selecting a non-negligible fraction of muons with pT\u003e300 GeV should use the HighPt selection, or explicit approval from the MCP conveners will be required before it can proceed to publication.\nFactors that can help determining whether a looser selection could be adequate for a given analysis or not:\nif muon momentum resolution systematics have a non-negligible impact on the analysis, as determined from the contribution to the total uncertainty on any measured quantity, from the impact on the exclusion limits, and in terms of “systematics ranking“, it is mandatory to use the HighPt selection. If the analysis’ results are sensitive to the possible occurrence of “outliers”, i.e. muon tracks with significantly mismeasured momentum, it is mandatory to use the HighPt selection. In case of doubts regarding points 1. and 2. above, or for any other specific situation, the details of the analysis need to be discussed with MCP, which will then guide and approve the choice of the most appropriate selection. Opting for an hybrid selection, such as for example “ HighPt for pT\u003eX otherwise Medium, with X\u003c1 TeV in any case ”, could be a possibility in some specific cases. These mainly consist of analyses that want to avoid “outliers”, are not so concerned about obtaining optimal muon momentum resolution, and are not impacted by muon momentum resolutions systematics, as defined in point 1 above.\nNote If you’re using the HighPt selection in Run2, please remember to turn on the do2StationsHighPt property (instructions are here) of the CP::MuonCalibrationAndSmearingTool. This option is not supported in Run3 yet.\nNote To perform the checks necessary to establish what indicated in points 1. and 2. above, the doExtraSmearing property (instructions are here) of the CP::MuonCalibrationAndSmearingTool has to be turned on. Once a decision has been taken on whether or not to use the HighPt selection, the doExtraSmearing property should be set to False. The extra smearing it provides is NOT a supported calibration, but rather a simple way to perform the checks indicated in points 1. and 2. above.\nLowPt working point The LowPt working point is designed to recover efficiency for tracks with pT\u003c5 GeV w.r.t. the other selection levels, at the cost of a moderate increase in the fake rate. The preliminary design concepts are detailed in the following presentations: MCP-19-JUL-2017 and MCP-26-JUL-2017.\nOne of the variables used in the LowPt selection is missing from many of the derivations with p-tag \u003c= p3263; if you’re impacted, please switch to more recent derivations!\nisBadMuon Flag: Event Veto Based on MC studies, muons that are flagged as “bad” tend to have significantly worse momentum resolution than those that pass the veto and in most cases exhibit pathological behavior. The occurrence of such muons is very rare, but a single mis-reconstructed muon can have a non-negligible impact on specific analyses (e.g. high-mass exotic resonances, SUSY searches). For both Run2 and Run3 analyses using high-Pt muons, the recommendation is therefore to veto the event if any muon passing the HighPt WP in the signal selection is marked as bad by the method. In case of doubts, please contact the MCP group. Please note that the veto is currently not optimised for Run3 and the work is in progress.\nWhen configured for HighPt WP muons, the method flags a muon as “bad” if it has a large relative error from the q/p measurement of the CB track. In this case, the veto inefficiency depends on the muon pT and eta, with inefficiency averaging around ~1% for 1 TeV tracks, and increasing with pT. Finally, the method can also be applied on Loose, Medium, and Tight WP muons, though it is not recommended. In that case, the method returns true if the relative error of the CB q/p measurement exceeds (within some tolerance) the corresponding errors from the ID and ME track fits. The veto efficiency remains above 99% for muon pT values up to a few TeV.\nLRT Muons For analyses using muons reconstructed from the Large Radius Tracking chain a.k.a. LRT muons, WPs are made available for both Run-2 and Run-3 conditions. The WP definitions are identical to what is used for standard muons, with the exception that no cuts are applied on the Inner Detector Track Particle of the muon since the LRT track reconstruction is already very restrictive in its definition to reject fakes. There are two standard and recommended ways of processing LRT muons:\nThe first way is a standalone treatment of only LRT muons. In this case, the muons from the MuonsLRT container are used as inputs, and the MuonSelectionTool expects that all muons have the patternRecoInfo available which is then used within the code. The second way is a the combined treatment of standard and LRT muons. The MuonLRTMergingAlg combines the two containers and performs an overlap removal based on a user configurable property (see https://gitlab.cern.ch/atlas/athena/-/tree/main/PhysicsAnalysis/MuonID/MuonIDAnalysis/LRTMuonAnalysisTools). The muons in the combined container are decorated with an isLRT flag which reflects the origin container, either standard (isLRT==0) or LRT (isLRT==1). This flag is then used by the MuonSelectionTool to then correctly apply the WP cuts on the muon object (the patternRecoInfo can be used in this case as well as a fallback). To configure the tool to run over LRT muons, set the UseLRT property to true. The other properties should be set nominally as done for standard muons.\nTechnical instructions for AnalysisBase #include \"MuonSelectorTools/MuonSelectionTool.h\" //Tool initialization CP::MuonSelectionTool m_muonSelection(\"MuonSelection\"); m_muonSelection.msg().setLevel( MSG::INFO ); //set to MSG::VERBOSE or MSG::DEBUG for debugging m_muonSelection.setProperty( \"MaxEta\", 2.5 ); m_muonSelection.setProperty( \"MuQuality\", 1); CHECK (m_muonSelection.initialize().isSuccess()); //In the event loop, check whether to select a given muon using the accept function if(!m_muonSelection.accept(**mu_itr)) continue; Technical instructions for Athena # Create a MuonSelectionTool if we do not yet have one from AthenaCommon.AppMgr import ToolSvc if not hasattr(ToolSvc,\"MyMuonSelectionTool\"): from MuonSelectorTools.MuonSelectorToolsConf import CP__MuonSelectionTool ToolSvc += CP__MuonSelectionTool(\"MyMuonSelectionTool\") ToolSvc.MyMuonSelectionTool.MaxEta = 2.5",
    "description": "MuonSelectionTool is a tool provided by the MCP group to select good quality muons for physics analysis. The tool selects muons based on (configurable):\nη cut identification quality (as defined in the Loose, Medium, Tight, HighPt, and LowPt working points) ID track requirements Tool usage in Release 21 The guideline for release 21 can be seen in the MCP twiki page\nTool usage in Release 22 Create and configure an instance of the tool The tool can be configured via a set of properties. In Athena, this is simply done in the JobOptions. In AnalysisBase, you can use the SetProperty method. Available Properties:",
    "tags": [],
    "title": "Muon Selection Tool",
    "uri": "/guidelines/muonselectiontool/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "Standard references for publications Analysis Reference Analyses using 13 TeV data (momentum corrections) Studies of the muon momentum calibration and performance of the ATLAS detector with pp collisions at sqrt(s)=13!TeV arXiv: 2212.07338, Eur. Phys. J. C 83 (2023) 686, Material Analyses using 13 TeV data (Identification, efficiency measurement) Muon reconstruction and identification efficiency in ATLAS using the full Run 2 pp collision data set at sqrt(s)=13 TeV arXiv:2012.00578, Eur. Phys. J. C 81, 578 (2021), Material Analyses using 13 TeV data (fake non prompt lepton background estimation) collaboration of egamma and mcp groups Tools for estimating fake/non-prompt lepton backgrounds with the ATLAS detector at the LHC arXiv:2211.16178JINST 18 2023 T11004 Analyses using 13 TeV data (Momentum scale/resolution corrections) Muon reconstruction performance of the ATLAS detector in proton–proton collision data at sqrt(s)=13 TeV arXiv:1603.05598, Eur. Phys. J. C 76 (2016) p. 292, Material Analyses using 7 and 8 TeV data (2011 and 2012) Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton-proton collision data arXiv:1407.3935, Eur. Phys. J. C 74 (2014) p. 3130, Material Analyses using 7 and 8 TeV data (2010 only) Muon reconstruction efficiency and momentum resolution of the ATLAS experiment in proton-proton collisions at sqrt{s}=7 TeV in 2010 arXiv:1404.4562, Eur. Phys. J. C 74 (2014) p. 3034, Material The corresponding BibTex can be found here.\nUseful material on the Muon Spectrometer, Muon Reconstruction and Muon Calibration Results Reference Muon Reco and Selections Internal note of Eur. Phys. J. C 81, 578 (2021) publication. A detailed description of the Muon Reconstruction and Selection is present. Int note Muon Spectrometer ATLAS Lecture, Oliver Kortner. Detailed presentation on the MS structure Muon reconstruction and performance ATLAS Lecture, Luca Martinelli. Detailed presentation on the muon reconstruction to the calibration procedure Muon Reconstruction MCP Tutorial, Patrick Scholer. Detailed presentation on the muon reconstruction starting from the MS detectors Muon Alignment Plots Twiki. Useful Run2 plots of the number of stations, sagitta and X0 Eta-meter of the Muon Spectrometer Twiki. Position in (x,y) and eta of all the MS chambers in Run2 Other MCP-related public results Results Reference Public Results pages Papers, Conference Notes, Pub Notes, Plots First Collisions Performance of the ATLAS Detector using First Collision Data arXiv:1005.5254, JHEP 09 (2010) 056 Performance with cosmics Studies of the performance of the ATLAS detector using cosmic-ray muons arXiv:1011.6665, EPJC 71 (2011) 1593 Standalone vertex finding Standalone Vertex Finding in the ATLAS Muon Spectrometer arXiv:1311.7070, JINST 9 (2014) P02001 Commissioning with cosmic rays Commissioning of the ATLAS Muon Spectrometer with Cosmic Rays arXiv:1006.4384, Eur. Phys. J. C 70 (2010) p.875-916 Publications in preparations Title Glance Link Muon Combined Performance Results in Run-3 ATLAS-MUON-2023-01",
    "description": "Standard references for publications Analysis Reference Analyses using 13 TeV data (momentum corrections) Studies of the muon momentum calibration and performance of the ATLAS detector with pp collisions at sqrt(s)=13!TeV arXiv: 2212.07338, Eur. Phys. J. C 83 (2023) 686, Material Analyses using 13 TeV data (Identification, efficiency measurement) Muon reconstruction and identification efficiency in ATLAS using the full Run 2 pp collision data set at sqrt(s)=13 TeV arXiv:2012.00578, Eur. Phys. J. C 81, 578 (2021), Material Analyses using 13 TeV data (fake non prompt lepton background estimation) collaboration of egamma and mcp groups Tools for estimating fake/non-prompt lepton backgrounds with the ATLAS detector at the LHC arXiv:2211.16178JINST 18 2023 T11004 Analyses using 13 TeV data (Momentum scale/resolution corrections) Muon reconstruction performance of the ATLAS detector in proton–proton collision data at sqrt(s)=13 TeV arXiv:1603.05598, Eur. Phys. J. C 76 (2016) p. 292, Material Analyses using 7 and 8 TeV data (2011 and 2012) Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton-proton collision data arXiv:1407.3935, Eur. Phys. J. C 74 (2014) p. 3130, Material Analyses using 7 and 8 TeV data (2010 only) Muon reconstruction efficiency and momentum resolution of the ATLAS experiment in proton-proton collisions at sqrt{s}=7 TeV in 2010 arXiv:1404.4562, Eur. Phys. J. C 74 (2014) p. 3034, Material The corresponding BibTex can be found here.",
    "tags": [],
    "title": "Publications and Presentations",
    "uri": "/publications/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "The SagittaBiasCorrections code is used by the MMC group to correct the sagitta bias. The sagitta of a circular arc is the distance from the center of the arc to the center of a line joining the two ends of the arc, it is inversely proportional to the pT, which equals to: pT [GeV] = 0.3qB[T]rho[m].\nThe code runs on the MCP ntuples as well as the ID performance ntuples and created the sagitta bias correction maps in eta-pT phase space.\nIntroduction The SagittaBiasCorrections package contains utilities for measuring the delta sagitta bias. It uses Jpsi meson or Z boson that decay into a pair of oppositely-charged muons. The positive and negative muons from these decays would emerge in roughly opposite directions with, on average, the same energy. If there is a systematic difference in the measured momentum of the oppositely-charged muons (in all regions of the detector), it means that there is a sagitta bias that we need to correct for. Delta sagitta doesn’t affect the central value of the dimuon mass, but affects it’s width.\nThe package is used to find the sagitta bias corrections using data and MC. It also contains plotting scripts for producing other helpful plots.\nInstallation First Time Prepare dir and clone the package:\nmkdir SagittaBiasCorr cd SagittaBiasCorr setupATLAS lsetup git git atlas init-workdir https://:@gitlab.cern.ch:8443/atlas/athena.git cd athena git clone --recursive https://:@gitlab.cern.ch:8443/atlas-mcp/SagittaBiasCorrections.git Then type:\ncd SagittaBiasCorrections/InDetAlignExample git checkout development # it is recommended to work on the development branch git branch -v -a Then compile:\ncd ../../../ mkdir build cd build asetup Athena,24.0.60 cmake ../athena/Projects/WorkDir make -j4 source */setup.sh cd ../athena/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/ cd run Validation -i 2 --file-input list_input_files -o output.root --ntupleType 2 When the run is complete the code produces the file Output_file.root where all the output histograms are saved.\nEvery Day Setup for every day use:\ncd SagittaBiasCorr cd build setupATLAS asetup --restore source */setup.sh Running Locally The SagittaBiasCorrections program can be run like this:\ncd ../athena/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/ cd run Validation -i 2 --file-input list_input_files -o output.root With the last command the code is executed with default parameters. The main parameters can be changed with command line arguments. These can be specified as value to an option at launch time. The options can be specified in short form with the prefix - (-o value) or in long form with the prefix -- (--opt value). (The long options may take parameter of the form --opt arg or --opt=arg )`.\nValidation --help will list all the available options. Some of the most used options are:\n-h OR --help → Show the help message -i OR --iterations → Number of iterations. -f OR --file-input → Name of the file containing the list of input files to use. -o OR --output → Specify the name of the output file -t OR --file-tools → Name of the file with the “IterationTool” to use (see below to have a description of the IterationTool). -N OR --ntupleType → Type of ntuple, possible values are: 0=Run2IDAlign, 1=Run3IDAlign, 2=MCPNtuple, 3=ZMassNtuple For all these options a value is required.\nThen, for example, if we type the command\nValidation -i 2 --file-input list_input_files -o output.root we launch the code for 2 iterations using the inputs listed in the file list_files and saving the result in the file output.root.\nIf not specified at launch time, the parameters take default values. The default values are:\nNumber of iterations = 1 File input Ntuples = “list_input_files” File output name = ‘Output_file.root\" File IterationTool = “iteration_tool_list.txt” To change the input Ntuples we could change the content of the file list_input_file, or add another list file and specify its name at launch time.\nMore parameters can be set modifying the configuration file userFile.txt. This is a simple text file in which to some keywords is assigned a value. An example extracted from the userFile.txt is\n#comment line n_iterations = 1 #set the number of iterations to use in the run #set the number of eta phi bins to use in the detector maps fnEtaBins = 13 #default 10 fnPhiBins = 13 #default 10 Running on Batch Script SubmitToCondor.py for running on batch can be found in run directory.\nTo submit the job to HTCondor, simply run the command:\npython SubmitToCondor.py With the last command the code is executed with default parameters. The main parameters can be changed with command line arguments. For example:\npython SubmitToCondor.py -i 1 -f list_input_files -o output.root -t iteration_tool_list.txt All possible options are:\nOption Explanation Default -b OR --batchName Name of the batch CB -e OR --nEtaBins Number of Eta bins 32 -f OR --fileInput What are the input files list_input_files -i OR --iterations How many iterations 1 -j OR --jobFlavour Job flavour espresso -N OR --Nevents Number of events to process NOUSERINPUT -n OR --ntuType ntuple type. default IDalignment NOUSERINPUT -o OR --output Output file output_condor.root -p OR --nPhiBins Number of Phi bins 32 -T OR --TrkCollection Name of the file with the IterationTool to use ID -t OR --fileTools Name of the file with the IterationTool to use iteration_tool_list.txt -u OR --userFile Name of the user file with running parameters userFile.txt BASIC STRUCTURE OF THE CODE AND ITERATION TOOLS The code is composed by various classes. These classes are:\nDiMuonEvent → It stores the dimuon event properties, and has methods to get the return all of them ValidationTool → It perform the events loop EventSelection → It stores the selection properties, and perform the dimuon event selection. The main method call have as parameter a DiMuonEvent object and return a bool TRUE or FALSE if the event has passed or not the selection. Histograms → It manages the kinematic histograms (the histograms related to the weak modes are instead defined in the specific classes) IterationTool → is the base class for the specific weak modes classes ITERATION TOOL CLASS The classes derived from the base class “IterationTool” are:\nDeltaSagitta DiMuonMassFitter PseudoMass VarMin these classes calculate the weak modes correction maps.\nThe basic idea is that these classes have a list of methods inheritated from the base class, and only these methods are called by the rest of the code. These methods are:\nBookHistogram FillHistogram FitMethod saveHistogram ResetHistogram According to the list in the iteration_tool_list.txt, the corrisponding IterationTool objects are created and the code runs calling for each of them the methods listed before.\n… Enjoy!!",
    "description": "The SagittaBiasCorrections code is used by the MMC group to correct the sagitta bias. The sagitta of a circular arc is the distance from the center of the arc to the center of a line joining the two ends of the arc, it is inversely proportional to the pT, which equals to: pT [GeV] = 0.3qB[T]rho[m].\nThe code runs on the MCP ntuples as well as the ID performance ntuples and created the sagitta bias correction maps in eta-pT phase space.",
    "tags": [],
    "title": "SagittaBiasCorrections",
    "uri": "/scale_resolution/sagittabiascorrection/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group \u003e MMC Tutorial",
    "content": "Preparation setupATLAS lsetup git mkdir MMCValidationTutorial cd MMCValidationTutorial mkdir build run Clone the code Now we will inject the calibration constants to a $Z \\rightarrow \\mu \\mu$ MC sample. First we setup the MuonCalNTupleMaker code\ngit clone --recursive https://gitlab.cern.ch/atlas-mcp/muoncalntuplemaker.git source Preparing for the injection of the custom calibration Firstly we need to copy the recommendation folder. We will copy there the new file with the parameters\ncd run mkdir MuonMomentumCorrections cp -r /cvmfs/atlas.cern.ch/repo/sw/database/GroupData/MuonMomentumCorrections/Recs2024_05_06_Run2Run3 MuonMomentumCorrections/CustomCalibration if you “ls” inside you will see all the folders that contains the Recs released in the past years (muon calibration recs) some example\n“Recs2024_05_06_Run2Run3” is the latest Run3 calibration that contains also the Run2 preliminary recs. This is the default one currently when you call the MuonCalibTool “Recs2021_12_11” is the Run2 rel21 folder and so on… Then we need a small macro to convert txt into a root file that can be read by the MuonCalibTool through the CalibInitialiser\ncp /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Validation/ConvertinRoot.cxx . root -l 'ConvertinRoot.cxx(\"2023\", \"Run3_CB_MCPTutorial_1309_10k_CB.txt\")' where 2023 is the year of the recs but you can put whatever you want. You have just to add the name of the txt file. Now move the produced file in the correct folder in order to use it.\nmv CB.root MuonMomentumCorrections/CustomCalibration/ScaleAndSmear/Data23/ Don’t be scared of this message mv: replace 'MuonMomentumCorrections/CustomCalibration/ScaleAndSmear/Data23/CB.root', overriding mode 0444 (r--r--r--)?. Just type y (the recommendation folder you have copied has inherited the same writing rights.\nWhen you call the MuonCalibTool you pass him the release folder (currently, “Recs2024_05_06_Run2Run3” is the default one). The “release” variable is passed from the MuonCalibTool to the muon calibrations tools (MuonCalibIntHighpTSmearTool, MuonCalibIntScaleSmearTool, MuonCalibIntSagittaTool). These are calling the CalibInitializer which through the CalibContainer calls the PathResolver thet generate that complete path. The problem is that inside the CalibInitializer in Line 17 and 45, it modifies the name of the folder adding the “MuonMomentumCorrections/” string. We have to put the file you want to test into a Folder called “MuonMomentumCorrections/” and inside you should have the same structure of the standard recs. To avoid this, we can setup $OurPath with the absolute path to the custom calibration file folder that you just created.\nexport CALIBPATH=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCValidationTutorial/run/:$CALIBPATH where inside the “run/” folder you should have a “MuonMomentumCorrections/” folder.\nInject the calibration in the code Edit ../source/MuonCalNTupleMaker/Root/NTupProcessor.cxx and change the L149 as follows\nRETURN_CHECK(\"initMuonTools\", m_muonCalSmearTool-\u003esetProperty(\"release\", \"CustomCalibration\" )); which tells the MuonCalNtupleMaker which is the name of the folder that will contain the calibration. In detail you have just to change Recs2024_05_06_Run2Run3 into CustomCalibration where the last is the name of our custom folder.\nAnother configuration of the MuonCalibTool is the calibMode which provides several user modes, which automatically configure the recommendations from MCP on momentum calibration. The user mode is configurable via the calibrationMode property, that can be set to MuonCalibTool::correctData_CB (0), MuonCalibTool::correctData_IDMS (1) or MuonCalibTool::notCorrectData_IDMS (2) MuonCalibTool::notcorrectData_CB (3). The current default option is noOption, forcing all analyses are required to make an explicit choice. In the MuonCalNTupleMaker MuonCalibTool::correctData_CB is configured.\nCompiling the code cd ../build lsetup \"asetup AthAnalysis,24.2.27\" emi cmake ../source \u0026\u0026 make -j 4 cd ../run/ source ../build/x86_64-el9-gcc13-opt/setup.sh # Setup the environment Producing the ntuples with the calibration To run easly the code, a top level python script source/python/processSample.py that will allow for job submission (locally or using condor)\nexport CALIBPATH=/afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCValidationTutorial/run/:$CALIBPATH # \u003c--- change the path here processSample.py -i /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Validation/group.perf-muons.mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.NTUP_MCP.r14799_67.27.0_ath24.0.15_EXT1 -o /afs/cern.ch/work/\u003cLETTER\u003e/\u003cUSERNAME\u003e/MMCValidationTutorial/run/ -c ../source/MuonCalNTupleMaker/data/config/calibratedpT.txt --runLocal --runLocal is used to run the code locally but you can submit the job on condor via --submitCondor. --config is used to select the config file to be used which contains informations about which variable will be stored in the final ntuple. The MuonCalibration tool needs to have as small as pobbile ntuple so most of the non-usefulinformation are not stored in the minintuple. calibratedpT.txt is the config to store the variables useful to perform studies on the calibrated objects, MinimalCalib.txt is the config used to create the standard (minimal) ntuples used as input for the MMC group. You can create your config file starting from the different examples in the data/config folder. More informations are here: submission and config\nSelections applied The MPA framework already has some selections (TTVA, Loose quality muons, $p_T \u003e 3\\ GeV$ and $|\\eta| \u003c 2.75$). No isolation requirements are applied. In the MuonCalNTupleMaker/MuonCalNTupleMaker/Selector.h other cuts are applied, such like according to the type of sample / invariant mass peak you want to study ($J/\\psi$, $\\Upsilon$, $Z$, or $high-p_T$):\nOS charge $2 \u003c m_{ll} \u003c 4.4\\ GeV$ and $60 \u003c m_{ll} \u003c 130\\ GeV$ trigger match $p_T$ cut according to the trigger thresholds What is produced jobList/ is a folder that contains the folders (one for each input) that contain the source used to run the job. localRunLogs/ is a folder that contains the folders (one for each input) and inside you find the log of the job mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.r14799_67.27.0_ath24.0.15_EXT1_0.root is the output file. Output file It contains a tree called MuonMomentumCalibrationTree with many variables. Let’s see the most important ones:\nChannelNumber is the DSID of the MC (Pos,Neg)_(CB,ID,ME)_(Pt,Eta,Phi,QoverP,Charge) are the interesting variables. Pos and Neg should select the positive and negative charge muon tracks but THIS IS NOT TRUE always. There are stored (fixed in the last ntuples producting) sam sign events in which you can have two negative/positive muons (one stored in the Neg and one in the Pos variables) so you should use the _Charge variable to be 100% sure that the charge is the one you want. CB, ID and ME are the types of track usually used in the MMC group (combined, ID tracks and ME tracks). You have for each track the momentum, eta and phi, QoverP and the charge. all the variables with _calib in the name are the variables related to the calibrated muon tracks. _MUON_SAGITTA_XXXX, _MUON_SCALE_XXXX, etc are the systematic variations of the variables related to the calibrated muon tracks. (Pos,Neg)_Truth_(Pt,Eta,Phi,Charge) are the truth variables Producting the plot Copy the macros in the work folder\ncp /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Validation/ImpactCalib* . cp -r /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Validation/Classes/ . cp /eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/MMCTutorial/Validation/makePlot.cxx . edit the ImpactCalib.h and write the right folder in the SAMPLE_FOLDER variable. Now we run on the tree to produce some histograms. This macro is an example of what the MuonMomentumValidation code does.\nroot -l ImpactCalib.C This will create the InvariantMass_calib_MMCTutorial.root file which contains some examples of histograms. Only the Z invariant mass histograms are filled (we just run on the $Z \\rightarrow \\mu \\mu$ sample) using the un-calibrated and calibrated muons. Now we can produce the final plot.\nroot -l makePlot.cxx You will have now the Mass_CB_mc.png plot. To see the plot use display\ndisplay Mass_CB_mc.png CONGRATULATION, you have applied the calibration to the $Z \\rightarrow \\mu \\mu$ sample.\nMuon Validation Tool More details on the tool here.\nMomentumValidation/Root: folder with the header and the classes MuonMomentumCorrections: folder with the MuonCalibTool to inject the corrections util: main cxx files (ReadGiacomoNtuple, FitAllResonancesSlices) ReadGiacomoNtuple_xAODMuon In order to make the selection from a di-muon ntuple one has to run the ReadGiacomoNtuple_xAODMuon.\nReadGiacomoNtuple_xAODMuon -i datalist.list -r Z -t CB ID ME -calibRelease Recs2023_08_28_Run2Run3 -grl grl.xml -save_dir_eos /eos/user/v/vandergr/MomentumValidation_Output/ FitAllResonancesSlices FitAllResonancesSlices is the main program which will call the FitModelRes class. FitModelRes is the class that handles the fitting procedure. The corrected MC has to be fitted first as its outputs are treated as inputs for the subsequent fits. A double-sided Cystall Ball + Gaussian + exponential (pol(2)) fit will be used.\nFitAllResonancesSlices -r Z -t CB ID ME -f etaL Plotting The directory MomentumValidation/scripts/ contains a set of root scripts (doPtPublicPlots, doEtaPublicPlots, doEtaRMSPlots) and a new python script (MMC_full_plotting.py) used to make some plots of the results.\nSome examples of the plots can be seen in this presentation in Slide 6.\nSelections The Selection will be applied in the class Root/Selection.cxx and defined in Root/Cuts.cxx where the definition of the cuts is in GeV (0 for Z, 1 for Jpsi, 2 for Yupsilon, and 3 for DrellYan). Here you see the list of default selections applied:\ntrigger, TTVA selections are already included in the ntuples (see previous steps) GRL: [ReadGiacomoNtuple_xAODMuon], [Selection] type: Combined [Selection] and author MuidCO [Selection] $p_T$ cut [Selection] [Cuts]: $24 \u003c p_T \u003c 300\\ GeV$ (for $Z$) $6.5 \u003c p_T \u003c 100\\ GeV$ (for $J/\\psi$) $5.0 \u003c p_T \u003c 50.0\\ GeV$ (for $\\Upsilon$) $p_T \u003e 1000\\ GeV$ (for $Drell-Yan$) leding $p_T$ cut [Selection] [Cuts]: $p_T \u003e 27\\ GeV$ (for $Z$) $p_T \u003e 27\\ GeV$ (for all the others) opposite charge: [Selection] invariant mass cut [Selection] [Cuts] $75 \u003c m_{ll} \u003c 105\\ GeV$ (for $Z$) $2.6 \u003c m_{ll} \u003c 3.5\\ GeV$ (for $J/\\psi$) $8.5 \u003c m_{ll} \u003c 11.5\\ GeV$ (for $\\Upsilon$) $m_{ll} \u003c 6000\\ GeV$ (for $Drell-Yan$) Documentation of the Classes Particle.h: container responsible for main particle properties (lorentz vector etc) Muon.h: container for muon properties of the Muon. Event.h: container for event properties. Contains all muons, triggers etc.. Histograms.h: Responsible for initialising, filling, writing and reading end-level histograms and trees. Selection.h: Responsible for making the selection. MiniMuonCalibration.h: Handles the muon calibration like sagitta correction MuonMomentumCalibrationTree: Responsible for reading the TTree while using -caf option. If some branches are missing, add them here. TreeReader.h: Fills Muons with information from the Trees. Use for m_TreeType == 2 for -caf option. Values will be converted to MeV. itools.h: Stores definitions like track names, resonance names, binning, sys names etc.. FitModelRes: Handles the fitting procedure",
    "description": "Preparation setupATLAS lsetup git mkdir MMCValidationTutorial cd MMCValidationTutorial mkdir build run Clone the code Now we will inject the calibration constants to a $Z \\rightarrow \\mu \\mu$ MC sample. First we setup the MuonCalNTupleMaker code\ngit clone --recursive https://gitlab.cern.ch/atlas-mcp/muoncalntuplemaker.git source Preparing for the injection of the custom calibration Firstly we need to copy the recommendation folder. We will copy there the new file with the parameters\ncd run mkdir MuonMomentumCorrections cp -r /cvmfs/atlas.cern.ch/repo/sw/database/GroupData/MuonMomentumCorrections/Recs2024_05_06_Run2Run3 MuonMomentumCorrections/CustomCalibration if you “ls” inside you will see all the folders that contains the Recs released in the past years (muon calibration recs) some example",
    "tags": [],
    "title": "Validation",
    "uri": "/scale_resolution/tutorial/validation/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools",
    "content": "The mcp-pipelines framework is for preservation and automation of common workflows in the MCP subgroup.",
    "description": "The mcp-pipelines framework is for preservation and automation of common workflows in the MCP subgroup.",
    "tags": [],
    "title": "Automated Monitoring Framework",
    "uri": "/commonsoftware/pipelineframework/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "User Documentation MuonPerformanceAnalysis MuonPerformanceAnalysis ntuple's production Automated Monitoring Framework",
    "description": "User Documentation MuonPerformanceAnalysis MuonPerformanceAnalysis ntuple's production Automated Monitoring Framework",
    "tags": [],
    "title": "Common Software Tools",
    "uri": "/commonsoftware/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "MuonEfficiencyCorrections is a CP package to supply scale factor and efficiency information along with associated uncertainties to analyzers.\nThis tool is included in all recent AnalysisBase and AthAnalysisBase releases, so you usually will not need to check it out by hand. In order to find the version which comes with a release, please make use of the GitLab web interface For end users, documentation on how to use the tool can be found below. How to setup The tool can be configured via the Property mechanism. If your code uses the asg::AnaToolHandle then you can do it via properties for AnalysisBase and AthAnalysis. Otherwise use JobOptions files for AthAnalysis. For nominal operation, you only need to set the following properties:\nOption Default value Available values Additional comments WorkingPoint Medium Loose, Medium, Tight, LowPt, HighPt; LowPtMVA, HighPt3Layers Muon working point used in the analysis. LowPtMVA is experimental but fully supported, HighPt3Layers for special cases. Run3 Working Points As specified in the release 22 chapter, the only currently supported WPs for Run3 are Tight, Medium, Loose, HighPt, and LowPt.\nThis means that muons have to be pre-selected using MuonSelectionTool.\nCombining Working Points When combining different WPs across kinematic thresholds (e.g. LowPt muons below a transverse momentum of 5GeV and Medium muons above that), systematic components (SF_SYST_1UP/DN) of the uncertainty on efficiency scale factors should be treated as fully correlated between the WPs, whereas statistical components (SF_STAT_1UP/DN) should be left uncorrelated.\nFurther properties are optional and not needed for normal tool operation:\nOption Default value Available values Additional comments DataEfficiencyDecorationName Efficiency Any string you like Name of the decoration used to decorate the muon with the measured efficiency in data MCEfficiencyDecorationName mcEfficiency Any string you like Name of the decoration used to decorate the muon with the measured efficiency in MC ScaleFactorDecorationName EfficiencyScaleFactor Any string you like Name of the decoration used to decorate the muon with the scale factor CalibrationRelease 230213_Preliminary_r22run2 Calibration releases available at calibration area The calibration release to use. LowPtThreshold 15.e3 any float The threshold (in MeV) below which JPsi based SFs should be applied. Negative numbers mean, that only Z-based SFs are used (not recommended to change!). UncorrelateSystematics false true / false This option will create one CP::SystematicSet in the recommended systematics for each bin of the statistical error of the SF map. This option will let the number of systematics explode! Only use if you really know, what you are doing. Use2DIsoCorrections false true / false If this option is true, 2D isolation SFs (vs muon pT and deltaR(muon,closest jet)) will be used (not recommended for the time being) UseLRT false true / false If this option is true, the tool will be aware of the source of muons (standard vs LRT) and the corresponding efficiencies will be applied Working examples are included in the package:\nEventLoop: MuonEfficiencyScaleFactorsTest executable. Athena: MuonEfficiencyCorrections_xAOD_Testing_jobOptions.py How to retrieve the scale factors Either obtain the SF directly via\nCorrectionCode getEfficiencyScaleFactor( const xAOD::Muon\u0026 mu, float\u0026 sf ) or add it as aux information to the muon via\nCorrectionCode applyEfficiencyScaleFactor( xAOD::Muon\u0026 mu ) It will then be available as a decoration with the name specified by the ScaleFactorDecorationName property, defaulting to EfficiencyScaleFactor.\nHow to retrieve the reconstruction efficiency per muon For MC, either obtain the reconstruction efficiency directly via\nCorrectionCode getMCEfficiency(const xAOD::Muon\u0026 mu, float\u0026 eff) or add it as aux information to the muon via\nCorrectionCode applyMCEfficiency(const xAOD::Muon\u0026 mu) It will then be available as a decoration with the name specified by the MCEfficiencyDecorationName property, defaulting to mcEfficiency.\nOption for systematics breakdown For Z-derived reconstruction+identification (pT\u003e15 GeV) and TTVA scale factors, it is now possible to retrieve a full breakdown of the Tag\u0026Probe systematic uncertainties, by setting to true the BreakDownSystematics property (default is false).\nNote This is an option that should be tested by analyses sensitive to correlations or profiling of the corresponding SF systematics.\nHow to provide new input files for the tool The tool uses histograms stored in ROOT files as inputs. It supports any histograms of the type TH1F, TH2F, TH3F and TH2Poly. The binning of the SF is determined by the axis binning of the histogram provided. Use the axis title to tell the tool what variable an axis is binned in. Examples: pt, eta, etaphi bin, charge.\nExample for SF binned in eta and phi: The X axis could be the eta one and have the title muon eta (the tool will recognize the 'eta'), and the Y axis could be the phi axis and have the title muon phi.\nPlease provide at least two histograms:\nOne named SF, with the scale factor as bin contents and the statistical error on the scale factor as bin errors One named SF_sys, with the systematic error on the scale factor as bin contents Additionally, the tool also supports Data and MC efficiencies, in case you can also provide (one of) them, please use:\nOne named Eff, with the efficiency measured in the data as bin contents and the statistical error on the efficiency as bin errors One named Eff_sys, with the systematic error on the efficiency measured in the data as bin contents One named MC_Eff, with the efficiency measured in MC as bin contents and the statistical error on the efficiency as bin errors One named MC_Eff_sys, with the systematic error on the efficiency measured in MC as bin contents You can bin your histograms in time. To do that, create numerous versions of your histograms, with the time period/run number as suffix to the name.\nExample for per period scale factors: SF_A, SF_B, (…) , SF_sys_A, SF_sys_B, (…) Example for per run data efficiencies: Eff_200804, Eff_200812, (…) , Eff_sys_200804, Eff_sys_200812, (…) You can use any string for the suffix, e.g. also SF_timeOfMyFirstACRShift, SF_whenIWasNotOnShift as long as the same strings are also put to the LumiData tree.\nProvide a TTree called LumiData in the same input file. It should have one entry for each run/period you provide, and three branches:\nPeriod (string): the name of the time period / suffix you used above (example: 200804 or A) FirstRun (int): the run number of the first run of the period (example: 200804) LastRun (int): the run number of the last run of the period (example: 300344). We recommend to set the LastRun of the last run/period in time to 999999 (open end) in order to use this one as pre-recommendation for future data taking. The tool will then automatically return the corresponding scale factor/efficiency by checking the randomRunNumber decorator of the xAOD::EventInfo class and propagate the systematic and statistical errors.\nPlease use the existing (calibration inputs) for further reference.\nTreatment of LRT muons For analyses using muons reconstructed from the Large Radius Tracking chain a.k.a. LRT muons, the Medium ID WP is supported for both Run-2 and Run-3 conditions. The identification efficiency correction treatment, however, is a bit different between standard and LRT muons. The correction is broken into two components, track-related and muon reconstruction-related (or MS-related). Note that the isolation treatment is identical between standard and LRT muons.\nThe track-related efficiency is obtained from the InclusiveTrackFilterTool where the efficiency has a central value of 1. with an uncertainty on it. This systematic should be applied separately (i.e. outside of MuonEfficiencyCorrections) and should be treated as an independent nuisance parameter in a fit. The MS-related uncertainty for LRT muons is obtained nominally using the getEfficiencyScaleFactor or applyEfficiencyScaleFactor methods as described above. The tool expects the muons to have access to the isLRT flag which reflects the origin container, either standard (isLRT==0) or LRT (isLRT==1) (the patternRecoInfo can be used in this case as well as a fallback). The isLRT flag is added as a decoration the muons by the MuonLRTMergingAlg which merges the standard and LRT muons containers. If the containers are not being merged and the MuonsLRT container is being used as is, make sure the patternRecoInfo is correctly available. Follow these instructions to ensure that LRT muons are treated correctly:\nSet the UseLRT property of the tool to true. For Run-2, set the CalibrationRelease property to 240620_LRT_r22run2. Only the Medium WP is supported for LRT muons, and the efficiencies are available for muon pT \u003e 3 GeV. For Run-3, set the CalibrationRelease property to 250418_Preliminary_r24run3 (old Run3 recommendations: 240620_Preliminary_r24run3). Only the Medium WP is supported for LRT muons, and the efficiencies are available for muon pT \u003e 15 GeV.",
    "description": "MuonEfficiencyCorrections is a CP package to supply scale factor and efficiency information along with associated uncertainties to analyzers.\nThis tool is included in all recent AnalysisBase and AthAnalysisBase releases, so you usually will not need to check it out by hand. In order to find the version which comes with a release, please make use of the GitLab web interface For end users, documentation on how to use the tool can be found below. How to setup The tool can be configured via the Property mechanism. If your code uses the asg::AnaToolHandle then you can do it via properties for AnalysisBase and AthAnalysis. Otherwise use JobOptions files for AthAnalysis. For nominal operation, you only need to set the following properties:",
    "tags": [],
    "title": "Muon Efficiency Corrections",
    "uri": "/guidelines/muonefficiencycorrections/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "The MuonCalibrationFit code derives calibration parameters for muon scale and resolution correction. It runs on centrally produced calibration ntuples, creates histograms of a given observables (such histograms are often referred to as ’templates’) for both data and simulation, and iteratively derives scale and resolution parameters using pre-defined minimisation function (e.g. chi square) and algorithms (e.g. Simplex). This page will hopefully gets you up to speed with installing, MuonCalibrationFit, running it, and interpretting its outputs.\nIf you have already installed MuonCalibrationFit, skip to Quick Start. For more information on specific python script job options, see this link. Once you have finished a round of calibration, you might want to look into what to do with the swarm of output files, and you can have a look here\nInstallation To install MuonCalibrationFit:\nmkdir MyWorkingArea # Create your working area cd MyWorkingArea # Move to your working area setupATLAS mkdir build run git clone --recursive https://gitlab.cern.ch/atlas-mcp/MuonCalibrationFit.git source cd build asetup AthAnalysis,21.2.97,here cmake ../source \u0026\u0026 make -j 4 cd ../run/ source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment The next time you wish to setup the code, it will be sufficient to restore the AthAnalysis environment as follows:\ncd MyWorkingArea/build/ asetup --restore cd ../run/ source ../build/x86_64-centos7-gcc62-opt/setup.sh # Setup the environment export PYTHONPATH=$PYTHONPATH:$TestArea/../source/MuonCalibrationFit/python export HTMLRAWDIR=\u003cdirectory-where-plots-will-be-stored\u003e export HTMLDISPLAYDIR=\u003cpath-to-directory-for-http-access\u003e export EMAIL_ADDRESS=\u003cemail-address\u003e Here, only PYTHONPATH is strictly necessary. If you have HTML setup through eos on lxplus (follow the instructions here: link), or through your institution web setup, you could define both HTML*DIR variables for a nice web display of your calibration results. If your email address is set correctly to EMAIL_ADDRESS, you will get a email notification once a calibration iteration has finished.\nPre-requisites Before you can run the code, there are two pre-requisites that you need to double check: 1. if the calibration ntuples are available on the interactive server you are running on, 2. if you can submit batch jobs, and through which system.\nCalibration Ntuples The calibration ntuples are centrally produced by the calibration subgroup, and are made available through the mcp group space on eos: /eos/atlas/atlascerngroupdisk/perf-muon. Multiple versions for a single campaign/software release may exist, so do consult your friendly calibration convenor before launching a full calibration run.\nIf you are running calibration on lxplus, you will be able to access eos directly, and simply pointing the files in the calibration will be enough (more on this later). Otherwise if you were running on locally or your institutional batch server, you will need to download these ntuples. These productions ranges from a couple hundred GBs to a couple TBs, so consult the convenors (for the smallest usable ntuple production), your institutional tech support (if running on institutional batch server) or your own sanity (if running locally).\nSubmitting jobs Due to the duration, CPU and VEM usage for calibration, it is often impractical to run calibration interactively (although this is an option, often used for debugging, more on this later), submitting jobs to a batch system is therefore, preferred.\nCurrently, MuonCalibrationFit supports submitting to Slurm and HTCondor. If your institution is using another job scheduler (torque comes to mind), contact your friendly calibration convenor.\nQuick start As an example, we will begin by inspecting the Run_ID.py file under source/MuonCalibrationFit/python/ link. As we will make modifications to this file, let’s first make a copy of it:\ncp Run_ID.py First_Calib.py The python scripts creates and submits calibration job, and First_Calib.py creates it by creating an instance of Job object, Iter1. By default, this launches one iteration of Inner Detector calibration fit in the geometry acceptance region of |eta| \u003c 2.7.\nWe can leave most of these options as they are, but double check the following two sets of options:\nIter1.Files = JobUtils.Files(os.path.expandvars('{}/share/NTupleList/V8Data/'.format(MCF_Dir)), 'Data_mc16d.txt', 'Signal_mc16d.txt', 'Background_mc16d.txt') Iter1.BaseFilePath = \"/eos/atlas/atlascerngroupdisk/perf-muon/MuonMomentumCalibration/muonptcalib_v08/MinimalVars/\" Here, double check if you have access to eos. Otherwise, change BaseFilePath to where your ntuples are stored, and cross check with the three txt files under source/MuonCalibrationFit/share/NTupleList/V8Data (as indicated by Files attribute), to see if the BaseFilePath do contain the root files indicated by the txt files.\nIter1.RunningMode = 'User' Iter1.RunningSystem = 'BNLCondor' DEFAULT_SLURM_OPTIONS = {\"RunJob\": {\"Time\":\"12:00:00\", \"Memory\": \"16000M\"}, \"MergerJob\": {\"Time\":\"00:20:00\", \"Memory\": \"1000M\"}} Iter1.SlurmSettingDict = DEFAULT_SLURM_OPTIONS Here, double check which job scheduler is available on your batch server. The above code is set up for slurm usage. And for HTCondor, replace the above lines with the following:\nIter1.RunningSystem = 'Condor' and you’re set for using HTCondor! If you want to debug something this could also be set to ‘Interactive’, which runs the job on the batch system directly instead of submitting to any job schedulers.\nAnother variable of interest is the MaxEvents attribute. Currently it’s set to 100000, but change it to -1 to run on the full dataset available in the ntuples.\nTo run this iteration of calibration, we just need to navigate to the run directory and submit this job as the following:\ncd $TestArea/../run nohup python $TestArea/../source/MuonCalibrationFit/python/First_Calib.py \u0026",
    "description": "The MuonCalibrationFit code derives calibration parameters for muon scale and resolution correction. It runs on centrally produced calibration ntuples, creates histograms of a given observables (such histograms are often referred to as ’templates’) for both data and simulation, and iteratively derives scale and resolution parameters using pre-defined minimisation function (e.g. chi square) and algorithms (e.g. Simplex). This page will hopefully gets you up to speed with installing, MuonCalibrationFit, running it, and interpretting its outputs.",
    "tags": [],
    "title": "MuonCalibrationFit",
    "uri": "/scale_resolution/muoncalibrationfit/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Open tasks",
    "content": "List of completed AQPs Project People End date Reports or Packages Modelling of the input variables for PLIT Zongpu Wang 24/10/2025 CDS note JIRA LowPt MVA in Run3 Luca Pagani 29/09/2025 CDS note JIRA CaloMuonScore in Run3 Lu Zheng 29/09/2025 CDS note JIRA Muon Momentum calibration on Run-3 data Ehsan Musajan 29/08/2025 CDS note JIRA 2D isolation SFs Arnav Sunil Avad 03/06/2025 CDS note JIRA Forward muons Celine Stauch 11/03/2025 CDS note JIRA Prompt Lepton Isolation Tagger Ema Maricic 22/01/2025 CDS note JIRA Momentum Calibration High-pT Yangfan Zhang 25/09/2024 CDS note JIRA Optimization of Loose WP in low pT regime Konstantinos Sioulas 24/09/2024 CDS note JIRA Momentum Calibration Run3 Xingyu Wu 26/07/2024 CDS note JIRA Reco and id efficiency measurements Dimitrios Iliadis 15/07/2024 JIRA Checking syst. model on efficiency measurements Carolina De Almeida Rossi 12/07/2024 CDS note JIRA Muon reco in busy environments Lars Linden 24/06/2024 CDS note JIRA Truth classification + PLIV Ka Yan Fan 11/04/2024 CDS note JIRA Muon mis-id rate measurement Fabiola Raffaeli 25/01/2024 CDS note JIRA PLIV for rel22 Zhuolin Zhang 23/01/2024 CDS note JIRA Momentum calibration as a function of muon types Chunhao Tian 15/01/2024 CDS note JIRA HighPt WP in rel 22 Camila Pazos 20/09/2023 CDS note JIRA Tight WP in rel 22 Veritas Gassmann 08/08/2023 CDS note JIRA Loose and Medium WPs in rel22 run2/3 Xinyan Liu 26/07/2023 CDS note JIRA Jpsi and Z calibration non closure Dionysios Fakoudis 24/07/2023 CDS note JIRA Muon Isolation R22+Run3 WP and SF Wenhao Ma 20/06/2023 CDS note JIRA Impact of jet activity on muon reconstruction and isolation efficiencies Simon Grewe 16/05/2023 CDS note JIRA Muon Pt Performance Validation with Jpsi, Z, and Upsilon Aaron Van Der Graaf 20/12/2022 CDS note JIRA Muon isolation SFs using PFlow jets and exclusive WPs Chen Jia 10/10/2022 CDS note Muon alignment studies for the Z and Jpsi resonances Andres Eloy Pinto Pinoargote 11/08/2022 CDS note Study muon reconstruction charge mis-id in data/mc Yifan Zhu 11/03/2022 CDS note Very low-pT muons identification Hao Pang 23/02/2022 CDS note Trigger and PID efficiencies in HI Petr Baron 15/09/2021 Slides Muon isolation scale factors in busy environments Giorgia Proto 30/07/2021 CDS note In-depth study of muon momentum performance in MS Mingzhe Xie 24/03/2021 Slides JIRA Muon momentum calibrations Siyuan Yan 10/03/2021 Slides Low-pT efficiencies with J/psi and Y Noemi Cavalli 09/12/2020 Slides Study of pi-\u003emu misidentification rates Kevin Barends 15/10/2020 Validation of resolution / scale corrections Lucas Klein 26/08/2020 Slides Isolation efficiency SFs Yingjie Wei 24/06/2020 Slides New B-field map validation Brendon Bullard 26/11/2019 CDS note List of completed Tasks Task Description People Reports or Packages Calibration of PLIV isolation WPs for muons Calibration (SF measurement) for new PLIV-based muon isolation WPs Fudong He, Rustem Ospanov IFF meeting 28.03.22",
    "description": "List of completed AQPs Project People End date Reports or Packages Modelling of the input variables for PLIT Zongpu Wang 24/10/2025 CDS note JIRA LowPt MVA in Run3 Luca Pagani 29/09/2025 CDS note JIRA CaloMuonScore in Run3 Lu Zheng 29/09/2025 CDS note JIRA Muon Momentum calibration on Run-3 data Ehsan Musajan 29/08/2025 CDS note JIRA 2D isolation SFs Arnav Sunil Avad 03/06/2025 CDS note JIRA Forward muons Celine Stauch 11/03/2025 CDS note JIRA Prompt Lepton Isolation Tagger Ema Maricic 22/01/2025 CDS note JIRA Momentum Calibration High-pT Yangfan Zhang 25/09/2024 CDS note JIRA Optimization of Loose WP in low pT regime Konstantinos Sioulas 24/09/2024 CDS note JIRA Momentum Calibration Run3 Xingyu Wu 26/07/2024 CDS note JIRA Reco and id efficiency measurements Dimitrios Iliadis 15/07/2024 JIRA Checking syst. model on efficiency measurements Carolina De Almeida Rossi 12/07/2024 CDS note JIRA Muon reco in busy environments Lars Linden 24/06/2024 CDS note JIRA Truth classification + PLIV Ka Yan Fan 11/04/2024 CDS note JIRA Muon mis-id rate measurement Fabiola Raffaeli 25/01/2024 CDS note JIRA PLIV for rel22 Zhuolin Zhang 23/01/2024 CDS note JIRA Momentum calibration as a function of muon types Chunhao Tian 15/01/2024 CDS note JIRA HighPt WP in rel 22 Camila Pazos 20/09/2023 CDS note JIRA Tight WP in rel 22 Veritas Gassmann 08/08/2023 CDS note JIRA Loose and Medium WPs in rel22 run2/3 Xinyan Liu 26/07/2023 CDS note JIRA Jpsi and Z calibration non closure Dionysios Fakoudis 24/07/2023 CDS note JIRA Muon Isolation R22+Run3 WP and SF Wenhao Ma 20/06/2023 CDS note JIRA Impact of jet activity on muon reconstruction and isolation efficiencies Simon Grewe 16/05/2023 CDS note JIRA Muon Pt Performance Validation with Jpsi, Z, and Upsilon Aaron Van Der Graaf 20/12/2022 CDS note JIRA Muon isolation SFs using PFlow jets and exclusive WPs Chen Jia 10/10/2022 CDS note Muon alignment studies for the Z and Jpsi resonances Andres Eloy Pinto Pinoargote 11/08/2022 CDS note Study muon reconstruction charge mis-id in data/mc Yifan Zhu 11/03/2022 CDS note Very low-pT muons identification Hao Pang 23/02/2022 CDS note Trigger and PID efficiencies in HI Petr Baron 15/09/2021 Slides Muon isolation scale factors in busy environments Giorgia Proto 30/07/2021 CDS note In-depth study of muon momentum performance in MS Mingzhe Xie 24/03/2021 Slides JIRA Muon momentum calibrations Siyuan Yan 10/03/2021 Slides Low-pT efficiencies with J/psi and Y Noemi Cavalli 09/12/2020 Slides Study of pi-\u003emu misidentification rates Kevin Barends 15/10/2020 Validation of resolution / scale corrections Lucas Klein 26/08/2020 Slides Isolation efficiency SFs Yingjie Wei 24/06/2020 Slides New B-field map validation Brendon Bullard 26/11/2019 CDS note List of completed Tasks Task Description People Reports or Packages Calibration of PLIV isolation WPs for muons Calibration (SF measurement) for new PLIV-based muon isolation WPs Fudong He, Rustem Ospanov IFF meeting 28.03.22",
    "tags": [],
    "title": "Completed Tasks",
    "uri": "/open-tasks/completed/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "The group is responsible for the offline muon momentum calibration and provides corrections for the analyses. There are two types of corrections:\ncorrection for potential charge-dependent momentum bias related to the knowledge of the detector geometry. This type of correction (aka Sagitta Bias Corrections) is applied only to the data; correction to the reconstructed muon transverse momenta in simulation in order to match the measurement of the same quantities in data. This type of correction is applied in the simulated events. MuonCalNTupleMaker Muon Calibration common space SagittaBiasCorrections MuonCalibrationFit MomentumValidation MMC Tutorial",
    "description": "The group is responsible for the offline muon momentum calibration and provides corrections for the analyses. There are two types of corrections:\ncorrection for potential charge-dependent momentum bias related to the knowledge of the detector geometry. This type of correction (aka Sagitta Bias Corrections) is applied only to the data; correction to the reconstructed muon transverse momenta in simulation in order to match the measurement of the same quantities in data. This type of correction is applied in the simulated events. MuonCalNTupleMaker Muon Calibration common space SagittaBiasCorrections MuonCalibrationFit MomentumValidation MMC Tutorial",
    "tags": [],
    "title": "Momentum Scale and Resolution group",
    "uri": "/scale_resolution/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "The MomentumValidation code is used by the MMC group to validate the calibration parameters. The code runs on the MuonCalNTupleMaker ntuples as well as the MuonPerformanceAnalysis ntuples.\nIntroduction The MomentumValidation package contains utilities for muon momentum scale and resolution analysis. It uses Jpsi, Z and Upsilon (WIP) resonances to validate smearing and scale corrections in data and MC. It also contains plotting scripts for producing validation plots.\nInstallation First Time The Validation framework was just migrated to AlmaLinux (El9). The centos7 version is not supported anymore, but there is an centos7 outdated version available as a separate branch\nPrepare dir and clone the package:\nsetupATLAS mkdir MomentumValidation cd MomentumValidation mkdir build run git clone --recursive https://:@gitlab.cern.ch:8443/atlas-mcp/MomentumValidation.git source Or you can use the ssh link:\ngit clone --recursive ssh://git@gitlab.cern.ch:7999/atlas-mcp/MomentumValidation.git source Then type:\ncd source asetup AnalysisBase 24.2.41 cd .. Then compile:\ncd build asetup AnalysisBase 24.2.41 cmake ../source \u0026\u0026 make -j 4 source ../build/*/setup.sh cd ../ Every Day Setup for every day use:\nsetupATLAS cd MomentumValidation cd build asetup --restore source */setup.sh cd ../.. export CALIBPATH=$SetYourPath:$CALIBPATH A new change that comes with the El9 migration is the $CALIBPATH. In this new version you have to create a custom folder which contains the calibrations that you want to validate with the Validation framework. Therefore create a folder in any location. In this folder you have input your calibration files later. Make sure to replace $SetYourPath with the absolute path to your custom calibration file folder that you just created.\nSetting up the custom Calibration folder In the custom calibration folder you have to have a folder called MuonMomentumCorrections. Navigate to your custom calibration folder and create the folder with mkdir MuonMomentumCorrections. Within this folder you can define custom calibrations. The name of the folder is the corresponding name that have to be called when running the Histograms, which is explained in the next step. A calibration folder has to follow a specific structure. It has to contain the two folder called sagittaBias and ScaleAndSmear. Within these folders there should be subfolder for the different data taking periods: Data16, Data17, Data18, Data22, Data23. Each of these folders contain root files for the different tracks: CB_data.root, CB_mc.root. These files follow a specific format. You can find examples for them at the global calibration path: /cvmfs/atlas.cern.ch/repo/sw/database/GroupData/MuonMomentumCorrections/\nYou can translate other formats of the calibration files to this root format by using the located in the git repo at Scripts/ConvertinRoot.cxx.\nImportant Note: all of these folders have to exists and have to contain the needed files otherwise the tool crashes. E.g. even if you run only Run 2 validation the files for the Run 3 calibration need to be existing the folders\nProducing Histograms In order to make the selection from a dimuon n-tuple one has to run the ReadGiacomoNtuple program. It creates all possible two combinations of muon pairs in the event.\nRunning locally The ReadGiacomoNtuple program will be run like this:\nReadGiacomoNtuple -i datalist.list -r Z -t CB ID ME -calibRelease Recs2023_08_28_Run2Run3 -grl grl.xml -save_dir_eos /eos/user/v/vandergr/MomentumValidation_Output/ Explanation:\n-i datalist.list will read the files from datalist.list. This list has to be written in the following manner: Path to file, type (MC or DATA), correction (has ptCorrection or not), and suffix to be added to the output file (NO for nothing) Example: For Data: rootFileData.root DATA CORR NO NO For MC: rootFileMC.root MC CORR NO NO -r Z: It will run for Z resonance. You can also run over Jpsi resonance. If you use -r Z Jpsi, it will first run over Z and then run over Jpsi, while producing a seperate output for each. -t CB ID ME: It will run over the CB, ID and ME track. Producing seperate outputfiles for each. -calibRelease Recs2020_03_03: The Calibration release which will be used. This has to match with the folder within your custom calibration file folder at $CALIBPATH Latest Run3 Calibration is Recs2023_08_28_Run2Run3 -grl grl.xml: The GoodRunList which will be used. -save_dir_eos path: Give an alternative saving path for the output, if not defined the output is saved to the folder from which the code was run. The output files will be like this: CB_data.root for Z, CB. CB_Jpsidata.root for Jpsi, CB.\nTo run uncorrected MC use you have to create a custom calibration with calibration values at 0 everywhere. TBC in the near future.\nRunning on batch Scripts for running on batch can be found in Scripts. In this folder there can be found a file list caf_list.list and a script submit_condor.sh for submitting the job to batch.\nOptions for histograms The following options for ReadGiacomoNtuple are available:\nOption Explanation Default -r Z Jpsi Which resonance to run: Z, Jpsi, Y, and/or Drell Z Jpsi Y Drell -t CB ID ME Which tracks to run: CB, ID and/or ME CB ID ME -grl grl.xml Load GoodRunList grl.xml false -i input.list Gets input list for rootfiles from input.list false -calibRelease Recs2020_03_03 Set calibRelease to Recs2020_03_03 Recs2020_07_01 -debug Debugging mode False -nevt n Set max number of events to n -1 -bar Shows a process bar false -jobNumber n Give jobnumber n to outputfile (for running on batch) false Fitting FitAllResonancesSlicesis the main program which will call the FitModelRes class. FitModelRes is the class that handles the fitting procedure. The corrected MC has to be fitted first as its outputs are treated as inputs for the subsequent fits. A Cystall Ball + Gaussian + exponential (pol(2)) fit will be used.\nRun fitting To run the FitAllResonancesSlices program follow the following steps:\nFirst you have to copy or link the dimuon histogram-outputs produced with ‘ReadGiacomoNtuple’ into your running directory. Then you have to make a directory out for storing the results:\nmkdir out Start fitting with the following command:\nFitAllResonancesSlices -r Z -t CB ID ME -f etaL Explanation:\n-r Z: It will run for Z resonance. You can also run over Jpsi resonance. If you use -r Z Jpsi, it will first run over Z and then run over Jpsi, while producing a seperate output for each. -t CB ID ME: It will run over the CB, ID and ME track. Producing seperate outputfiles for each. -f etaL: instructs against which distribution to differentiate the measurement. These are some options already implemented: etaL: pseudorapidity of the leading muon phiL: phi of the leading muon pTL: $p_T$ of the leading muon etaS: pseudorapidity of the sub-leading muon phiS: phi of the sub-leading muon pTS: $p_T$ of the sub-leading muon pTMean: mean $p_T$ pTStar: $p_T$ defined only from angles mmEta: pseudorapidity of the dimuon mmPhi: phi of the dimuon mmPt: $p_T$ of the dimuon In order to run the fit with direct CB calibration use the flag called -sysVersion 0. The default value for this flag is 1 and corresponds to the ID+MS Calibration. The full command to run the fit for the CB track with direct CB calibration is the following:\nFitAllResonancesSlices -r Z -t CB -f etaL pTL -sysVersion 0 Plotting For the plotting a new python script was written which has a lot of functionality already but is still and work in progress and will be extended. The python plotting script is located at MomentumValidation/scripts/MMC_full_plotting_refactored.py. The script can be used like this:\npython MMC_full_plotting_refactored.py -r Z -t CB -f Eta -i $input_path -o $out_path Explanation:\n-r Z: Which resonance should be run? (Z/Jpsi) -t CB: Which track should be run? (CB/ID/ME) -f eta: Which variable should be plotted? (Eta/Pt) -i $input_path: Path to the input files coming from the fit. e.g. /eos/user/v/vandergr/MomentumValidation_Output/out/ -o $out_path: Path to a folder where the plots will be saved to Additional command line flags:\n--directCB True: Has to be used to run the plotting for directCB calibration --extension .png: Which format should be used for the plots? (.png/.pdf) --uncorrData $uncorr_data_path: Path to uncorrected data. If given it will be plotted additionally to the data and MC --uncorrMC $uncorr_mc_path: Path to uncorrected MC. If given it will be plotted additionally to the data and MC Selection The Selection will be applied in the class Selection. In order to turn Selections on or off you have to add or comment/uncomment them in MomentumValidation/Root/Selection.cxx.\nFor now the following Selections are used:\n$p_T$ Cut: For Z: $24 \u003c p_T \u003c 300\\ GeV$ For Jpsi: $6.3 \u003c p_T \u003c 100\\ GeV$ Lead $p_T$ Cut (for Z only): $p_T \u003e 27\\ GeV$ |d0BL_significance| \u003c 3 (Applied on NTuples, commented out in code: ClassifyGoodMuons in Selection.cxx) |deltaZ0BL_sin(theta)| \u003c 0.5 (Applied on NTuples, commented out in code: ClassifyGoodMuons in Selection.cxx) Isolation Cut (for Z only): ptVarCone30_TightTTVA_Pt1000/Pt \u003c 0.06 Quality Cut: \u003c 1 CB track Cut: muon Type == 0 Charge: != 0 Author Cut: == 1 The triggers are removed for now, as they were applied in the NTuples for now. They can be put back in the Method PassSelection in Selection.cxx .\n(The following triggers were applied:)\n(For Z: Trig_HLT_mu26_ivarmedium or Trig_HLT_mu50) (For Jpsi: Trig_HLT_2mu6_bJpsimumu) Setting Cuts The class Cuts holds the definition of the Cuts in GeV for several Resonances, being 0 for Z, 1 for Jpsi, 2 for Yupsilon, and 3 for DrellYan. You can change the values for the Cuts here. Please check if they are applied in MomentumValidation/Root/Selection.cxx.\nSetting Triggers In order to set the triggers you have to pushback the Triggername to m_TriggerNames in MomentumValidation/Root/Cuts.cxx. The available Trigger can be found in MomentumValidation/Root/TreeReader.cxx. Here they are set in InitRead() and filled in FillTriggers(). If a trigger must be added, add it to both functions. The name and value have to be on the same position of each vector (iTriggerNames and iTriggerPassed). Then you can add the triggername in Cuts.cxx. The Trigger selection will be applied with DidPassedTrigger() (from Event class) in the selection. (If more then one trigger is applied, they can be used exclusively or inclusively by using andFlag.)\nDocumentation of the Classes Class Particle.h: container responsible for main particle properties (lorentz vector etc)\nClass Muon.h: container for muon properties of the Muon.\nClass Event.h: container for event properties. Contains all muons, triggers etc..\nClass Histograms.h: Responsible for initialising, filling, writing and reading end-level histograms and trees.\nClass Selection.h: Responsible for making the selection.\nClass MiniMuonCalibration.h: Handles the muon calibration like sagitta correction\nClass MuonMomentumCalibrationTree: Responsible for reading the TTree while using -caf option. If some branches are missing, add them here.\nClass TreeReader.h: Fills Muons with information from the Trees. Use for m_TreeType == 2 for -caf option. Values will be converted to MeV.\nClass itools.h: Stores definitions like track names, resonance names, binning, sys names etc..\nClass FitModelRes: Handles the fitting procedure",
    "description": "The MomentumValidation code is used by the MMC group to validate the calibration parameters. The code runs on the MuonCalNTupleMaker ntuples as well as the MuonPerformanceAnalysis ntuples.\nIntroduction The MomentumValidation package contains utilities for muon momentum scale and resolution analysis. It uses Jpsi, Z and Upsilon (WIP) resonances to validate smearing and scale corrections in data and MC. It also contains plotting scripts for producing validation plots.\nInstallation First Time The Validation framework was just migrated to AlmaLinux (El9). The centos7 version is not supported anymore, but there is an centos7 outdated version available as a separate branch",
    "tags": [],
    "title": "MomentumValidation",
    "uri": "/scale_resolution/momentumvalidation/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "This page gives an overview of the systematic uncertainties for momentum and efficiency corrections.\nMomentum Corrections Systematics The CP::MuonCalibrationAndSmearingTool tool supports the ASG systematics interface.\nHere is a list of the independent systematics available, relevant to charge-dependent momentum corrections:\nName Meaning Types Tool Settings MUON_SAGITTA_DATASTAT Statistical error on the charge dependant corrections ± 1σ Default tool settings MUON_SAGITTA_RESBIAS Variations in the momentum scale (charge-dependent), based on the residual charge-dependent bias and data/MC nonClosure ± 1σ Default tool settings MUON_SAGITTA_GLOBAL The residual global charge-dependent momentum bias after the calibration, which is a constant in all the eta and phi region, describing the limit of the sensitivity of the calibration method on the global effect ± 1σ Default tool settings MUON_SAGITTA_PTEXTRA pT dependent variation to describe the non-sagitta effect in the end-cap (|eta| \u003e 1.05) regions ± 1σ Default tool settings MUON_SAGITTA_RESBIAS_NEGETA Same as MUON_SAGITTA_RESBIAS but only affecting muons in negative eta ± 1σ if doEtaSagittaSys is true MUON_SAGITTA_RESBIAS_POSETA Same as MUON_SAGITTA_RESBIAS but only affecting muons in positive eta ± 1σ if doEtaSagittaSys is true Here is a list of the independent systematics available, relevant to charge-independent momentum corrections:\nName Meaning Types Tool Settings MUON_ID Variations in the ID track resolution component of the CB track ± 1σ If ID+MS scheme is used MUON_MS Variations in the MS track resolution component of the CB track ± 1σ If ID+MS scheme is used MUON_CB Variations in the CB track resolution directly ± 1σ If CB scheme is used MUON_SCALE Variations in the momentum scale ± 1σ For both scheme, but only if the Corr_Scale scheme is used If the Decorr_Scale correlation scheme is used, the MUON_SCALE systematic is split into its component\nName Meaning Types Tool Settings MUON_SCALE_ID Variations in the momentum scale corrections related to the ID track component of the CB track ± 1σ If ID+MS scheme is used MUON_SCALE_MS Variations in the momentum scale corrections related to the MS track component of the CB track ± 1σ If ID+MS scheme is used MUON_SCALE_MS_ELOSS Variations in the energy loss corrections related to the MS track component of the CB track ± 1σ If ID+MS scheme is used MUON_SCALE_CB Variations in the momentum scale corrections to CB track directly ± 1σ If CB scheme is used MUON_SCALE_CB_ELOSS Variations in the energy loss corrections to CB track directly ± 1σ If CB scheme is used Efficiency Correction Systematics Reconstruction and Identification Efficiencies The supported uncertainties on reconstruction and identification efficiencies are summarised in the table below.\nName Description MUON_EFF_RECO_STAT Statistical error on SFs derived on Z events MUON_EFF_RECO_SYS Systematic error on SFs derived on Z events MUON_EFF_RECO_STAT_LOWPT Statistical error on SFs derived on J/Psi events MUON_EFF_RECO_SYS_LOWPT Systematic error on SFs derived on J/Psi events Analyses sensitive to correlations or profiling of the reconstruction and identification efficiency systematics for SFs derived on Z events should check the full systematic breakdown by setting the BreakDownSystematics option to true.\nWhen combining different WPs across kinematic thresholds (e.g. LowPt muons below a transverse momentum of 5GeV and Medium muons above that), systematic components (SF_SYST_1UP/DN) of the uncertainty on efficiency scale factors should be treated as fully correlated between the WPs, whereas statistical components (SF_STAT_1UP/DN) should be left uncorrelated.\nIsolation Efficiencies The supported uncertainties on isolation efficiencies are summarised in the table below. Because corrections for isolation efficiencies are solely derived on Z events, only one statistical and one systematic uncertainty component is required.\nName Description MUON_EFF_ISO_STAT Statistical error on the SF MUON_EFF_ISO_SYS Systematic error on the SF Track-to-Vertex-Association Efficiencies The supported uncertainties on TTVA efficiencies are summarised in the table below. Because corrections for TTVA efficiencies are solely derived on Z events, only one statistical and one systematic uncertainty component is required.\nName Description MUON_EFF_TTVA_STAT Statistical error on the SF MUON_EFF_TTVA_SYS Systematic error on the SF Analyses sensitive to correlations or profiling of the TTVA efficiency systematics should check the full systematic breakdown by setting the BreakDownSystematics option to true.",
    "description": "This page gives an overview of the systematic uncertainties for momentum and efficiency corrections.\nMomentum Corrections Systematics The CP::MuonCalibrationAndSmearingTool tool supports the ASG systematics interface.\nHere is a list of the independent systematics available, relevant to charge-dependent momentum corrections:\nName Meaning Types Tool Settings MUON_SAGITTA_DATASTAT Statistical error on the charge dependant corrections ± 1σ Default tool settings MUON_SAGITTA_RESBIAS Variations in the momentum scale (charge-dependent), based on the residual charge-dependent bias and data/MC nonClosure ± 1σ Default tool settings MUON_SAGITTA_GLOBAL The residual global charge-dependent momentum bias after the calibration, which is a constant in all the eta and phi region, describing the limit of the sensitivity of the calibration method on the global effect ± 1σ Default tool settings MUON_SAGITTA_PTEXTRA pT dependent variation to describe the non-sagitta effect in the end-cap (|eta| \u003e 1.05) regions ± 1σ Default tool settings MUON_SAGITTA_RESBIAS_NEGETA Same as MUON_SAGITTA_RESBIAS but only affecting muons in negative eta ± 1σ if doEtaSagittaSys is true MUON_SAGITTA_RESBIAS_POSETA Same as MUON_SAGITTA_RESBIAS but only affecting muons in positive eta ± 1σ if doEtaSagittaSys is true Here is a list of the independent systematics available, relevant to charge-independent momentum corrections:",
    "tags": [],
    "title": "Systematic Uncertainties",
    "uri": "/guidelines/systematics/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Most users will not need to add a new package, and could safely skip this section. On the other hand, it will illustrate the inner workings of the pipelines platform, and may be worth reading simply for your own understanding.\nStep 1: Docker-izing your package The first step takes place outside the mcp-pipelines code. You first need to configure your package to pre-compile itself into a docker image. You need to write a Dockerfile and a .gitlab-ci.yml file to compile the image and upload it to the gitlab container registry. An example of how this was accomplished with the SagittaBiasCorrection package is shown:\nARG ROOT_VERSION FROM docker.io/rootproject/root:$ROOT_VERSION COPY . /home/SagittaBiasCorrections ARG PROJECT_SOURCE_DIR=/home/SagittaBiasCorrections USER root RUN apt-get update \u0026\u0026 \\ apt-get -y install cmake \\ libeigen3-dev \\ libboost-all-dev \u0026\u0026 \\ cd /home/SagittaBiasCorrections \u0026\u0026 \\ mkdir -p build \u0026\u0026 \\ cd build \u0026\u0026 \\ useradd -ms /bin/bash dockeruser \u0026\u0026 \\ export CMAKE_PREFIX_PATH=$CMAKE_PREFIX_PATH:/usr/include/eigen3 \u0026\u0026 \\ mv ../Docker/CMakeLists.txt.nobuild ../Docker/CMakeLists.txt \u0026\u0026 \\ cmake ../Docker \u0026\u0026 \\ make \u0026\u0026 \\ chown -R dockeruser /home/SagittaBiasCorrections ENV PATH=\"${PATH}:/home/SagittaBiasCorrections/build/SagittaBiasCorrections\" USER dockeruser WORKDIR /home/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/run ENTRYPOINT [\"/bin/bash\"]\tThe Dockerfile builds its layers on top of the ROOT image, configured by an argument ROOT_VERSION. It then builds the code and sets the entrypoint, the default command to run inside the container, to bash. Therefore it will operate like logging in to a machine with this package pre-compiled. The .gitlab-ci.yml file looks like:\nstages: - build .run_for_all_versions: parallel: matrix: - ROOT_VERSION: 6.28.00-ubuntu22.04 variables: GIT_SUBMODULE_STRATEGY: recursive build_image: extends: .run_for_all_versions only: - master - docker tags: - docker-machine stage: build image: name: gcr.io/kaniko-project/executor:v1.14.0-debug entrypoint: [\"\"] script: - /kaniko/executor --context \"${CI_PROJECT_DIR}\" --dockerfile \"${CI_PROJECT_DIR}/Docker/Dockerfile\" --destination \"${CI_REGISTRY_IMAGE}:${ROOT_VERSION}-${CI_COMMIT_SHORT_SHA}\" --build-arg \"ROOT_VERSION=${ROOT_VERSION}\" This uses kaniko as a docker-in-docker solution to build the image for the SagittaBiasCorrections inside the gitlab pipeline, which itself is executing inside a docker image. This pipeline will create new images and label them with the CI_COMMIT_SHORT_SHA for the master branch only. It also demonstrates how to pass arguments to the build, and if other ROOT_VERSIONs were defined in the matrix, it would build images for multiple ROOT versions. For technical reasons, 6.28 was the most recent ROOT version that would support this package.\nYou should then see the container registry populating for your project. Check out the container registry for any of the existing projects to see what this looks like if you are just following along here and not writing your own package.\nStep 2: Publishing your package You might think that your project is now published because it is in the container registry. While in some sense it is, for the container to be most useful for us it needs to be distributed on /cvmfs. This is accomplished by creating a merge request to the sync repo. Simply add the name of your container registry to the recipe.yaml file. If you look closely there you should see several images owned by atlas-mcp.\nIt takes some time for the images to propagate to /cvmfs. It should not take more than 1 day, however. In this case contact the maintainers of the sync repo. You can verify that your image is on /cvmfs with an ls. For example, to see the atlas-mcp owned images, type:\nls /cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/atlas-mcp/ Step 3: Writing the steering into mcp-pipelines At this point, your code is pre-compiled and distributed, making it available on both lxplus and on the grid. Now, you need to tell the pipelines code how to perform actions within the image. Recall, for our docker image the entrypoint was simply to open a “bash” terminal. So the bulk of the work will be to write out the bash script that constitutes “running the tool”. Some tools might require lots of configuration, like the ATLASPlotting tool, and so take as an argument a configuration file. Others require far less.\nTo give the pipelines access to your tool, you need to create a new file in the folder python/sections. An example might look like:\nfrom TaskFlavourBase import TaskFlavourBase import os class Section(TaskFlavourBase): ####################################################### # mandatory overrides of base class: # these 3 must be done as static members ####################################################### type = \"SagittaBiasCorrections\" image_location = \"/cvmfs/unpacked.cern.ch/gitlab-registry.cern.ch/atlas-mcp/sagittabiascorrections\" default_image = \"6.28.00-ubuntu22.04-c596b2d8\" def __init__(self, header, pipeline): super().__init__(header, pipeline) # updates using the tag information # script and outputs must come after superclass initialization to read self.args, self.name, etc. self.output = 'Output_file.root' # configuration for script to run inside the container InDSName = self.get_InDSs()[0] self.script = f'ls $PWD/{InDSName}* \u003e\u003e myInputs.txt \u0026\u0026 cp -r /home/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/run . \u0026\u0026 cd run \u0026\u0026 Validation --file-input ../myInputs.txt {self.args} \u0026\u0026 mv Output_file.root .. \u0026\u0026 cd ..' largeFileDir = self.pipeline.largeFileDir if hasattr(self.pipeline, 'largeFileDir') else f'{os.getcwd()}' self.singularityHome = largeFileDir self.local_script=f'ls {largeFileDir}/localInputs/{InDSName}/* \u003e\u003e {largeFileDir}/localInputs/{InDSName}.txt \u0026\u0026 cp -r /home/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/run . \u0026\u0026 cd run \u0026\u0026 Validation --file-input {largeFileDir}/localInputs/{InDSName}.txt {self.args} \u0026\u0026 mv Output_file.root .. \u0026\u0026 cd .. \u0026\u0026 rm -r run \u0026\u0026 root -l \"/home/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/scripts/thePlots.C(\\\\\\\"Output_file.root\\\\\\\", 5)\" \u0026\u0026 mkdir -p plots \u0026\u0026 mv *.pdf plots' self.help_script='Validation -h' # script to run locally after downloading from the grid # this is used to make plots locally not on the grid self.finish_script = 'mv \"$(\\ls -1dt ./gridOutput/*/ | head -n 1)\" output \u0026\u0026 cd output \u0026\u0026 mv \"$(\\ls -1dt *Output_file.root | head -n 1)\" Output_file.root \u0026\u0026 root -l \"/home/SagittaBiasCorrections/InDetAlignExample/weakmodemonitoring/scripts/thePlots.C(\\\\\\\"Output_file.root\\\\\\\", 5)\"' # once new image is available update this self.publishable = [os.path.join(f'{self.pipeline.workflowDir}', \"output\", 'Output_file.root'), os.path.join(f'{self.pipeline.workflowDir}', 'output', 'Output_file_plots.root')] self.isFinalOutput = True As you can see, all that needs to be done is set some variables in the constructor. The main task is to define what script is executed at different times. These include:\nself.script: Main script executed on the grid. self.local_script: Main script executed locally. May differ from grid script mostly because some tools like AMI are not accessible from a grid node. self.help_script: Print a useful message about the optional arguments to the tool. self.finish_script: After grid running is complete, run some script locally, for example to hadd files. Not all tools will require all scripts to be defined, in particular self.finish_script can often be left blank. Other variables to define include:\nself.singularityHome: where is singularity executed by default. largeFileDir will make it execute in the folder you specified as your preferred location for large files on your first call to mcp test. self.publishable: List of files which must be present for the output to be published the the website. self.isFinalOutput: If True, will be eligible for results to send to the website. For example, we do not send the intermediate nTuples to the website, just the final plots. Step 4: Writing the documentation into mcp-pipelines The last step is to write the documentation for the new tool into the file python/ModelConfig.py, making it compatible with mcp help. An example snippet of this file for the MuonxAODAnalysis package is shown:\n''' Inherit common information, and extend with MuonxAODAnalysis specific information InDS is not in generic class because some classes make it required while others do not ''' class MuonxAODAnalysisConfig(_GenericTaskConfig): InDS: str = pydantic.Field(title=\"InDS on rucio to run the Ntupler on.\") @staticmethod def getHeader(): return '[MuonxAODAnalysis.\u0026lt;job_name\u0026gt;]' The pydantic field variables are used tell mcp help which keys are allowable in the pipeline.conf under this tool’s headers.\nNext: Advanced: setting up automated monitor service",
    "description": "Most users will not need to add a new package, and could safely skip this section. On the other hand, it will illustrate the inner workings of the pipelines platform, and may be worth reading simply for your own understanding.\nStep 1: Docker-izing your package The first step takes place outside the mcp-pipelines code. You first need to configure your package to pre-compile itself into a docker image. You need to write a Dockerfile and a .gitlab-ci.yml file to compile the image and upload it to the gitlab container registry. An example of how this was accomplished with the SagittaBiasCorrection package is shown:",
    "tags": [],
    "title": "Adding a new package",
    "uri": "/commonsoftware/pipelineframework/tutorial/add-new-package/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group \u003e MuonTPPostProcessing",
    "content": "Note The current default T\u0026P ntuples have been produced using the old version MPA, before migrating to the Component Accumulator. MuonTPPostProcessing has been also migrated to be compatible with these ntuples.\nTo process the default ntuples, please checkout the latest compatible version v3.11.1\ncd source git checkout v3.11.1 cd ../build \u0026\u0026 cmake ../source \u0026\u0026 make -j # don't forget to recompile! In this page, we describe how to run the common MCP analyses. We assume that you are running the code from the run folder in your working area. The examples are meant to run at CERN with the HTCondor cluster. If you are working at MPP, please use the MPP input config files, and set SLURM as engine for your job.\nCurrent default input config files Run-2:\nCERN/EOS (accessible also outside of CERN, with xrootd): MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 MPP (only from MPCDF servers): MuonTPPostProcessing/data/InputConfMPI/R22/v66.16.1 Run-3:\nCERN/EOS (accessible also outside of CERN, with xrootd): MuonTPPostProcessing/data/InputConfEOS/R22run3/v66.29.0 MPP (only from MPCDF servers): MuonTPPostProcessing/data/InputConfMPI/R22run3/v66.29.0 Z Reco analysis To produce T\u0026P histograms to calculate the muon identification and reconstruction efficiency with Z-\u003emumu decays, run:\n​ Run-2 Run-3 SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 -R ../source/MuonTPPostProcessing/data/RunConf/ZReco/ZReco -H ../source/MuonTPPostProcessing/data/HistoConf/ZRecoTTVA/ZRecoTTVA_SFHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run3/v66.29.0 -R ../source/MuonTPPostProcessing/data/RunConf/ZReco/ZRecoRun3 -H ../source/MuonTPPostProcessing/data/HistoConf/ZRecoTTVA/ZRecoTTVA_SFHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR This creates histograms in your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nThe next step is copy the default ZReco DSConfig files from MuonTPPostProcessing/python/SFFileCreation/DSConfigsMPI/ZReco, and change the path to the histogram files, like this:\npaths = { \"all\": \"/afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e/\" # THE LAST SLASH IS FUNDAMENTAL! } Finally, you can produce the default efficiency plots with the GeneratePlots.py script.\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a ZmumuTPReco -w -o \u003coutput_folder\u003e Z TTVA analysis To produce T\u0026P histograms to calculate the muon track and vertex association efficiency with Z-\u003emumu decays, run:\n​ Run-2 Run-3 SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 -R ../source/MuonTPPostProcessing/data/RunConf/ZTTVA/ZTTVA -H ../source/MuonTPPostProcessing/data/HistoConf/ZRecoTTVA/ZRecoTTVA_SFHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run3/v66.29.0 -R ../source/MuonTPPostProcessing/data/RunConf/ZTTVA/ZTTVARun3 -H ../source/MuonTPPostProcessing/data/HistoConf/ZRecoTTVA/ZRecoTTVA_SFHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR This creates histograms in your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nThe next step is copy the default ZReco DSConfig files from MuonTPPostProcessing/python/SFFileCreation/DSConfigsMPI/TTVA, and change the path to the histogram files, like this:\npaths = { \"all\": \"/afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e/\" # THE LAST SLASH IS FUNDAMENTAL! } Finally, you can produce the default efficiency plots with the GeneratePlots.py script.\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a ZmumuTPTTVA -w -o \u003coutput_folder\u003e J/Psi analysis To produce T\u0026P histograms to calculate the muon identification and reconstruction efficiency with J/Psi-\u003emumu decays, run:\n​ Run-2 Run-3 SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 -R ../source/MuonTPPostProcessing/data/RunConf/JPsiReco/JPsiReco -H ../source/MuonTPPostProcessing/data/HistoConf/JPsiReco/JPsiReco_BasicHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run3/v66.29.0 -R ../source/MuonTPPostProcessing/data/RunConf/JPsiReco/JPsiRecoRun3 -H ../source/MuonTPPostProcessing/data/HistoConf/JPsiReco/JPsiReco_BasicHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR This creates histograms in your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nThe next step is copy the default ZReco DSConfig files from MuonTPPostProcessing/python/SFFileCreation/DSConfigsMPI/JPsi, and change the path to the histogram files, like this:\npaths = { \"all\": \"/afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e/\" # THE LAST SLASH IS FUNDAMENTAL! } Finally, you can produce the default efficiency plots with the GeneratePlots.py script.\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a JPsiTPReco -w -o \u003coutput_folder\u003e Z Isolation analysis To produce T\u0026P histograms to calculate the muon isolation efficiency with Z-\u003emumu decays, run:\n​ Run-2 Run-3 SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 -R ../source/MuonTPPostProcessing/data/RunConf/ZIso/ZIso -H ../source/MuonTPPostProcessing/data/HistoConf/ZIso/BkgSub_BasicHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run3/v66.29.0 -R ../source/MuonTPPostProcessing/data/RunConf/ZIso/ZIsoRun3 -H ../source/MuonTPPostProcessing/data/HistoConf/ZIso/BkgSub_BasicHistos.conf -J \u003cjob_name\u003e --engine HTCONDOR This creates histograms in your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nThe next step is copy the default ZReco DSConfig files from MuonTPPostProcessing/python/SFFileCreation/DSConfigsMPI/Isolation, and change the path to the histogram files, like this:\npaths = { \"all\": \"/afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e/\" # THE LAST SLASH IS FUNDAMENTAL! } Finally, you can produce the default efficiency plots with the GeneratePlots.py script.\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a ZmumuTPIso -w -o \u003coutput_folder\u003e HighEta analysis To produce T\u0026P histograms to calculate the muon identification and reconstruction efficiency with Z-\u003emumu decays in the very forward region (HighEta) of the ATLAS detector, run:\n​ Run-2 Run-3 SubmitToBatch.py -I ../source/MuonTPPostProcessing/data/InputConfEOS/R22run2/v66.16.1 -R ../source/MuonTPPostProcessing/data/RunConf/HighEta/HighEta -H ../source/MuonTPPostProcessing/data/HistoConf/HighEta/HighEta.conf -J \u003cjob_name\u003e --engine HTCONDOR Not yet supported for run-3.\nThis creates histograms in your Cluster folder, on lxplus by default at /afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e.\nThe next step is copy the default ZReco DSConfig files from MuonTPPostProcessing/python/SFFileCreation/DSConfigsMPI/HighEta, and change the path to the histogram files, like this:\npaths = { \"all\": \"/afs/cern.ch/work/\u003cu\u003e/\u003cusername\u003e/Cluster/OUTPUT/\u003cYEAR-MONTH-DAY\u003e/\u003cJOB_NAME\u003e/\" # THE LAST SLASH IS FUNDAMENTAL! } Finally, you can produce the default efficiency plots with the GeneratePlots.py script.\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t EffPlots SysPlots -a ZmumuTPHighEta -w -o \u003coutput_folder\u003e",
    "description": "Note The current default T\u0026P ntuples have been produced using the old version MPA, before migrating to the Component Accumulator. MuonTPPostProcessing has been also migrated to be compatible with these ntuples.\nTo process the default ntuples, please checkout the latest compatible version v3.11.1\ncd source git checkout v3.11.1 cd ../build \u0026\u0026 cmake ../source \u0026\u0026 make -j # don't forget to recompile! In this page, we describe how to run the common MCP analyses. We assume that you are running the code from the run folder in your working area. The examples are meant to run at CERN with the HTCondor cluster. If you are working at MPP, please use the MPP input config files, and set SLURM as engine for your job.",
    "tags": [],
    "title": "Default analyses",
    "uri": "/efficiency/postprocessing/default/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Guidelines for Physics Analyses",
    "content": "This is an example of how to setup the Tools to manage the muons in Athena.\nMuonMomentumCorrections (more details here) MuonSelectorTools (more details here) MuonEfficiencyCorrections (more details here) Header #include \"xAODTracking/VertexContainer.h\" #include \"xAODMuon/MuonContainer.h\" #include \"MuonSelectorTools/MuonSelectionTool.h\" #include \"MuonMomentumCorrections/MuonCalibTool.h\" #include \"MuonEfficiencyCorrections/MuonEfficiencyScaleFactors.h\" #include \"IsolationSelection/IsolationSelectionTool.h\" #include \"xAODTracking/TrackParticlexAODHelpers.h\" Tool definitions CP::IMuonCalibrationAndSmearingTool* m_muonCalibTool; CP::MuonSelectionTool *m_muonSelectionTool; CP::IsolationSelectionTool *m_IsolationSelectionTool; CP::MuonEfficiencyScaleFactors *m_muonTTVAEfficiencySF; CP::MuonEfficiencyScaleFactors *m_muonEfficiencySF; CP::MuonEfficiencyScaleFactors *m_muonIsoEfficiencySF; Tool initialisation StatusCode myAlg::initialize() { //MuQuality //0 = Tight //1 = Medium //2 = Loose //3 = VeryLoose //4 = HighPt //5 = LowPt //6 = LowPtMVA //7 = HighPt3Layers ATH_MSG_INFO(\"Setting up muon selection tool\"); m_muonSelectionTool = new CP::MuonSelectionTool(\"MuonSelection\"); ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"MuQuality\", 1)); //using Medium ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"MaxEta\", 2.5)); ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"IsRun3Geo\", true)); ATH_CHECK(m_muonSelectionTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon calibration tool for CB\"); m_muonCalibTool = new CP::MuonCalibTool(\"MuonCalibrationTool\"); ATH_CHECK(asg::setProperty(m_muonCalibTool,\"CalibMode\", MuonCalibTool::correctData_CB)); //set the calibration mode ATH_CHECK(asg::setProperty(m_muonCalibTool,\"release\", \"Recs2024_05_06_Run2Run3\")); ATH_CHECK(asg::setProperty(m_muonCalibTool, \"IsRun3Geo\", true)); ATH_CHECK(m_muonCalibTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon isolation tool\"); m_IsolationSelectionTool = new CP::IsolationSelectionTool(\"IsolationSelectionTool\"); ATH_CHECK(asg::setProperty(m_IsolationSelectionTool, \"MuonWP\", \"Loose_VarRad\")); ATH_CHECK(m_IsolationSelectionTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon TTVA SF tool\"); m_muonTTVAEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonTTVAEfficiencyScaleFactors\"); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"WorkingPoint\", \"TTVA\")); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(m_muonTTVAEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon efficiency tool for Medium\"); m_muonEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonEfficiencyScaleFactorsMedium\"); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"WorkingPoint\", \"Medium\")); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(m_muonEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon Iso SF tool\"); m_muonIsoEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonIsoEfficiencyScaleFactors\"); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"WorkingPoint\", \"Loose_VarRadIso\")); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(m_muonIsoEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool return StatusCode::SUCCESS; } Applying the calibrations StatusCode myAlg::execute() { //Retrieve the Muon Container const xAOD::MuonContainer* muons = nullptr; CHECK( evtStore()-\u003eretrieve(muons,\"Muons\") ); const xAOD::VertexContainer* pv = nullptr; CHECK( evtStore()-\u003eretrieve(vertices,\"PrimaryVertices\") ); for(const xAOD::Muon *mu : *muons) { // momentum calibration ATH_MSG_INFO(\"\\n\\n++++++++++++++++++++++++++++\\n new muon\"); xAOD::Muon* calib_muonCB = new xAOD::Muon(); calib_muonCB-\u003emakePrivateStore( *mu ); ATH_MSG_INFO(\"-\u003emuon with uncalibrated pt=\"\u003c\u003cmu-\u003ept()\u003c\u003c\" eta=\"\u003c\u003cmu-\u003eeta()\u003c\u003c\" phi=\"\u003c\u003cmu-\u003ephi()); if( !m_muonCalibTool-\u003eapplyCorrection( *calib_muonCB )) { ATH_MSG_ERROR( \"Cannot apply calibration nor smearing\" ); return StatusCode::FAILURE; } // selections (TTVA, WP, Iso) double m_d0Significance = 0; double m_z0SinTheta = 0; const xAOD::TrackParticle* trk = muon-\u003etrackParticle(xAOD::Muon::TrackParticleType::CombinedTrackParticle); if (trk) { m_d0Significance = xAOD::TrackingHelpers::d0significance(trk, ei-\u003ebeamPosSigmaX(), ei-\u003ebeamPosSigmaY(), ei-\u003ebeamPosSigmaXY()); m_z0SinTheta = std::abs(trk-\u003ez0() + trk-\u003evz() - pv-\u003ez()) * std::sin(trk-\u003etheta()); } if (abs(m_d0Significance) \u003e 3 || abs(m_z0SinTheta) \u003e 0.5) continue; if (!(m_muonSelectionTool-\u003eaccept(*calib_muonCB))) continue; if (!(m_IsolationSelectionTool-\u003eaccept(*calib_muonCB))) continue; ATH_MSG_INFO(\"-----------muon accepted-----------\"); ATH_MSG_INFO(\"pT: \" \u003c\u003c calib_muonCB-\u003ept() \u003c\u003c \" - eta: \" \u003c\u003c calib_muonCB-\u003eeta() \u003c\u003c \" - phi: \" \u003c\u003c calib_muonCB-\u003ephi()); // SFs retrieval float mu_TTVA_sf(0.); m_muonTTVAEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_TTVA_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_TTVA_sf); float mu_IDReco_sf(0.); m_muonEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_IDReco_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_IDReco_sf); float mu_Iso_sf(0.); m_muonIsoEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_Iso_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_Iso_sf); ATH_MSG_INFO(\"\\n+++++++++++++++++++++++++++++\\n\\n\"); delete calib_muonCB; } //end of loop on muons return StatusCode::SUCCESS; }",
    "description": "This is an example of how to setup the Tools to manage the muons in Athena.\nMuonMomentumCorrections (more details here) MuonSelectorTools (more details here) MuonEfficiencyCorrections (more details here) Header #include \"xAODTracking/VertexContainer.h\" #include \"xAODMuon/MuonContainer.h\" #include \"MuonSelectorTools/MuonSelectionTool.h\" #include \"MuonMomentumCorrections/MuonCalibTool.h\" #include \"MuonEfficiencyCorrections/MuonEfficiencyScaleFactors.h\" #include \"IsolationSelection/IsolationSelectionTool.h\" #include \"xAODTracking/TrackParticlexAODHelpers.h\" Tool definitions CP::IMuonCalibrationAndSmearingTool* m_muonCalibTool; CP::MuonSelectionTool *m_muonSelectionTool; CP::IsolationSelectionTool *m_IsolationSelectionTool; CP::MuonEfficiencyScaleFactors *m_muonTTVAEfficiencySF; CP::MuonEfficiencyScaleFactors *m_muonEfficiencySF; CP::MuonEfficiencyScaleFactors *m_muonIsoEfficiencySF; Tool initialisation StatusCode myAlg::initialize() { //MuQuality //0 = Tight //1 = Medium //2 = Loose //3 = VeryLoose //4 = HighPt //5 = LowPt //6 = LowPtMVA //7 = HighPt3Layers ATH_MSG_INFO(\"Setting up muon selection tool\"); m_muonSelectionTool = new CP::MuonSelectionTool(\"MuonSelection\"); ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"MuQuality\", 1)); //using Medium ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"MaxEta\", 2.5)); ATH_CHECK(asg::setProperty(m_muonSelectionTool, \"IsRun3Geo\", true)); ATH_CHECK(m_muonSelectionTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon calibration tool for CB\"); m_muonCalibTool = new CP::MuonCalibTool(\"MuonCalibrationTool\"); ATH_CHECK(asg::setProperty(m_muonCalibTool,\"CalibMode\", MuonCalibTool::correctData_CB)); //set the calibration mode ATH_CHECK(asg::setProperty(m_muonCalibTool,\"release\", \"Recs2024_05_06_Run2Run3\")); ATH_CHECK(asg::setProperty(m_muonCalibTool, \"IsRun3Geo\", true)); ATH_CHECK(m_muonCalibTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon isolation tool\"); m_IsolationSelectionTool = new CP::IsolationSelectionTool(\"IsolationSelectionTool\"); ATH_CHECK(asg::setProperty(m_IsolationSelectionTool, \"MuonWP\", \"Loose_VarRad\")); ATH_CHECK(m_IsolationSelectionTool-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon TTVA SF tool\"); m_muonTTVAEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonTTVAEfficiencyScaleFactors\"); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"WorkingPoint\", \"TTVA\")); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(asg::setProperty(m_muonTTVAEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(m_muonTTVAEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon efficiency tool for Medium\"); m_muonEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonEfficiencyScaleFactorsMedium\"); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"WorkingPoint\", \"Medium\")); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(asg::setProperty(m_muonEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(m_muonEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool ATH_MSG_INFO(\"Setting up muon Iso SF tool\"); m_muonIsoEfficiencySF = new CP::MuonEfficiencyScaleFactors(\"MuonIsoEfficiencyScaleFactors\"); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"WorkingPoint\", \"Loose_VarRadIso\")); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"IsRun3Geo\", true)); ATH_CHECK(asg::setProperty(m_muonIsoEfficiencySF, \"CalibrationRelease\", \"240711_Preliminary_r24run3\")); ATH_CHECK(m_muonIsoEfficiencySF-\u003einitialize().isSuccess()); //initialize the tool return StatusCode::SUCCESS; } Applying the calibrations StatusCode myAlg::execute() { //Retrieve the Muon Container const xAOD::MuonContainer* muons = nullptr; CHECK( evtStore()-\u003eretrieve(muons,\"Muons\") ); const xAOD::VertexContainer* pv = nullptr; CHECK( evtStore()-\u003eretrieve(vertices,\"PrimaryVertices\") ); for(const xAOD::Muon *mu : *muons) { // momentum calibration ATH_MSG_INFO(\"\\n\\n++++++++++++++++++++++++++++\\n new muon\"); xAOD::Muon* calib_muonCB = new xAOD::Muon(); calib_muonCB-\u003emakePrivateStore( *mu ); ATH_MSG_INFO(\"-\u003emuon with uncalibrated pt=\"\u003c\u003cmu-\u003ept()\u003c\u003c\" eta=\"\u003c\u003cmu-\u003eeta()\u003c\u003c\" phi=\"\u003c\u003cmu-\u003ephi()); if( !m_muonCalibTool-\u003eapplyCorrection( *calib_muonCB )) { ATH_MSG_ERROR( \"Cannot apply calibration nor smearing\" ); return StatusCode::FAILURE; } // selections (TTVA, WP, Iso) double m_d0Significance = 0; double m_z0SinTheta = 0; const xAOD::TrackParticle* trk = muon-\u003etrackParticle(xAOD::Muon::TrackParticleType::CombinedTrackParticle); if (trk) { m_d0Significance = xAOD::TrackingHelpers::d0significance(trk, ei-\u003ebeamPosSigmaX(), ei-\u003ebeamPosSigmaY(), ei-\u003ebeamPosSigmaXY()); m_z0SinTheta = std::abs(trk-\u003ez0() + trk-\u003evz() - pv-\u003ez()) * std::sin(trk-\u003etheta()); } if (abs(m_d0Significance) \u003e 3 || abs(m_z0SinTheta) \u003e 0.5) continue; if (!(m_muonSelectionTool-\u003eaccept(*calib_muonCB))) continue; if (!(m_IsolationSelectionTool-\u003eaccept(*calib_muonCB))) continue; ATH_MSG_INFO(\"-----------muon accepted-----------\"); ATH_MSG_INFO(\"pT: \" \u003c\u003c calib_muonCB-\u003ept() \u003c\u003c \" - eta: \" \u003c\u003c calib_muonCB-\u003eeta() \u003c\u003c \" - phi: \" \u003c\u003c calib_muonCB-\u003ephi()); // SFs retrieval float mu_TTVA_sf(0.); m_muonTTVAEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_TTVA_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_TTVA_sf); float mu_IDReco_sf(0.); m_muonEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_IDReco_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_IDReco_sf); float mu_Iso_sf(0.); m_muonIsoEfficiencySF-\u003egetEfficiencyScaleFactor(*calib_muonCB, mu_Iso_sf); ATH_MSG_INFO(\"- SF: \" \u003c\u003c mu_Iso_sf); ATH_MSG_INFO(\"\\n+++++++++++++++++++++++++++++\\n\\n\"); delete calib_muonCB; } //end of loop on muons return StatusCode::SUCCESS; }",
    "tags": [],
    "title": "Example of how setup the tools",
    "uri": "/guidelines/examplesetup/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "The identification working points group is responsible for defining the selections for good quality muons to be used in physics analyses. For the working points defined in software releases \u003c=21 look here\nFor the working points defined in software releases \u003e=22 look here",
    "description": "The identification working points group is responsible for defining the selections for good quality muons to be used in physics analyses. For the working points defined in software releases \u003c=21 look here\nFor the working points defined in software releases \u003e=22 look here",
    "tags": [],
    "title": "Identification",
    "uri": "/identification/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group \u003e MuonTPPostProcessing",
    "content": "The input configuration file tells the code where to pick up the primary data to process and some auxillary files.\nThese are the parameters that can be defined inside the file:\nInput: Location path of the T\u0026P n-tuples (e.g. /ptmp/mpp/junggjo9/FirstFile.root or root://eos…). GRL: No good run selection is ran on data. You need to specify which GRLs have to be used. PRWDataFile: Location of the ILumiCalcFiles/actualMu files to rerun/apply pile-up reweighting on MC. Also required if you want to study the efficiency as function of time. PRWMCFile: The corresponding config files of each MC dataset \u0026 campaign. For example, the input config file for period H of the 2022 year of data taking looks like:\n####################################################################################### #File generated at 2023-01-25 16:21:17 by dcieri ######################################################################################### #Used rucio containers: # --- group.perf-muons:group.perf-muons.data22_13p6TeV.00436195.physics_BphysDelayed.NTUP_MCP.f1290_m2137_v66.29.0_EXT0 # --- group.perf-muons:group.perf-muons.data22_13p6TeV.00436195.physics_Main.NTUP_MCP.f1290_m2137_v66.29.0_EXT0 ######################################################################################### # Data-taking runs in this config file: # --- 436195 ######################################################################################### #InputFiles: Input root://eosatlas.cern.ch:1094//eos/atlas/atlasgroupdisk/perf-muons/dq2/rucio/group/perf-muons/0d/bb/group.perf-muons.31834526.EXT0._000002.NTUP_MCPTP.root Input root://eosatlas.cern.ch:1094//eos/atlas/atlasgroupdisk/perf-muons/dq2/rucio/group/perf-muons/2b/ec/group.perf-muons.31834526.EXT0._000003.NTUP_MCPTP.root Input root://eosatlas.cern.ch:1094//eos/atlas/atlasgroupdisk/perf-muons/dq2/rucio/group/perf-muons/50/7d/group.perf-muons.31834526.EXT0._000004.NTUP_MCPTP.root Input root://eosatlas.cern.ch:1094//eos/atlas/atlasgroupdisk/perf-muons/dq2/rucio/group/perf-muons/b9/8a/group.perf-muons.31600138.EXT0._000001.NTUP_MCPTP.root Input root://eosatlas.cern.ch:1094//eos/atlas/atlasgroupdisk/perf-muons/dq2/rucio/group/perf-muons/e0/25/group.perf-muons.31834526.EXT0._000001.NTUP_MCPTP.root ######################################################################################### #Good run Lists to be applied: GRL GoodRunsLists/data22_13p6TeV/20230116/data22_13p6TeV.periodAllYear_DetStatus-v109-pro28-04_MERGED_PHYS_StandardGRL_All_Good_25ns_ignore_TRIGLAR.xml ######################################################################################### #Pile-up reweighting lumicalc files: PRWDataFile GoodRunsLists/data22_13p6TeV/20230116/ilumicalc_histograms_None_428648-440613_OflLumi-Run3-002.root ######################################################################################### #Pile-up reweighting config files: PRWMCFile /cvmfs/atlas.cern.ch/repo/sw/database/GroupData/dev/PileupReweighting/share/DSID601xxx/pileup_mc21a_dsid601190_FS.root ######################################################################################### ######################################################################################### Producing input config files Luckily, you don’t need to write manually the input config files, but you can use some scripts already available in the repository.\nIn the case that you have a local T\u0026P ntuple root file, you can use the write_in_cfg.py script to produce your Input Config file.\nwrite_in_cfg.py --in_file \u003cntuple_file_path\u003e --out_cfg \u003cinput_conf_name\u003e Add the --run3 option, if your ntuple is from run3 data or mc.\nIn the case that the T\u0026P ntuples have been produced on the grid and stored in a tier disk, you need to use the CreateInputConfigs.py script. This takes as input argument a txt file, listing the data sets containers of your ntuples.\nFor a particular MuonPerformanceAnalysis production tag, you can generate this list file using ListDisk.py script, in the ClusterSubmission package.\nFirst you need to setup rucio and pyami (after having setup the MuonTPPostProcessing area), and create a voms proxy.\nlsetup rucio pyami voms-proxy-init -voms atlas After that you can run the ListDisk script to get the list of containers.\nListDisk.py -p \u003cmpa_production_tag\u003e -o \u003coutput_path\u003e -r \u003cntuple_rse\u003e Now, you can finally create the Input Config files, using the CreateInputConfigs.py script.\nCreateInputConfigs.py -i \u003cinput_list_file\u003e -o \u003coutput_folder\u003e --engine \u003cengine_name\u003e -J \u003cjob_name\u003e --rse \u003cntuple_rse\u003e The engine to use are the same as the SubmitToBatch.py script, so SLURM,HTCONDOR,LOCAL,SGE. There are additional flags you can use to run the scripts. In particular, the most important are:\n--readROOTFiles Directly read from folder containing merged root files (default: False) -d, --downloadBeforeMerge Download the datasets (in your Cluster folder) before they are merged together (default: False) --mergeFromRSE, -m: Submit a merge job to the cluster first and write the final paths to the config files (default: False) --isLowMu Specify whether you want to create configs for the low mu runs (default: False) --noGRL Do not add a GRL to the input configs (default: False) --noIlumi Do not add a ilumicalc file to the input configs (default: False) --run3 Special configurations for run3 datasets",
    "description": "The input configuration file tells the code where to pick up the primary data to process and some auxillary files.\nThese are the parameters that can be defined inside the file:\nInput: Location path of the T\u0026P n-tuples (e.g. /ptmp/mpp/junggjo9/FirstFile.root or root://eos…). GRL: No good run selection is ran on data. You need to specify which GRLs have to be used. PRWDataFile: Location of the ILumiCalcFiles/actualMu files to rerun/apply pile-up reweighting on MC. Also required if you want to study the efficiency as function of time. PRWMCFile: The corresponding config files of each MC dataset \u0026 campaign. For example, the input config file for period H of the 2022 year of data taking looks like:",
    "tags": [],
    "title": "Input Configuration Files",
    "uri": "/efficiency/postprocessing/input-configs/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Introduction to mcp-pipelines This comprehensive tutorial will guide you through the mcp-pipelines tool. The code is a sort of “meta” code, wrapping several key tools used by the MCP group, providing an extendable common interface for all of them, and automating their execution. This code serves several purposes:\nPreservation: By requiring underlying tools to be packaged in docker images, the code naturally preserves the ability to reproduce old results. The mcp-pipelines gitlab area also serves as a shared repository where QT students in the MCP subgroup can publish their results for others to use. Automation: mcp-pipelines provides an interface to automate the execution of arbitrarily complex analysis chains. Therefore, during data taking it is easy to automate the process of monitoring the quality of data and comparing against a reference. Distribution: The monitoring dashboard site provides an easy mechanism for results to be shared with ATLAS colleagues. What you’ll learn You’ll learn how the mcp-pipelines code works, how to execute existing pipelines, how to automate a pipeline and how to extend the framework with new tools.\nNote You’ll need the following prerequisites:\nA CERN computing account Access to lxplus (or other machine with cvfms mounted) A valid grid certificate Clone the mcp-pipelines gitlab Overview There are many inter-related components to the full mcp-pipelines system. Here we will discuss a brief overview, and will provide more detail throughout the tutorial as necessary.\nThe mcp-pipelines gitlab is the main repository for this project. New users should create new branches or forks to commit new features and then submit a merge request to master. The mcp-test Virtual Machine (VM). This is a machine managed by CERN OpenStack. It is not to be used directly by the user, but is interacted with via the mcp-pipelines code. It is in charge of scheduling grid jobs for automated monitoring. It has a small storage volume of 250 GB, and is not meant for long term storage of any data. Rather, long term data storage is achieved via the webeos site (the site you are on now). This site is owned by the cimcp service account, managed by the MCP subgroup conveners. The code for the site is also available in the mcp-pipelines gitlab repository, in the www folder. Modifications to the code are in no way auotmatically propagated to the site or to the mcp-test machine. When new merge requests change functionality it must be propagated by expert users. There are several tools packaged in Docker images which can be managed by the pipelines toolkit. At the moment, these include:\nMuonxAODAnalysis: a simple nTuple maker for AOD/DOAD inputs. SagittaBiasCorrections: A package to accept a special MCP_NTUP data and compute data-driven sagitta measurment corrections as a function of eta, phi. This package is maintained by the Momentum Scale and Resolution Group. MuonTPPostProcessing Accepts special MCP_NTUP formatted data and compute efficiencies of data or MC. This package is maintained by the Efficiency Working Group. ATLASPlotting: a simple histogram maker accepting arbitrary nTuples as input. ▶ How do I clone and set up the code? ssh -Y CERNUSER@lxplus.cern.ch cd to/your/preferred/working/directory git clone ssh://git@gitlab.cern.ch:7999/atlas-mcp/mcp-pipelines.git cd mcp-pipelines git checkout muonWeekTutorial2025 source setup.sh voms-proxy-init -voms atlas Note: the URL used to git clone will need to change if you don’t have SSH Keys set up with your CERN gitlab account. Select the “Clone with HTTPS” option instead on the gitlab page in the “Code” drop down menu. Additionally, be sure to check out the muonWeekTutorial2025 tag if running in the future, as this tutorial was written to follow along with that tagged version of the code exactly.\nNext: the mcp command line interface",
    "description": "Introduction to mcp-pipelines This comprehensive tutorial will guide you through the mcp-pipelines tool. The code is a sort of “meta” code, wrapping several key tools used by the MCP group, providing an extendable common interface for all of them, and automating their execution. This code serves several purposes:\nPreservation: By requiring underlying tools to be packaged in docker images, the code naturally preserves the ability to reproduce old results. The mcp-pipelines gitlab area also serves as a shared repository where QT students in the MCP subgroup can publish their results for others to use. Automation: mcp-pipelines provides an interface to automate the execution of arbitrarily complex analysis chains. Therefore, during data taking it is easy to automate the process of monitoring the quality of data and comparing against a reference. Distribution: The monitoring dashboard site provides an easy mechanism for results to be shared with ATLAS colleagues. What you’ll learn You’ll learn how the mcp-pipelines code works, how to execute existing pipelines, how to automate a pipeline and how to extend the framework with new tools.",
    "tags": [],
    "title": "Introduction",
    "uri": "/commonsoftware/pipelineframework/tutorial/introduction/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework",
    "content": "This comprehensive tutorial will guide you through the mcp-pipelines tool. The chapters are designed to follow the order:\nIntroduction\nThe mcp command line interface\nRunning an existing analysis locally\nThe MuonxAODAnalysis package\nThe ATLASPlotting package\nWriting your own analysis\nMonitoring results over time\nRunning an existing analysis on the grid\nAdvanced: adding a new package\nAdvanced: setting up automated monitor service\nYou can start by clicking the introduction section and then the “Next” button at the end of each section.",
    "description": "This comprehensive tutorial will guide you through the mcp-pipelines tool. The chapters are designed to follow the order:\nIntroduction\nThe mcp command line interface\nRunning an existing analysis locally\nThe MuonxAODAnalysis package\nThe ATLASPlotting package\nWriting your own analysis\nMonitoring results over time\nRunning an existing analysis on the grid\nAdvanced: adding a new package\nAdvanced: setting up automated monitor service\nYou can start by clicking the introduction section and then the “Next” button at the end of each section.",
    "tags": [],
    "title": "MCP Pipelines Tutorial",
    "uri": "/commonsoftware/pipelineframework/tutorial/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Momentum Scale and Resolution group",
    "content": "Tutorial held on 12-13 September 2024 (see Indico page)\nSagitta Bias corrections Calibration Validation",
    "description": "Tutorial held on 12-13 September 2024 (see Indico page)\nSagitta Bias corrections Calibration Validation",
    "tags": [],
    "title": "MMC Tutorial",
    "uri": "/scale_resolution/tutorial/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Overview In this section we discuss a way to use the ATLASPlotting package and the published outputs of existing pipelines to create summary plots with several executions of the same pipeline compared. The most obvious use case is to execute the pipeline on every new data run, and compare the results over time, but almost anything can be done. Simply, these “summary” pipelines have access to all published pipeline output, and as we will see, we can inject arbitrary C++ code into the ATLASPlotting runtime environment, creating any number of plots from the published workflow output.\nAn example: muon efficiency summary Here we will discuss how the muon efficiency subgroup publishes plots like this on the main dashboard page:\nThis plot shows the efficiency of the four most commonly used WPs over time, with run number on the x axis. These data were taken at the beginning of the 2025 data taking period, so the first few runs with low efficiency were not in “good for physics” data taking mode. Then, once the machine was ready, the detector performance was as expected: with high single-muon reconstruction efficiency in data. The corresponding pipeline.conf is very simple:\n[Pipeline] Name = mtppp_summary WebInputs = mtppp [ATLASPlotter.plots] Image = master-accbecc3 ConfigFile = plots.conf Just specifying which pipeline outputs should be readble in this “summary pipeline” environment, and telling ATLASPlotter to run with a given configuration. Even the plots.conf file is easy to write! In this case the relevant code to make the above plot looks like:\n[General] OutputRootFile = mtppp_summary.root OutputPDFFile = mtppp_summary.pdf Label = Internal [PlotDefault] Type=Macro [efficiency_plot] PostMacroPath=macros/efficiency_summary.cxx PostMacroArgs=(\"PLOT\") Latex=CB effeciency So then, where do we tell the code to make this 2D plot with efficiency on the y axis and run number on the x axis? In this plots.conf file, simply tell ATLASPlotting to let us inject arbitrary C++ code as a ROOT macro, and that this ROOT macro will handle the creation of histograms. The ATLASPlotting code will simply add the ATLAS label and format the axes nicely. So then, the meat is in the macro macros/efficiency_summary.cxx. We won’t copy-paste the entire file contents here, but you can read it yourself later. The main point to stress is that there are some helper functions provided in common/macros which can be included to help you glob the web files, assemble the 2D histograms, or run automatic tests on similarity.\nTry it yourself Now, having seen how the mtppp_summary pipeline works, try to write a summary pipeline taking information from one of the existing published pipelines and plotting it over time. A suggested problem could be to plot the sagitta bias corrections over time. To check your solution, compare to the existing pipeline tier0_sagitta_summary.\nNext: Running an existing analysis on the grid",
    "description": "Overview In this section we discuss a way to use the ATLASPlotting package and the published outputs of existing pipelines to create summary plots with several executions of the same pipeline compared. The most obvious use case is to execute the pipeline on every new data run, and compare the results over time, but almost anything can be done. Simply, these “summary” pipelines have access to all published pipeline output, and as we will see, we can inject arbitrary C++ code into the ATLASPlotting runtime environment, creating any number of plots from the published workflow output.",
    "tags": [],
    "title": "Monitoring results over time",
    "uri": "/commonsoftware/pipelineframework/tutorial/monitoring/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Understanding the pipeline.conf file In this module we will study the workflow pipeline-test. It is a simple data vs. MC comparison. There is not much physics behind it, as the datasets were chosen to be as small as possible to facilitate speedy execution in this tutorial. We will show more interesting, physically-motivated studies later in the tutorial.\nThe main file in all workflows is the pipeline.conf file. This is written using python ConfigParser syntax. This file specifies which tools to execute on which datasets, and any arguments to those tools. Let’s peek inside it:\ncd workflows/pipeline-test more pipeline.conf The output should look like:\n[Pipeline] Name = test-pipeline [MuonxAODAnalysis.data] Args = -n 100 InDS = data23_13p6TeV.00451543.physics_Main.merge.AOD.r14858_p5785 Image = 24.2.38-alma9-996c44d1 [MuonxAODAnalysis.mc] Args = -n 100 InDS = mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.deriv.DAOD_PHYS.e8514_s4159_r15530_p6491 Image = 24.2.38-alma9-996c44d1 [ATLASPlotter.test_multi_dataset] Image = master-accbecc3 ConfigFile = plots.conf Args = --plotNo 0,1 Dependencies = MuonxAODAnalysis.data,MuonxAODAnalysis.mc The file is organized into Sections. The first section is always “Pipeline”, which specifies global attributes. Here, just the name is specified. Then, three jobs are defined in this pipeline. The first two use the MuonxAODAnalysis package to create TTrees from AOD or DAOD data. The third job takes as input the output of the previous two jobs and creates histograms from the TTrees. There are several important keywords in each job specification. Those common to ALL tools are:\nImage: the name of the version of the tool to use. InDS: the name of the input dataset. Args: arguments to pass to the underlying tool. ▶ How do I know what options are available? By now you should know, the first stop should be mcp help if you aren’t sure how to continue. There’s a few options here to assist in drafting the pipeline:\nmcp help available-images mcp help image-config-args [PIPELINE NAME] mcp help pipeline-config-args They should be relatively self-explanatory. They show which images are available for each tool, which args can be paired with each image, and a full explanation of every possible field accepted in the pipeline.conf file.\nTime to run! The first time you run mcp test, you will be prompted to specify where large files should go. This is usually your personal eos directory: /eos/home-X/XXXX.\nNow, try running any of the pipelines locally. Some suggestions are shown below, as well as example output for each. Running each pipeline could take up to 10 minutes, as it needs to locally download some files from rucio, and it can take up to 5 minutes for each file download. 1 file from each dataset is automatically downloaded and stored in the directory specified as the preferred location for large files. You can remove these files later with a mcp clean command.\n▶ Try to run the pipeline-test workflow mcp test pipeline-test This is a simple workflow used to check that nothing is broken. It overlays information from data and MC and produces histograms of kinematic variables. You should see an output folder is now created in the main workflow folder pipeline-test. In that output folder there will be .root and .pdf files. You can view them a few ways, but the most straightforward is likely to download the .pdf to your laptop and open it. You should see output that looks like:\n▶ Try to run the tier0_sagitta workflow mcp test tier0_sagitta This workflow computes sagitta bias corrections on data. You should see a plot called “plots_dsagittaMap_VarMin_cummulative” in the output file with the suffix _plots.root. It should look something like this (but the precise values may vary):\n▶ Try to run the mtppp workflow mcp test mtppp This workflow computes the muon reconstruction efficiency in data. You should see output like this in the .pdf created, but the precise values may vary:\nNote What is the difference between mtppp and mtppp_summary? (or similar for other workflows) Strictly speaking, nothing: they are both valid workflows. However, in practice we often separate between “ordinary” workflows, which take data as input and make histograms, and “summary” workflows, which take as input the output of other workflows over time. These “summary” workflows are the ones most commonly displayed on the dashboard main page, because they show the stability (or instability) of the detector over time. While one workflow might have several stages, and output can be piped between them, so-called “summary” workflows are designed to display information over time from various runs. You can try to execute the summary workflow too! Just don’t submit it to the grid, as it will not work.\nYou may get some errors, especially “Server responded with an error: [3001]”, and “unkonwn branch” printout. These are not an issue. To see output, look in the corresponding workflow folder and see if a new .root file or “output” folder has been created.\nNext steps: drafting our own workflows Now that we have a hang of how to execute the existing code, next we will focus on how to write our own workflows. In order to do this, we need to explain a bit more about the underlying packages being used. We’ll focus on the packages used in the workflow pipeline-test. These give plenty of freedom in defining your own plots and are general-purpose. Other packages in the workflows mtppp and tier0_sagitta are single-purpose and less extendable, although they are heavily relied on to compute standard values like efficiencies and sagitta bias corrections.\nNext: The MuonxAODAnalysis package",
    "description": "Understanding the pipeline.conf file In this module we will study the workflow pipeline-test. It is a simple data vs. MC comparison. There is not much physics behind it, as the datasets were chosen to be as small as possible to facilitate speedy execution in this tutorial. We will show more interesting, physically-motivated studies later in the tutorial.\nThe main file in all workflows is the pipeline.conf file. This is written using python ConfigParser syntax. This file specifies which tools to execute on which datasets, and any arguments to those tools. Let’s peek inside it:",
    "tags": [],
    "title": "Running an existing analysis locally",
    "uri": "/commonsoftware/pipelineframework/tutorial/run_local/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Grid submission To run on the grid, the exact same command is executed, except with test replaced by submit. Try it now for one workflow!\n▶ Run the pipeline-test workflow on the grid mcp clean pipeline-test mcp submit pipeline-test Don’t forget mcp clean! It is good practice to remove previous output so you don’t get confused between the output from the grid and from a local call to mcp test.\nYour output should end with lines like:\nINFO : uploading workflow sandbox 20:24:17 [INFO]: uploading workflow sandbox INFO : gathering files under /afs/cern.ch/work/k/kenelson/mcp-pipelines/workflows/pipeline-test 20:24:17 [INFO]: gathering files under /afs/cern.ch/work/k/kenelson/mcp-pipelines/workflows/pipeline-test INFO : upload sandbox 20:24:20 [INFO]: upload sandbox INFO : submit workflow user.kenelson.test-pipeline20250606-20.23.49_default 20:24:21 [INFO]: submit workflow user.kenelson.test-pipeline20250606-20.23.49_default INFO : successfully submitted with request_id=1050301 20:24:22 [INFO]: successfully submitted with request_id=1050301 If you get an error like rucio.common.exception.CannotAuthenticate or in any way related to authentication, you most likely need to run voms-proxy-init -voms atlas to get a valid grid proxy.\nMonitoring the grid job The grid job can be monitored like any other. If you are new to monitoring grid jobs, visit bigpanda.cern.ch. There is a drop down menu at the top of the page which defaults to “Job by ID”. You can also click the “My BigPanDA to be taken directly to your personal user page, where this running job should show up.\n▶ How can I monitor the job progress on the command line? The setup.sh script sets up multiple command line utilities, one of which is pbook. The command help() in this environment will print more information. We are concerned with the show() command:\npbook show() Which sould produce output like:\nShowing only max 1000 tasks in last 14 days. One can set days=N to see tasks in last N days, and limit=M to see at most M latest tasks JediTaskID ReqID Status Fin% TaskName ________________________________________________________________ 45083602 203244 running 8% user.kenelson.test-pipeline20250606-20.23.49_default_001_MuonxAODAnalysis_mc 45083600 203243 scouting 0% user.kenelson.test-pipeline20250606-20.23.49_default_000_MuonxAODAnalysis_data Which tells us that our multi-stage job with two separate calls to MuonxAODAnalysis are being executed in parallel. The final job to merge output and creat final histograms will appear here once these two jobs are finished.\n▶ Some files were created by mcp submit. What are they? If you ls in the directory for the workflow you submitted, you will see a file pchain.cwl. This file specifies to the grid how to run the chain of jobs and pipe output between each stage. A full explanation of the file is outside the scope of this tutorial, but you can read more here. Even in the simple case of a two-stage job, the pchain is a 64-line file with a non-obvious syntax.\nThere are still other files. The file output.log is the same output printed to the screen, logged for convenience in debugging. default.yaml is the specification of datasets used by the pchain.\n▶ Download and examine the results After waiting for the execution of your job on the grid to complete (and monitoring via bigpanda.cern.ch or pbook), you can download it with:\nmcp get pipeline-test Usually, the output can be found in the folder workflows/workflow-name/output, although in general it can be different for different workflows. The output is in general NOT identical to locally produced output via mcp test, but is similar. mcp test downloads one input file, while when running on the grid all input files are used. The main difference between local and grid running should be statistical precision.\nTags There is one concept so far we have ignored: tags. This provides a solution for deploying a similar family of workflows over different files. Say for example, you want to update the InDS field of a workflow. One option is to simply change pipeline.conf, but now you would need to change it back if you want to replicate old results. We want the ability to reproduce any previous result without modifying a single line of code. Tags provide the solution by overriding select parameters in the pipeline.conf file. One example is in the pipeline-test workflow:\nmore workflows/pipeline-test/tags/mc23a.conf This tag tells the code to override only one parameter: the input dataset to the MC job. In this case, it swaps a MC23d dataset for a MC23a dataset.\nYou can submit a job to the grid (or locally) by adding the -t argument:\nmcp test pipeline-test -t mc23a As always, tab-completion will assist you in writing the commad line prompt. If you type -t and hit tab, it will show all the tags for that workflow. If you submit a non-default tag to the grid, you will need to download it with mcp get pipeline-test -t mc23a (or similar for your workflow).\nWhat can be accomplished with tags? In principle, anything. Arbitrary modifications of the pipeline can be achieved. However, it is highly discouraged to modify too many fields; at this point, you are better off creating a separate workflow folder. Suggested modifications include: accepting different file formats but running the same analysis, changing input file name, a “debug” version of the pipeline which print more information.\nNext: Advanced: adding a new package",
    "description": "Grid submission To run on the grid, the exact same command is executed, except with test replaced by submit. Try it now for one workflow!\n▶ Run the pipeline-test workflow on the grid mcp clean pipeline-test mcp submit pipeline-test Don’t forget mcp clean! It is good practice to remove previous output so you don’t get confused between the output from the grid and from a local call to mcp test.\nYour output should end with lines like:",
    "tags": [],
    "title": "Running an existing analysis on the grid",
    "uri": "/commonsoftware/pipelineframework/tutorial/run_grid/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "For posterity, this section discusses how the automated monitoring and VM server were set up. It can be safely skipped by most users.\nSetting up the VM server In order to run a pipeline on a regular basis, we first need a machine where we can run cron jobs without bothering admins. A virtual machine (VM) has already been set up via CERN OpenStack cloud resource management. The VM recieves commands from individual users who run the mcp cron command, which modifies the list of cronjobs executing on the VM. This section will describe how the VM was set up and configured for reference.\nThe instructions to set up a VM and mount a new volume are linked. The VM created is called atlas-muon-mcp.cern.ch and it can be accessed via ssh atlas-muon-mcp.cern.ch from lxplus. The volume mounted at /data has 250 GB of storage and is where the mcp-pipelines code is cloned.\nThe VM does not by default have all of the features of lxplus. The /cvmfs directory must be mounted and the singularity package must be installed. Directions to install cvmfs and singularity are linked. The most relevant information from these installation guides are to run the commands:\nsudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs sudo dnf install -y https://github.com/apptainer/releases/download/v1.3.4/apptainer-1.3.4-1.x86_64.rpm The mcp-pipelines code was cloned into the /data directory (our volume mount):\ncd /data git clone ssh://git@gitlab.cern.ch:7999/atlas-mcp/mcp-pipelines.git A Robot grid certificate associated with the accounts perf-muons and cimcp (a service account for the MCP group) is used to authenticate inside the VM. The final step is to set up a cron job that runs at a specific interval on the VM. The file cron/run.sh in the mcp-pipelines code is the file executed in the cron job. This file will read a ledger of jobs which are to be executed on that regular interval. The ledger can be modified on the command line using the mcp cron interface. In order to configure the cron job, execute crontab -e and paste in the command:\n0 * * * * /data/mcp-pipelines/cron/run.sh Which tells the machine to run the main script every hour. Next, we need to set up /eos access in order to publish the results to the web eos site:\nsudo dnf -y install locmap-release sudo dnf -y install locmap sudo locmap --enable all sudo locmap --configure all locmap --enable eosclient locmap --configure eosclient These commands may take a few minutes to execute. Now, you should be able to kinit with a CERN username (for example, cimcp) and access /eos. This authentication is automatically performed in the pipeline script. Then, check that everything is configured properly via crontab -l:\n[USERNAME@aiatlasmuon02 ~] crontab -l # HEADER: This file was autogenerated at 2024-09-20 21:05:09 +0200 by puppet. # HEADER: While it can still be managed manually, it is definitely not recommended. # HEADER: Note particularly that the comments starting with 'Puppet Name' should # HEADER: not be deleted, as doing so could cause duplicate cron jobs. 0 * * * * /data/mcp-pipelines/cron/run.sh # Puppet Name: eos-cleanup */5 * * * * /usr/local/sbin/eos-cleanup.sh \u0026\u003e /dev/null You’ll notice that there are other lines in the file, where were auto-generated by installing eos. These should remain.",
    "description": "For posterity, this section discusses how the automated monitoring and VM server were set up. It can be safely skipped by most users.\nSetting up the VM server In order to run a pipeline on a regular basis, we first need a machine where we can run cron jobs without bothering admins. A virtual machine (VM) has already been set up via CERN OpenStack cloud resource management. The VM recieves commands from individual users who run the mcp cron command, which modifies the list of cronjobs executing on the VM. This section will describe how the VM was set up and configured for reference.",
    "tags": [],
    "title": "Setting up the automated monitor server",
    "uri": "/commonsoftware/pipelineframework/tutorial/setting-up-online-monitor/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "A simple example: pipeline-test The ATLASPlotting package is a simple progam to create histograms and plots from nTuples. Let’s begin by walking through the configuration file in the pipeline-test workflow. It again uses the python ConfigParser syntax, and is therefore split up into sections which each hold some key-value pairs. The first section, General, reads:\n[General] OutputRootFile = muonxAODAnalysis_datamc.root OutputPDFFile = muonxAODAnalysis_datamc.pdf Label = Internal TreeName = AnalysisTree SumWgtTree = MetadataTree SumWgtBranch = AllExecutedEvents_SumOfEventWgts DSIDBranch = DatasetNumber Path = . : localOutputs/data : localOutputs/mc The [General] section holds information about the global inputs and outputs: the input trees, a path variable specifying where to search for inputs, and the names of output files. A full list of the parameters which should be supplied in the [General] section is:\nKeyword Purpose Default Value PlotWidth Width of TCanvas. Height=800. 1000 OutputDir Define where output is written (relative path to $PWD). output Path Colon-separated list of directories in which to search for input .root files “.” OutputPDFFile Name of .pdf output file None OutputRootFile Name of .root output file with histograms None IgnoreSumWgtsSF Do not compute sum of weights or SFs for the dataset (by default they are computed). Can be useful to turn this flag on for data-only plots 0 TreeName Name of TTree in .root input files which will be used to draw from None SumWgtTree Name of TTree with sum of weights (CutBookkeepers) information None SumWgtBranch Branch corresponding to the pre-filtered sum of MC event weights to normalize MC to data None DSIDBranch Branch corresponding to DSID number for sum of weights computation None The [General] section is also used as a default for other parameters. If a parameter should be applied to all other sections, it can simply be specified in general instead. A productive example here is the specification of Label to be “Internal”, so all plots will say “ATLAS Internal”.\nThe next section specifies some key-value pairs to do a find and replace throughout the rest of the file. In this simple example, we want our cut to be more human-readable, so we define the string HAS2MUON to mean that there are two muons in the event. With complicated cuts, it is often easier to read when these aliases are used. The Length$() function is a special syntactic sugar provided in the TTree::Draw interface. For more information, see the documentation.\n[FindAndReplace] HAS2MUON = (Length$(muon_pt)\u003e1) The PlotDefault section is a way to provide default parameters to many plots. However, it differs from [General] because PlotDefault can be used as a prefix, and multiple PlotDefault blocks can be defined for different families of similar plots. There are many options, and the full specification can be found at the bottom of this page under “Full List of Options”. Here, we describe the main options that control what gets plotted, and skip over some plot area beautification.\n[PlotDefault] # samples, legend, stacking certain histograms Type = Plot HStack = Zmumumc23a Samples = data23 : Zmumu_mc23a Legend = 0.6 : 0.7 : 0.9 : 0.85 Names = data \u003e\u003e data23 (2k events) Colors = data \u003e\u003e 1 Names = Zmumumc23a \u003e\u003e WWZ#rightarrow 2l+4#nu Colors = Zmumumc23a \u003e\u003e 617 StackOptions = hist f Latex = MCP Pipeline # cuts and weighting Cuts = 1.0 Weight = 1.0 In this section we specify which samples will be plotted, which samples will be stacked (usually backgrounds/MC), and what the names+colors are for these samples. We also specify that the stack will be filled in, and that the cuts to apply are that two muons are in each event. These will apply to all plots in this config file.\nSpecifying samples Samples are specified in thier own blocks beginning with the prefix SampleHandler.. The example specification below creates to samples: one for data and one for MC:\n[SampleHandler.data23] Files = *MuonxAODAnalysis.data*.root HIndex = data Weight = 1 Cuts = 1 Error = NONE Lumi = 1 IsTemplate = 0 [SampleHandler.Zmumu_mc23a] Files = *MuonxAODAnalysis.mc*.root HIndex = Zmumumc23a Weight = 1 Cuts = 1 Error = NONE Lumi = 1 IsTemplate = 0 All keys shown must be speicified, and all can be colon-separated vectors, so long as the length of the vector for each keyword is the same. There can be no default values in these blocks. The meaning of each key is summarized in the table:\nKeyword Purpose Files Which files to glob for this dataset. Searches in the Path defined in the General section. HIndex Short name used through the config file, e.g., to decide which histograms are stacked. Multiple samples can draw to multiple histograms. No one-to-one correspondence of histograms and samples. Samples help toggle sets of files. Weight Additional (multiplicative) weight besides the global weight. Can be any generic function of TTree branches. Cuts Additional cuts ANDed with global cuts. Error If not NONE, a percent error applied to the events in the histogram. Can also be a function of branches (e.g., “muon_pt”). Error bars would be proportional to that. Lumi Extra multiplicative weight, must be a number. Helps readability to separate event weights from Lumi. IsTemplate Usually 0. A template is a histogram from an orthogonal CR for data-driven modeling. Templates skip global cuts and rely on local Cuts within the SampleHandler block. ReplaceString.[YOUR STRING] Define an alias for this sample only. Useful for generic expressions like “muon_pt_WHICHSIGN” which can be replaced differently per sample (e.g., “Pos”, “Neg”). Affects the whole sample. See tier0_WPsVariables workflow. Making plots Now, so far it may seem like we aren’t making anything simpler with some 60+ lines of configuration. But, now an enormous amount of information is specified in an extendable way, making it easy to add a new sample or change a cut. Finally, specifying a new high-quality plot becomes extremely simple. You just need to specify what gets plotted, the bins, and the title. All of our work in the Default and General sections will pay off. For example, the existing configuration file plots muon pT and eta:\n[muon_pt*0.001] Bins=(50,0,100) Title=;muon p_{T} [GeV];N muons / 1 GeV [muon_eta] Bins=(60,-3,3) Title=;muon #eta;N muons / 0.1 Note that the title is specified following the TH1 syntax: stringt;stringx;stringy;stringz. The header for each subsequent section, after the special sections discussed, is simply the string representing the variable to plot. The bins are specified as the number of bins, minimum, then maximum. 2D plots can be specified using a colon between two variables, and then instead of 3 arguments, there would be 6 arguments in the parenthases for the number of bins in both the x and y dimension.\nNext, we suggest you try and extend the plots.conf file to add some plots. For a few illustrative examples, we pick nPrecisionLayers, “rho”, and a plot showing the tightest WP passed. The N precision layers plot is the simplest, while for the others you need to use multiple branches. The variable $\\rho$ is defined as:\n$$ \\rho = \\frac{p_{T}^{ID}-p_{T}^{ME}}{p_T^{CB}} $$\nFor the “tightest WP” plot, you should have 3 bins on the x-axis, one for muons that pass Loose but not Medium, the next for muons which pass Medium but not Tight, and the third bin for Tight muons. Try giving each bin a human-readable label.\n▶ Add more plots: nPrecisionLayers, rho, tightest WP In order to plot the three variables requested, you should have added some new plot blocks at the bottom of the plots.conf file. These should look something like:\n[muon_nprecisionLayers] Bins=(5,0,5) Title=;N precision layers;N muons [(muon_id_pt - muon_me_pt)/muon_pt] Bins=(50,-1,1) Title=;#rho=(p_{T}^{ID}-p_{T}^{ME})/p_{T}^{CB};Muons/0.04 Cuts = (muon_pt \u003e 0) \u0026\u0026 (muon_me_pt\u003e0) \u0026\u0026 (muon_id_pt\u003e0) [muon_Loose + muon_Medium + muon_Tight] Bins=(3, 0, 3) Title=;Tightest WP passed;N muons XBinLabels= Loose : Medium : Tight If you are running in to trouble because only blank canvases are being added to your .pdf file, you may need to remove the Args keyword in the pipeline.conf file, which is currently requesting only the 0th and 1st plots to be created: Args = --plotNo 0,1.\nThe new output from these three plots should look like:\nThere are many, many more things that can be plotted! We provided those three examples, but take some time and get creative! See what else you can plot!\nFull List of Options There are many options that can be included in a plot block to modify the canvas area. These include:\nKeyword Purpose Default Value Bins Specify binning like (Nbins, min edge, max edge). For 2D/3D: (NbinsX, minX, maxX, NbinsY, minY, maxY, …) None Colors Can be used multiple times. Maps HIndex to TColor. Syntax: Colors= [hindex] » [color] None Cuts Expression ANDed with Sample cuts (global to all samples). None DoHistPull If set, plot histograms in ratio panel. 0 DoPlots If set, generate output PDF. 1 DrawOverflow If set, add overflow to last bin. 0 DrawStackOnly If set, draw only THStack. 0 DrawUnderflow If set, add underflow to first bin. 0 FixedPullRatio Fixed distance for ratio panel above/below unity. None GOptions Draw options for graphs. None HStack Colon-separated list of HIndex values to stack. None LabelSizeX Size of X axis label. None LabelSizeY Size of Y axis label. None LegendStyle TLegend style, e.g., “lep” for line, error, point. None LogY Use logarithmic Y axis. 0 LogX Use logarithmic X axis. 0 MacroPath Macro run between histogram creation and drawing. HISTS is a pointer to vector of TH1*. None MacroArgs Arguments to MacroPath. Used to modify canvas after draw. None MarginPad1 Four colon-separated margin values for main pad: Top:Right:Bottom:Left. None MarginPad2 Same as MarginPad1 but for ratio pad. None MaxPullRatio Max zoom range above/below unity for ratio plot. None MCSubtract If set, subtract stacked histograms from data. Stack not drawn. None MCSubtractOnly Subtract only a subset of named histograms from data. Similar to HStack. None MinPullRatio Min zoom range above/below unity for ratio plot. None Names Can be used multiple times. Maps HIndex to label. Syntax: Names= [hindex] » [label] None NLegendColumns Number of columns in TLegend. 1 Norm Normalize histograms to unit area. 0 NormToBin Normalize non-stacked hists to a background bin (e.g., for cutflow). 0 NormToBkg Normalize non-stacked hists to integral of background. 0 OffsetX Offset of X axis title in main pad. None OffsetY Offset of Y axis title in main pad. None OffsetY2 Offset of Y axis title in ratio pad. None Options Draw options for histograms. None PostMacroPath Macro run after all histograms are drawn. HISTS pointer available. Useful for decoration or fitting. None PostMacroArgs Arguments to PostMacroPath. None PullHistHeight Ratio panel height relative to main. None PullTitle Title for ratio panel. None Samples Colon-separated list of samples to plot. None StackMaximumFrac Y max = this value × stack height (unless YLim set). 1.4 StackOptions Draw options for THStack. None SumW2 If true, use SumW2 flag to compute error bars with weights. 0 Title Histogram title. Syntax: Title=main;x-title;y-title;z-title None TitleX X axis title. None TitleY Y axis title. None TitleSizeX X axis title size. None TitleSizeY Y axis title size. None Type Type of plot. Currently, only “Plots” is supported. None Weight Event weight (float). Multiplied by cuts. Efficient to keep weights separate from booleans. 1.0 XBinLabels Colon-separated labels for X bins. None YLim Colon-separated Y axis min:max. None Next: Writing your own analysis",
    "description": "A simple example: pipeline-test The ATLASPlotting package is a simple progam to create histograms and plots from nTuples. Let’s begin by walking through the configuration file in the pipeline-test workflow. It again uses the python ConfigParser syntax, and is therefore split up into sections which each hold some key-value pairs. The first section, General, reads:\n[General] OutputRootFile = muonxAODAnalysis_datamc.root OutputPDFFile = muonxAODAnalysis_datamc.pdf Label = Internal TreeName = AnalysisTree SumWgtTree = MetadataTree SumWgtBranch = AllExecutedEvents_SumOfEventWgts DSIDBranch = DatasetNumber Path = . : localOutputs/data : localOutputs/mc The [General] section holds information about the global inputs and outputs: the input trees, a path variable specifying where to search for inputs, and the names of output files. A full list of the parameters which should be supplied in the [General] section is:",
    "tags": [],
    "title": "The ATLASPlotting package",
    "uri": "/commonsoftware/pipelineframework/tutorial/atlasplot/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "The mcp command The main way to use the code is via the mcp command line interface. To gain access to this command, you must source the setup script and make sure you have a valid grid certificate at the beginning of each session. You may have done this already on the previous page after you cloned the repository. If not, run now:\ncd mcp-pipelines source setup.sh voms-proxy-init -voms atlas Also note that the setup script must be set up from the root directory of this project in order to properly resolve paths. From there on, any command can be run from any directory.\n▶ Try running mcp If you execute:\nmcp You should see output like:\nThe command mcp wraps the pipeline interface. Use mcp help paired with any of the following for a help message Usage: mcp test \u003cdir-name\u003e: test a pipeline by downloading inputs and running locally mcp clean \u003cdir-name\u003e: clean a directory by removing output files mcp submit \u003cdir-name\u003e: submit a pipeline defined in workflows/\u003cdir-name\u003e/pipeline.conf mcp get \u003cdir-name\u003e: download results from the specified workflow. Some postprocessing (for example .pdf creation) may be run locally mcp publish \u003cdir-name\u003e: send output plots to the web mcp cron list \u003cdir-name\u003e: list current cron jobs mcp cron add \u003cdir-name\u003e: add a cron job on the remote VM for this job mcp cron drop \u003cdir-name\u003e: remove a cron job on the remote VM mcp help \u003coption\u003e: print extra help information about some topics ▶ What happens if you type “mcp” and hit tab? If you run:\nmcp [TAB] You should see output like:\nclean cron get help publish submit test When you hit tab at any point when typing a command starting with mcp, it will try to offer a sensible tab-completion. If tab completion is not working, it may be because some part of the current command is ill-formated OR because you forgot to source the setup.sh script.\nmcp help An extremely useful command for new users is mcp help. This command can provide more information on the other commands, as well as listing possible configuration options when drafting pipelines.\nNote Try running mcp help with different options and start digesting the printout.\nExplaining the command line options to mcp Most of the mcp XXXX commands take a workflow as an arugment. A workflow is a folder in the directory workflows, which contains a full specification of how to run the analysis from start to finish, with no additional input from you required.\n▶ Can you tab-complete workflow names? Try it! mcp clean [TAB] Will output:\nmmc_sagitta_bias mtppp_summary tier0_sagitta_summary mmc_scale_and_smear pipeline-test tier0_WPsVariables mtppp tier0_sagitta tier0_WPsVariables_summary When you hit tab at any point when typing a command starting with mcp, it will try to offer a sensible tab-completion. In this case, it will tab-complete the possible workflows listed in the workflows folder. Similar behavior exists if you instead try tab to complete the other commands, i.e. mcp submit for grid submission.\nmcp test will run the pipeline locally, downloading only one file per rucio dataset for a low-statistics comparison and validation of the configuration. mcp test uses the same docker images that are specified in the configuration file, so the environment is identical to the one used on the grid.\nThe command mcp clean will check a workflow folder for output files and tell the user which files it will delete, warn the user of their deletion and wait for confirmation, and then delete the relevant files. For example, the user will typically want to run mcp clean after they validate the pipeline via mcp test.\nThe command mcp submit will execute the pipeline by sending jobs to the grid, in a pchain, which will handle the ordered execution of all jobs in the pipeline and pass inputs and outputs appropriately.\nThe command mcp get will download the results of a pipeline and possibly execute some final commands locally, depending on the pipeline specification. For example, it may download, execute a ROOT hadd, and draw histograms from a plotting job.\nmcp publish will send output files from a particular pipeline to the web. This site is accessible only to those on the atlas-readaccess-current-physicists eGroup.\nmcp cron is used to interact with a remote virtual machine (VM) called mcp-test. mcp cron list will display the current list of jobs running on the remote machine on a regular basis. mcp cron add is used to add to the list, and mcp cron drop will remove a job from the list. Since this will modify the online monitoring, do not run mcp cron add/drop commands without first consulting with subgroup conveners.\nNote Don’t run mcp submit without first executing mcp test! This will allow you to check the plots and verify that the entire workflow runs without crashing. If there are plots which use cuts which can dramatically reduce the acceptance (for example, high pT muons), then consider adding one plot which is more inclusive so that any problems with the plot configuration can be caught more easily.\nNext: Running an existing analysis locally",
    "description": "The mcp command The main way to use the code is via the mcp command line interface. To gain access to this command, you must source the setup script and make sure you have a valid grid certificate at the beginning of each session. You may have done this already on the previous page after you cloned the repository. If not, run now:\ncd mcp-pipelines source setup.sh voms-proxy-init -voms atlas Also note that the setup script must be set up from the root directory of this project in order to properly resolve paths. From there on, any command can be run from any directory.",
    "tags": [],
    "title": "The MCP command line interface",
    "uri": "/commonsoftware/pipelineframework/tutorial/mcp-cli/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Overview MuonxAODAnalysis is used to create nTuples storing information about muons for many analysis projects. The code can be found here. The README at this link provides a full explanation of the command line interface.\nNote What branch of MuonxAODAnalysis to use? At the moment, the pipelines code uses the release_24 branch. This is because the pipelines require the managed packages to be distributed in Docker images, and at the moment release 25 of AnalysisBase is not available in Docker. This tutorial will use the release_24 branch.\nThe easiest way to play with the MuonxAODAnalysis package’s inputs is to do so by modifying the pipeline.conf file for the pipeline-test workflow. The Args keyword under each heading defines which arguments are passed to the underlying tool. For example, in the existing pipeline.conf file the number of events is limited to 100 to promote fast running for the simple test pipeline:\n[MuonxAODAnalysis.data] Args = -n 100 ▶ Recall: how can you find the possible Args to MuonxAODAnalysis on the command line? The mcp help interface provides the answers to most simple questions like this. In this case:\nmcp help image-config-args pipeline-test Will accept a pipeline and print a help message for each tool used in the workflow demonstrating the possible command line arguments. The output should include a message like:\n15:37:58 [INFO]: usage: SubmitLocal.py [-h] [--submission-dir SUBMISSION_DIR] --inputDS INPUTDS [--nevents NEVENTS] [--version VERSION] [--useGRL] [--usePRW] [--calibrate] [--systematics] [--trigger] [--efficiency] [--tool-config TOOL_CONFIG] [--metadata-file METADATA_FILE] To illustrate the use of command line arguments to the MuonxAODAnalysis package, let’s try and add pileup reweighting to the MC. This was listed as one of the possible command line options in the previous question block.\nNote The pipeline will take shortcuts! Let’s say you have already run mcp test pipeline-test once. If you execute the same command again, you will notice that it runs much faster. Why? The code will look for the output of previous stages and not repeat work if it can avoid it, only re-running the final plotting step. This is convenient because it allows you to tweak plotting and make small changes like moving legends without reproducing nTuples. However, in this case we want to reproduce the nTuple. The best way to get around this is to run rm -rf localOutputs/ in the pipeline-test directory. This will remove the cached output nTuples and force the pipeline to run the nTuple step. mcp clean could accomplish something similar, but would also remove the input files we downloaded from rucio, requiring even more time to download them again.\n▶ Adding pileup reweighting to the MC In order to add pileup reweighting the MC, you should have modified the pipeline.conf file accordingly:\n[MuonxAODAnalysis.mc] Args = -n 100 --usePRW To verify that this worked, let’s print out some of the pileup weights from the nTuple. First open the output file with ROOT:\nroot -l localOutputs/mc/MuonxAODAnalysis.mc_0.root Then, print out the weights stored in the tree:\nroot [1] AnalysisTree-\u003eScan(\"pileupWeight\") And you should see that in general pileup weights are no longer 1. Since each of us downloaded a random file, the values you see may not match precisely these:\n************************ * Row * pileupWei * ************************ * 0 * 1.2694757 * * 1 * 1.2694757 * * 2 * 1.2694757 * * 3 * 1.2694757 * If you see pileup weights that are exactly 1, there may be a few issues:\nDid you add PRW to MC, not data? Did you remove the previous output file in the localOutputs area, causing nTuples to be recreaed? Did you open the output file for MC, not data? Using new recommendations One use case of the pipelines code is to compare recommendations and validate that pre-release recommendations are in relative agreement with old recommendations before their release to the collaboration. With the pipelines framework, a workflow to validate pre-release recommendations can be preserved and automated. Here we will study how to inject arbitrary new recommendations files into MuonxAODAnalysis.\nAn example test set of recommendations can be found here. Provided are recommendations for run 3 from 2022 and 2024 for momentum calibration. In order to inject them into MuonxAODAnalysis we need first upload them into the workflow folder:\nscp recommendations.tar USER@lxplus.cern.ch:/path/to/mcp-pipelines/workflows/pipeline-test ssh USER@lxplus.cern.ch cd /path/to/mcp-pipelines/workflows/pipeline-test tar -xvf recommendations.tar Next, we need to tell MuonxAODAnalysis to reconfigure the momentum calibration tool using the local recommendations, rather than what it finds on cvmfs. .root files are ignored by default, so we need to explicitly tell the docker image to mount the recommendations folder, as shown. The argument to MuonxAODAnalysis to use the local recommendations is --tool-config, and we can point it to a new file that we will write called muonxaod_tool_config.conf. Just like pipeline.conf, the tool config file is written using python ConfigParser syntax. First, we add the tool config argument to the pipeline.conf file:\n[MuonxAODAnalysis.mc] Args = -n 100 --usePRW --tool-config muonxaod_tool_config.conf Mount = recommendations\u003c/pre\u003e Next, we need to write the tool config file. It has a section called ENVIRONMENT which allows any UNIX environment variables to be modified. We will want to extend the CALIBPATH variable to point additionally to the recommendations folder we just created. Therefore, the first few lines of our configuration file should look like:\n[ENVIRONMENT] CALIBPATH=$PWD/recommendations:$CALIBPATH Note Order matters! Here, we must pre-pend the local recommendations on the front of the CALIBPATH, forcing it to search locally first before resorting to using cvmfs. Otherwise, if the same recommendation can be found on cvmfs it would override our local changes.\nEach subsequent block of the tool configuration file is used to set the properties of one of the tools used by MuonxAODAnalysis. The header should be the name of the tool, which can be found by searching AnalysisToolsConfig.py. In this case, we want to modify the properties of the calibration tool. (Hint: search for “Calibration” in that file… you should find where the variable “name” is set for the calibraiton tool).\nWithin the block for the calibration tool, we want to set some properties of the tool. To see all possible properties, check the code for the MuonCalibTool in athena. The relevant properties here are “release” and “calibMode”. We want to set the “release” to one of those found in recommendations/MuonMomentumCorrections and the “calibMode” to 0, forcing the data to be corrected for sagitta bias using the combined calibration method.\nThefore, the full configuration file should look like:\n[ENVIRONMENT] CALIBPATH=$PWD/recommendations:$CALIBPATH [MuonCalibTool] release = Recs2024_05_06_Run2Run3_local calibMode = 0 Now, you should be able to run the code and verify that you get different momentum for your muons when you use the tool configuration file.\n▶ Verify that the recommendations are being applied The full pipeline.conf file should look like:\n[Pipeline] Name = test-pipeline [MuonxAODAnalysis.data] Args = -n 100 InDS = data23_13p6TeV.00451543.physics_Main.merge.AOD.r14858_p5785 Image = 24.2.38-alma9-996c44d1 [MuonxAODAnalysis.mc] Args = -n 100 --usePRW --calibrate --tool-config muonxaod_tool_config.conf InDS = mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.deriv.DAOD_PHYS.e8514_s4159_r15530_p6491 Image = 24.2.38-alma9-996c44d1 Mount = recommendations [ATLASPlotter.test_multi_dataset] Image = master-accbecc3 ConfigFile = plots.conf Args = --plotNo 0,1 Dependencies = MuonxAODAnalysis.data,MuonxAODAnalysis.mc Note that we have added both the tool configuration file and the --calibrate argument, which specifies that the calibration should be applied.\nThen, delete again the localOuputs, and run with mcp test pipeline-test and open the new root file with calibration applied. You should be able to see that there is a new branch for calibrated muon pT.\nroot -l localOutputs/mc/MuonxAODAnalysis.mc_0.root In the ROOT environment, you can print out the muon transverse momentum before and after applying the calibration:\nAnalysisTree-\u003eScan(\"muon_pt\") *********************************** * Row * Instance * muon_pt * *********************************** * 0 * 0 * 43072.324 * * 0 * 1 * 17150.072 * AnalysisTree-\u003eScan(\"muon_pt_calib\") *********************************** * Row * Instance * muon_pt_c * *********************************** * 0 * 0 * 42796.503 * * 0 * 1 * 16954.240 * Note that your values of pT might differ, since a random file was downloaded for the dataset. If something is not working, double check:\nDid you specify that the tool config file is passed to MuonxAODAnalysis? Did you copy and extract the recommendations folder inside the pipeline workflow folder? Did you specify the CALIBPATH variable in the tool config? Did you specify the release in the tool config? Did you mount the recommendations folder? Did you make sure to delete the old local outputs, forcing the nTuples to be reproduced? For more reading on applying recommendations, see the pipeline mmc_scale_and_smear.\nNext: The ATLASPlotting package",
    "description": "Overview MuonxAODAnalysis is used to create nTuples storing information about muons for many analysis projects. The code can be found here. The README at this link provides a full explanation of the command line interface.\nNote What branch of MuonxAODAnalysis to use? At the moment, the pipelines code uses the release_24 branch. This is because the pipelines require the managed packages to be distributed in Docker images, and at the moment release 25 of AnalysisBase is not available in Docker. This tutorial will use the release_24 branch.",
    "tags": [],
    "title": "The MuonxAODAnalysis package",
    "uri": "/commonsoftware/pipelineframework/tutorial/muonxaod/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Common Software Tools \u003e Automated Monitoring Framework \u003e MCP Pipelines Tutorial",
    "content": "Recap To this point, we’ve covered:\nHow to run jobs locally How to configure MuonxAODAnalysis to produce nTuples with PRW or special recommendations How to configure ATLASPlotting to plot the output This covers most use cases of a simple pipeline, other than submission to the grid (which is discussed at the end to not interrupt the flow of the tutorial, since it will take a while for your grid jobs to finish). Other tools to compute efficiencies and sagitta bias corrections are provided, and can be similarly investigated using mcp help. Here, we provide some suggestions and examples for pipelines. It is suggested that you try to recreate these existing pipelines on your own to check your understanding.\nUse MuonxAODAnalysis to compare two recommendations This suggested problem builds off of the last two sections. If you skipped them, it is suggested that you go back and review, as knowledge of topics such as injecting recommendations into MuonxAODAnalysis will be assumed.\nPreviously, we showed how to write a tool configuration file to modify all of the properties of the tools used by MuonxAODAnalysis. Can you write a pipeline which compares the invariant mass distribution for the Z peak using the CB and IDMS methods of calibration muon pT? Feel free to build off of the existing pipeline-test workflow or create your own new workflow.\n▶ Compare CB and IDMS calibration modes using MuonxAODAnalysis Here, we need to modify several pieces of the pipeline-test workflow. First, we need to add a new tool-config file to apply the calibration in IDMS mode (1). You will also want to borrow the previously used tool configuration file from the section discussing the MuonxAODAnalysis package. The tool config file for IDMS should look like:\n[ENVIRONMENT] CALIBPATH=$PWD/recommendations:$CALIBPATH [MuonCalibTool] release = Recs2024_05_06_Run2Run3_local calibMode = 1 Next, we need to modify the pipeline.conf to have another call to the MuonxAODAnalysis package for the IDMS calibration, separate to the nominal CB calibration. We should also remove the argument limiting the number of entries to 100 if it is still there, and the line in ATLASPlotter limiting the number of plots if it is still there:\n[Pipeline] Name = test-pipeline [MuonxAODAnalysis.data] InDS = data23_13p6TeV.00451543.physics_Main.merge.AOD.r14858_p5785 Image = 24.2.38-alma9-996c44d1 [MuonxAODAnalysis.mc] Args = --usePRW --calibrate --tool-config muonxaod_tool_config.conf InDS = mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.deriv.DAOD_PHYS.e8514_s4159_r15530_p6491 Image = 24.2.38-alma9-996c44d1 Mount = recommendations [MuonxAODAnalysis.IDMS] Args = --usePRW --calibrate --tool-config muonxaod_tool_config_idms.conf InDS = mc23_13p6TeV.601190.PhPy8EG_AZNLO_Zmumu.deriv.DAOD_PHYS.e8514_s4159_r15530_p6491 Image = 24.2.38-alma9-996c44d1 Mount = recommendations [ATLASPlotter.test_multi_dataset] Image = master-accbecc3 ConfigFile = plots.conf Dependencies = MuonxAODAnalysis.data,MuonxAODAnalysis.mc The pipeline.conf file is also referencing the recommendations folder used in the prior section. If you didn’t download it then, you will need to navigate back and download it, then add it to your workflow folder. In plots.conf we can add a plot for the invariant mass of the 2 muon system, as well as a Name/Color and SampleHandler block for the new IDMS calibrated MC sample:\n[General] Path = . : localOutputs/data : localOutputs/mc : localOutputs/IDMS ... [PlotDefault] Names = ZmumuIDMS \u003e\u003e Z#rightarrow#mu#mu, IDMS Calibration Colors = ZmumuIDMS \u003e\u003e 417 ... [SampleHandler.Zmumu_IDMS] Files = *MuonxAODAnalysis.IDMS*.root HIndex = ZmumuIDMS Weight = 1 Cuts = 1 Error = NONE Lumi = 1 IsTemplate = 0 ... [diObjM(muon_pt_calib[0]*0.001, muon_eta[0], muon_phi[0], 0.106, muon_pt_calib[1]*0.001, muon_eta[1], m\\ uon_phi[1], 0.106)] Bins=(80,80,100) Title=;m_{#mu#mu} [GeV];Arbitrary Units Cuts = HAS2MUON Latex = Normed shape comparison for IDMS and CB calibration Samples = Zmumu_mc23a : Zmumu_IDMS HStack = None DrawUnderflow= 0 DrawOverflow = 0 MarginPad1 = 0.03 : 0.06 : 0.2 : 0.15 Norm = 1 YLim = 0 : 0.06 You’ll notice a call to a function diObjM which we need to define. It takes the 4-vectors of two objects and computes the combined mass. If we create a folder called macros inside our workflow directory, any C++ code inside it will automatically be loaded in to the interpreter when we execute the ATLASPlotting job. So we can simply create a new C++ file in the macros folder and add a new function to it:\n#ifndef MACROS_FOUR_MOMENTA #define MACROS_FOUR_MOMENTA double diObjM(double pt1, double eta1, double phi1, double m1, double pt2, double eta2, double phi2, double m2) { return (ROOT::Math::PtEtaPhiMVector(pt1, eta1, phi1, m1)+ ROOT::Math::PtEtaPhiMVector(pt2, eta2, phi2, m2)).M(); } #endif Notice the use of guards with the directive #ifndef. This prevents the macro from being loaded more than once and is important! Now, we did need to modify 4 files but the number of new lines of configuration and code is still quite low, and it keeps different ideas encapsulated to different files. You should be able to produce a pretty plot that looks something like this:\nDemonstrating that the CB calibration produces a more narrow Z mass peak.\nMeasure single-run sagitta bias corrections Try to write a pipeline that measures the single-run sagitta bias corrections for run 499387 in the 2025 data taking period. You will need to search for the right input file on rucio, which should be in NTUP_MCPSCALE format:\nrucio ls data25_13p6TeV:*499387*physics_Main*NTUP_MCPSCALE* You will need to specify some default arguments to the underlying package used to compute the corrections. Try to use mcp help to see the options available. To work with the desired nTuple format, the options --ntupleType 2 --TrkCollection CB should be used at a minimum. Can you figure out additionally how to set the number of eta and phi bins to 9 each, and to set the number of iterations to 5?\n▶ Measure sagitta corrections for run 499387 using SagittaBiasCorrections Create a new folder in the workflows directory, and within it create a folder “tags” with an empty file default.yaml, and then a file for the pipeline configuration called pipeline.conf. Compared to other pipelines, we mainly just need to swap out the name of the header to point to the SagittaBiasCorrections package and then provide a new file for the requested run.\n[Pipeline] Name = sagittabias [SagittaBiasCorrections.data] InDS = data25_13p6TeV.00499387.physics_Main.merge.NTUP_MCPSCALE.f1591_m2272_c1619_m2238 Args = --iterations 5 --ntupleType 2 --TrkCollection CB --nEtaBins 9 --nPhiBins 9 prunArgs = --nJobs 1 --nFilesPerJob 200 The prunArgs are suggested for grid running to make the data files all go to a single job. Also shown are the suggested options to set the number of eta and phi bins and number of iterations. For more information, see the existing tier0_sagitta workflow.\nAfter running the job locally or on the grid, you should see output .root files. The file with the suffix _plots will contain TCanvas objects that you can draw JSROOT. The plot plots_dsagittaMap_VarMin_cummulative should look like:\nShowning that there are larger sagitta bias corrections needed in the endcaps near phi=0. Hover over the plots to see the value in each bin.\nMeasure single-run efficiency Try to write a pipeline that measures the single-run efficiency for run 499387 in the 2025 data taking period. You will need to search for the right input file on rucio, which should be in NTUP_MCPTP format:\nrucio ls data25_13p6TeV:*499387*physics_Main*NTUP_MCPTP* ▶ Measure efficiency for run 499387 using MuonTPPostProcessing Create a new folder in the workflows directory, and within it create a folder “tags” with an empty file default.yaml, and then a file for the pipeline configuration called pipeline.conf. Compared to other pipelines, we mainly just need to swap out the name of the header to point to the MuonTPPostProcessing package and then provide a new file for the requested run.\n[Pipeline] Name = mtppp [MuonTPPostProcessing.data] InDS = data25_13p6TeV.00499387.physics_Main.merge.NTUP_MCPTP.f1591_m2272_c1619_m2237 prunArgs = --nJobs 1 --nFilesPerJob 200 The prunArgs are suggested for grid running to make the data files all go to a single job. For more information, see the existing mtppp workflow.\nAfter running the job locally or on the grid, you should see an output pdf with plots like:\nDemonstrating that the loose muon efficiency is around 98% for this run. You can hover over the plots to get precise values.\nNext: Monitoring results over time",
    "description": "Recap To this point, we’ve covered:\nHow to run jobs locally How to configure MuonxAODAnalysis to produce nTuples with PRW or special recommendations How to configure ATLASPlotting to plot the output This covers most use cases of a simple pipeline, other than submission to the grid (which is discussed at the end to not interrupt the flow of the tutorial, since it will take a while for your grid jobs to finish). Other tools to compute efficiencies and sagitta bias corrections are provided, and can be similarly investigated using mcp help. Here, we provide some suggestions and examples for pipelines. It is suggested that you try to recreate these existing pipelines on your own to check your understanding.",
    "tags": [],
    "title": "Writing your own analysis",
    "uri": "/commonsoftware/pipelineframework/tutorial/write-your-own/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "Note The main documentation page of the IFF can be found here\nThe old IFF twiki page is here",
    "description": "Note The main documentation page of the IFF can be found here\nThe old IFF twiki page is here",
    "tags": [],
    "title": "Isolation Forum",
    "uri": "/isolationforum/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group \u003e MuonTPPostProcessing",
    "content": "The Run Config files define the analysis stream to run and the probe-match selections. Default run config files are stored in the MuonTPPostProcessing/data/RunConf folder.\nAccessing the Tree Before we can define selections \u0026 cuts we need to learn how to access the information in the run \u0026 histo configuration files. All information is interfaced through the TreeVarReader class. Each instance of the class is uniquely identified via its name.\nTo create an instance you should use the Reader keyword, handled by the CutAndReaderUtils functions.\nReader \u003cdata_type\u003e \u003cinformation name\u003e All information is stored w.r.t. standard ATLAS unit system, i.e. [MeV] \u0026 [mm]. If you need to convert the units then use floatGeV [GeV] or floatMu [µm] as data-types.\nWarning Once the TreeReader data-type is fixed it cannot be changed later. E.g.\nReader floatGeV probe_pt Reader float probe_pt will always end up in [MeV] → [GeV]\nSome special data types are available for dedicated purposes:\n# Run \u0026 Luminosity data-type Reader RunNumber run ### Returns the current (Random)runNumber on (MC)data Reader BinnedRunNumber binned ### Maps the runNumber into an ordered list Reader BinnedLumi 4. ### Slices the runs \u0026 lumiblocks into bins of 4 size Reader BinnedLumi 0.5 ### Slices the runs \u0026 lumiblocks into bins of 0.5 size # Constants Reader Pseudo TheAnswer 42 # Trigonometric functions Reader SinTheta probe ### Returns the sinθ of the probe Reader CotTheta tag ### Returns the cotθ of the tag-µ Reader D0Sig probe ### Returns the σ(d0)/d0 of the probe Simple Arithmetics It is possible to use simple arithmetics operation (+ - * /) and some basic functions (full list here), to define new Readers using the New_MathReader blocks. For example if you want to calculate\n$$\\frac{\\cos\\phi_\\text{probe} - \\cos\\phi_\\text{tag}}{\\sin\\phi_\\text{probe} + \\sin\\phi_\\text{tag}}$$ the needed block would look like:\nNew_MathReader / New_MathReader - New_MathReader cos Reader float probe_phi End_MathReader New_MathReader cos Reader float tag_phi End_MathReader End_MathReader New_MathReader + New_MathReader sin Reader float probe_phi End_MathReader New_MathReader sin Reader float tag_phi End_MathReader End_MathReader Alias fancy_cosStuff ### Give the reader a name without typing everything later End_MathReader Define a cut Cuts can be defined on any TreeReader object. To create a cut you’ll need to replace the Reader keyword by the Cut keyword. E.g.\nCut floatGeV probe_pt \u003e 25 Allowed comparison operators are \u003c , \u003c=, = , != , \u003e=, \u003e, |\u003c| , |\u003c=| , |=|, |! =| , |\u003e=| , |\u003e|.\nTo decide whether the cut belongs to the Probe or Match selection the keyword has to be augmented using the Probe, Match or Global suffix. E.g.\nProbeCut floatGeV probe_pt \u003e 25 MatchCut floatGeV probe_pt \u003e 25 GlobalCut floatGeV probe_pt \u003e 25 The selection/event is aborted as soon as one Cut in the list is failed.\nNote GlobalCuts or defined outside analysis selections are propagated everywhere.\nCombined cuts, like (A ∨ B) ∧ C, can be realized by making use of the CombCut blocks.\nProbeCombCut AND CombCut OR Cut float probe_eta |\u003e| 2.2 Cut int probe_q = 1 End_CombCut Cut Uint_8 probe_innerSmallHits \u003e= 2 End_CombCut Define the selection To define the probe-math selection let’s have a look at the structure of T\u0026P ntuple file. The trees are ordered in subdirectories, defining the Analysis stream, the considered probes and the charge product (Opposite or Same Charge) between the tag and the probe.\nThe analysis trees are loaded then by creating inside your RunConfig file a New_TPSelection block. For example, a simple selection for Z → µµ T\u0026P using IDProbes with opposite charge, matched to the Medium working point looks like,\nNew_TPSelection ZmumuTPMerged OC IDProbes ProbeCut floatGeV probe_pt \u003e 10. ### Select probe with pT \u003e 10 GeV MatchCut bool probe_matched_MediumMuons = 1 NameProbeSel TPTutorialProbes ### Name the probes (optional) NameMatchSel MediumGeneva ### Name the matches in the file End_TPSelection Detector Regions To avoid to much duplication, you can define detector regions, using the New_DetectorRegion block before your TPSelection, which provide a simple mechanism to test your probe selection in slices of the detector geometry, phase-space, or other properties. For example,\nNew_DetRegion RegionName IDRange Cut float probe_eta |\u003c| 2.5 End_DetRegion New_DetRegion RegionName Transition CombCut AND Cut float probe_eta |\u003e| 1.05 Cut float probe_eta |\u003c| 1.3 End_CombCut End_DetRegion Match Selections Analog to the detector regions, you can define different match selections using the New_MatchSelection block,\nNew_MatchSelection MatchName MediumMuonsSA Cut bool probe_matched_MediumMuons = 1 Cut bool probe_matched_MSTracks = 1 End_MatchSelection Adding DetRegions and Match selections to the TP_selection block Once you defined your DetRegion and MatchSelection block, you can use them inside your New_TPSelection block, using the DetRegion and the Matches keywords, like this,\nNew_TPSelection ZmumuTPMerged OC IDProbes ProbeCut floatGeV probe_pt \u003e 10. ### Select probe with pT \u003e 10 GeV MatchCut bool probe_matched_MediumMuons = 1 NameProbeSel TPTutorialProbes ### Name the probes (optional) DetRegion IDRange Transition Matches MediumMuonsSA End_TPSelection In this way, the TPSelection TPTutorialProbes is duplicated and ran foreach possible combination of Matches and DetRegion.\nWeighting the Monte Carlo Monte Carlo is normalized to 1/fb using the meta-data from the n-tuples \u0026 cross sections from the PMGCrossSectionTool.\nYou can additionally apply prwWeight, µ scale-factors or histograms making use of the Weight keyword in your Run Config file.\nHierarchical order\nGlobalWeight ProbeWeight (inside New_TPSelection only) MatchWeight (inside New_TPSelection only) For example,\nGlobalWeight PileUp ### Might need the prw Tool configured GlobalWeight TagTrigger ### Trigger selection inside input config demanded ProbeWeight TagSF \u003cWP\u003e ### Tag scale-factor ProbeWeight ProbeSF \u003cWP\u003e ### Probe scale-factor You can also apply weights using histograms in an external root file, with the following syntax.\n### Define first the readers you want to use in your run config Reader floatGeV probe_pt Reader float probe_eta Reader float probe_phi ##### Now you can use the usual weight creation mechanisms to pipe your weight to a TP selection or to declare it being global ##### For the sake of simplicity we use global weights # GlobalWeight Histo1D \u003cname_of_the_weight\u003e \u003cx_variable\u003e \u003clocation of the ROOT file\u003e \u003cname of the histogram in the file\u003e # GlobalWeight Histo2D \u003cname_of_the_weight\u003e \u003cx_variable\u003e \u003cy_variable\u003e \u003clocation of the ROOT file\u003e \u003cname of the histogram in the file\u003e # GlobalWeight Histo3D \u003cname_of_the_weight\u003e \u003cx_variable\u003e \u003cz_variable\u003e \u003cz_variable\u003e \u003clocation of the ROOT file\u003e \u003cname of the histogram in the file\u003e ### Be aware that there must be no space in the file name of in the name of the histogram itself GlobalWeight Histo1D demonstrator_weight probe_pt MuonEfficiencyCorrections/190530_r21/TTVA_Z.root sf_2016 GlobalWeight Histo2D demonstrator_weight_2D probe_pt probe_eta MuonEfficiencyCorrections/190530_r21/TTVA_Z.root sf_2017",
    "description": "The Run Config files define the analysis stream to run and the probe-match selections. Default run config files are stored in the MuonTPPostProcessing/data/RunConf folder.\nAccessing the Tree Before we can define selections \u0026 cuts we need to learn how to access the information in the run \u0026 histo configuration files. All information is interfaced through the TreeVarReader class. Each instance of the class is uniquely identified via its name.\nTo create an instance you should use the Reader keyword, handled by the CutAndReaderUtils functions.",
    "tags": [],
    "title": "Run Configuration Files",
    "uri": "/efficiency/postprocessing/run-configs/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "The group is responsible for deriving efficiency scale factors and associated uncertainties for use in physics analyses. The scale factors are provided through the MuonEfficiencyCorrections package, the usage of which is documented in Guidelines for Physics Analyses.\nSubgroup meetings are scheduled every week on Wednesday, 2pm (CERN time). Please subscribe to the atlas-cp-muon-tagprobe egroup in order to receive email announcements and support. Do get in touch if you are interested in joining our efforts. Open tasks are listed here for potential AQP/OTP as well as for contributions from physics analysis teams.\nMuonEfficiencyCorrections for Developers MuonTPPostProcessing Useful links MuonPerformanceAnalysis ntuples production Muon Combined Performance tutorial",
    "description": "The group is responsible for deriving efficiency scale factors and associated uncertainties for use in physics analyses. The scale factors are provided through the MuonEfficiencyCorrections package, the usage of which is documented in Guidelines for Physics Analyses.\nSubgroup meetings are scheduled every week on Wednesday, 2pm (CERN time). Please subscribe to the atlas-cp-muon-tagprobe egroup in order to receive email announcements and support. Do get in touch if you are interested in joining our efforts. Open tasks are listed here for potential AQP/OTP as well as for contributions from physics analysis teams.",
    "tags": [],
    "title": "Efficiency Working Group",
    "uri": "/efficiency/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group \u003e MuonTPPostProcessing",
    "content": "The Histogram configuration files define the binning and the histogram names in which the distributions are going to be plotted. The first step is to create the template for your histograms, which define the binning.\n### The 1DHisto creates a 1-dimensional template with equal sized binning ### 1DHisto \u003cname\u003e \u003cnBins\u003e \u003cxlow\u003e \u003cxhigh\u003e 1DHisto PtHisto 15 0 150 ### Analogously for 2D \u0026 3D \u003cname\u003e \u003cnBinsX\u003e \u003cxlow\u003e \u003cxhigh\u003e \u003cnBinsY\u003e \u003cylow\u003e \u003cyhigh\u003e 2DHisto EtaPhi 50 -2.5 2.5 32 -3.2 3.2 3DHisto z0D0SigEta 10 -0.5 0.5 50 -5. 5. 25 0. 2.5 ### We can also define a variable sized binning up to 3D separated by new lines VarHisto Var2DHisto -2. -1.9 -1.5 -1 -0.5 -0.25 0 0.25 0.5 1. 2. 0 10 20 50 70 100 1000 End_VarHisto Once the templates are defined, you can create your histograms, with the NewVar block.\nNewVar Type 1D ### possible 1D/2D/3D/2DPoly Template PtHisto Reader floatGeV probe_pt Name PtProbes xLabel p_{T} [GeV] BinXLabel 1 Very low BinXLabel 2 low BinXLabel 4 middle EndVar You can also use the MatchSel, MuonProbes, ProbeSel, DetRegion, Excl_MuonProbes, Excl_DetRegion and Excl_ProbeSel keywords in this block, to create the histogram only for dedicated selections. E.g.\nNewVar Type 1D Template Pt Reader floatGeV probe_pt xLabel p_{T} [GeV] Name Pt_IDProbes MuonProbes IDProbes ### Only added if the IDProbes is processed DetRegion noCrack HighEta### Only added for |η| \u003e 0.1 or |η| \u003e 2.5 ProbeSel TPTutorialProbes ### Only used in this tutorial EndVar NewVar Type 1D Template Pt Reader floatGeV probe_pt xLabel p_{T} [GeV] Name Pt_NoIDProbes Excl_MuonProbes IDProbes Excl_DetRegion noCrack HighEta Excl_ProbeSel TPTutorialProbes EndVar Other options can be defined inside the blocks using the extra keywords pullOverFlow, pullUnderFlow, NoBinNorm, useUnitWeight.\nAuxillary variable Auxillary variable allows the code to extend the defined histogram in auxillary dimension, which will be used later for the fitting (for example in the invariant mass spectrum). First you need to define the template for your auxillary variable as before.\nNew_Template 1DHisto Mll1D 29 61 119 xAxisTitle m_{ll} [GeV] End_Template After that, you can define a NewAuxVar block, connecting the auxillary variable to the variable block you created before.\nNewAuxVar ### Connect with primary variable Variable PtProbes Template Mll1D Name Aux_Pt Reader floatGeV dilep_mll EndAuxVar If you want to extend multiple histograms with the same auxillary variable, you can use the defineMultiLine block:\ndefineMultiLine Mll_AuxVar NewAuxVar Template Mll1D Name Aux_@{VarToFit} Variable @{VarToFit} Reader floatGeV dilep_mll EndAuxVar End_MultiLine Then, for each variable to fit, you just add the following snippet,\ndefineVar VarToFit PtProbes @Mll_AuxVar",
    "description": "The Histogram configuration files define the binning and the histogram names in which the distributions are going to be plotted. The first step is to create the template for your histograms, which define the binning.\n### The 1DHisto creates a 1-dimensional template with equal sized binning ### 1DHisto \u003cname\u003e \u003cnBins\u003e \u003cxlow\u003e \u003cxhigh\u003e 1DHisto PtHisto 15 0 150 ### Analogously for 2D \u0026 3D \u003cname\u003e \u003cnBinsX\u003e \u003cxlow\u003e \u003cxhigh\u003e \u003cnBinsY\u003e \u003cylow\u003e \u003cyhigh\u003e 2DHisto EtaPhi 50 -2.5 2.5 32 -3.2 3.2 3DHisto z0D0SigEta 10 -0.5 0.5 50 -5. 5. 25 0. 2.5 ### We can also define a variable sized binning up to 3D separated by new lines VarHisto Var2DHisto -2. -1.9 -1.5 -1 -0.5 -0.25 0 0.25 0.5 1. 2. 0 10 20 50 70 100 1000 End_VarHisto Once the templates are defined, you can create your histograms, with the NewVar block.",
    "tags": [],
    "title": "Histogram Configuration Files",
    "uri": "/efficiency/postprocessing/histo-configs/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group \u003e Efficiency Working Group \u003e MuonTPPostProcessing",
    "content": "The DSConfig files MuonTPPostProcessing has a huge python-codebase to create final/official MCP plots and Scale-Factor files. To process the histogram produced using WriteTagProbeHistos, you need to define instances of the python DSConfig class, inside a dedicated python file, called DSConfig file.\nFor example, a minimum instantiation of the class for a data sample of 2022 would look like.\nData_2022=DSconfig( name=\"Data_2022\", # name to be used by the code internally filepath=\"myhisto.root\", # The path to your histogram file (str or list) label=\"data 2022\", # The label your sample will get in the plots sampletype=Data # The type of sample, choose between Data, Signal, Irreducible, Reducible lumi=1. # in 1/fb units ) For all the variables, please refer to the class definition, or have a look at the default files here.\nTo run a full analysis, you need to define at least one Data sample and one Signal sample for the MC. Background samples can also be optionally defined with the Reducible/Irreducible keyword.\nThe luminosity value can be calculated on the flight, by parsing the InputConfig files, you used to generate the histograms. This can be done by adding the following snippet of code at the top of your DSconfig file.\nInCfgs = [] InCfgPath = ResolvePath(\"\u003cPath_To_InputConfig_Files\") Data_Samples = [S[:S.rfind(\".\")] for S in os.listdir(paths[\"all\"]) if S.find(\"data_\u003cyear\u003e\") != -1] for S in Data_Samples: InCfgs += ReadInputConfig(\"%s/%s.conf\" % (InCfgPath, S)) Lumi = 0. for R in GetNormalizationDB(InCfgs).GetRunNumbers(): if GetWeightHandler().getTriggerHelper().is_2k16(R): Lumi += CalculateRecordedLumi(R) Lumi /= 1.e3 Then you can use the Lumi variable inside your DSConfig class instantiation.\nCalculating the efficiencies Essentially, the efficiency can be simply calculated by dividing the n. of matches for the n. of probes, so simply dividing the histograms in the file. Unfortunately, there is background in the data sample that needs to be subtracted beforehand.\nThe signal and background contributions in data are extracted via a fit to the tag−probe invariant mass spectrum in the 61–121 GeV range, separately for the samples of all selected probes, and for the samples of matched probes\nAll SM processes producing an opposite-charge pair of prompt muons are treated as signal, and modelled with a invariant mass tag-probe template obtained using MC simulation. The background contribution stemming from all processes involving non-prompt muons is modelled using the following functional form:\n$$f(m_\\text{tag-probe}) = \\left(1-\\frac{m_\\text{tag-probe}}{\\Lambda}\\right)^{p_1} \\left(\\frac{m_\\text{tag-probe}}{\\Lambda}\\right)^{p_2}$$ where the Λ parameter, approximating the energy necessary to produce the dimuon pair, is set as 2.5 times the upper boundary of the considered mass spectrum. The 𝑝1 and 𝑝2 parameters are instead obtained via a separate fit using a sample of same-charge tag-and-probe pairs, satisfying all the selection criteria except the isolation requirements.\nTherefore, to perform the background fit, you need process both OC and SC phase spaces in your RunConfig and define an AuxiliaryHisto binned in mℓℓ for the variable you want to plot.\nHow to create plots Once you have your DSConfig file, you can finally execute the GeneratePlots.py macro to produce the plots you want. The macro can produce different type of plots, by setting the -t, -T, --plottype option, which can be:\nProbePlots: Create plots of the probe and match distributions. EffPlots: Create an efficiency plot (SF in the ratio panel). SysPlots: Create a breakdown plot of the systematic uncertainties. ClosurePlots: Create a closure plot (reweighted MC vs. original MC, more on that later). MasterEffPlots: Special case for Z → µµ reconstruction analysis to plot all WPs in one plot/ OverlayEffPlots: Special case to plot both efficiencies from Z and J/Psi decays in the same canvas. To execute GeneratePlots.py, issue the following command,\nGeneratePlots.py -c \u003cpath_to_DSConfig_file\u003e -t \u003cplot_type\u003e -a \u003cAnalysis Stream\u003e -o \u003coutput_folder\u003e where the supported analysis stream are defined inside Defs.py. Please note, that the analysis stream you are selecting must coincide with what you wrote in your RunConfig file.\nOther important flags that can be set are:\n-r, -R, --regions: Specify the detector regions to be used/ -v, -V, --var: Specific variable to plotted. Specify the variable \\ auxiliary variable pair via \u003cvar\u003e,\u003caux_var\u003e. -m, -M, --measurement: What probe - match pair should be ran. Please type them via \u003cprobe\u003e,\u003cmatch\u003e. Here are the possible Probes values, and here the Matches. For other options, have a look at the argument parser inside the script.\nCalculate Scale Factors Scale Factors can be calculated using the CreateSF.py script, which is executed with,\nCreateSF.py -a \u003cAnalysis Stream\u003e -c \u003cDSConfig_file\u003e --outputDir \u003coutput_folder\u003e --InCfgDir \u003cpath_to_input_config_file\u003e (--run3)",
    "description": "The DSConfig files MuonTPPostProcessing has a huge python-codebase to create final/official MCP plots and Scale-Factor files. To process the histogram produced using WriteTagProbeHistos, you need to define instances of the python DSConfig class, inside a dedicated python file, called DSConfig file.\nFor example, a minimum instantiation of the class for a data sample of 2022 would look like.\nData_2022=DSconfig( name=\"Data_2022\", # name to be used by the code internally filepath=\"myhisto.root\", # The path to your histogram file (str or list) label=\"data 2022\", # The label your sample will get in the plots sampletype=Data # The type of sample, choose between Data, Signal, Irreducible, Reducible lumi=1. # in 1/fb units ) For all the variables, please refer to the class definition, or have a look at the default files here.",
    "tags": [],
    "title": "Calculate Efficiency and SFs",
    "uri": "/efficiency/postprocessing/efficiency_sf_calc/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Welcome to the new Muon Combined Performance group page. This page replaces the old MCP Twiki\nMandate The Muon Combined Performance Working Group aims at optimizing and measuring the reconstruction and identification performance for muons in ATLAS. Muon reconstruction is based on the combined use of data from three subdetectors: Muon Spectrometer, Inner Detector, and Calorimeters. The activity is thus determined from the combined performance of these detectors and of the corresponding event reconstruction software, providing a link between Detector Software and Physics Analysis. The MCP activities cover three main areas: combined muon reconstruction and identification, data quality assurance, and guides for usage in physics analysis. The group collaborates closely with the Muon Trigger Signature Group.\nGeneral Information Conveners Conveners Luca Martinelli, Simone Francescato Contact Information MCP mailing list, MCP conveners Jira Project ATLASMCP GitLab Project atlas-mcp Open Tasks Combined Performance, Muon Software Derivations MCPDxAOD MuonDerivationFramework Muon Detector Performance Coordinators: S. Rosati, M. Iodice Muon Software Group Twiki, Coordinators: P. Kluit, P. Scholer Muon Trigger Signature Group Twiki, Coordinators: G. Meyer, Y. Yamaguchi Old twiki page MuonPerformance Subgroups The muon combined performance (mcp) group delivers (in terms of recommendations to physics analyses):\nmuon identification and working points selection\nmuon reconstruction, identification and isolation efficiency measurements\nmuon momentum calibration\nThe mcp activities span across four subgroups:\nSubgroup Convener(s) Mailing list Identification working points Alberto Prades Ibanez, Matous Vozak mail to conveners atlas-cp-muon-workingpoints Efficiency measurement Giorgia Proto, Alice Reed mail to conveners atlas-cp-muon-tagprobe Scale/Resolution measurement Siyuan Yan, Tamar Zakareishvili mail to conveners atlas-cp-muon-momentum-calibration Isolation Forum Matteo Bauce, Francisco Alonso, Abraham Tishelman-Charny mail to conveners atlas-phys-gen-IsoFakeForum Mailing list for all MCP subgroup conveners :atlas-cp-muon-subgroups-conveners\nMeetings This week meetings (MCP, MMC, Eff\u0026WP)\nGroup When Indico category MCP Plenary every Wednesday @16.00 CERN Time indico Muon Momentum Calibration every Wednesday @10.00 CERN Time indico Muon Efficiency and WPs every Wednesday @12.00 CERN Time indico Isolation and Fake Forum every Monday @14.00 CERN Time indico Managers \u0026 Contacts Task Contact Person mcp ntuple Production Dimitris Iliadis Derivation Framework Physics Contact in PMG group Marco Vanadia Derivation requests Manager MC production contact in PMG group Simon Grewe Contacts from Physics Analysis groups A description of the tasks of the physics analysis groups contacts is shown here:\nGroup Contact B-Physics \u0026 Light States Tatiana Lyubushkina Exotics Sebastien Roy-Garand Heavy Ions Qipeng Hu Higgs and Di-Higgs Tamas Marton Baer Standard Model Matous Vozak HMBS Simon Grewe Top Alberto Prades Ibanez Upgrade Studies Johannes Junggeburth Mailing list for MCP liaisons: atlas-cp-muon-liaisons\nUpdating the documentation page This page is made using the Hugo framework.\nThe documentation source is hosted on GitLab in the atlas-mcp repository.\nPeople that want to contribute to the documentation are encouraged to open a merge request. Information on how to do this can be found here",
    "description": "Welcome to the new Muon Combined Performance group page. This page replaces the old MCP Twiki\nMandate The Muon Combined Performance Working Group aims at optimizing and measuring the reconstruction and identification performance for muons in ATLAS. Muon reconstruction is based on the combined use of data from three subdetectors: Muon Spectrometer, Inner Detector, and Calorimeters. The activity is thus determined from the combined performance of these detectors and of the corresponding event reconstruction software, providing a link between Detector Software and Physics Analysis. The MCP activities cover three main areas: combined muon reconstruction and identification, data quality assurance, and guides for usage in physics analysis. The group collaborates closely with the Muon Trigger Signature Group.",
    "tags": [],
    "title": "ATLAS Muon Combined Performance Working Group",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "ATLAS Muon Combined Performance Working Group",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
